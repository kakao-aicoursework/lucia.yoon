{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_langchainEx1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. 엔플라잉 (N.Flying) - 비행 (Rooftop)\n",
      "2. 방탄소년단 (BTS) - 작은 것들을 위한 시 (Boy With Luv)\n",
      "3. 마마무 (Mamamoo) - 너나 해 (Egotistic)\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "text = \"여름에 듣기 좋은 K-pop 3가지 추천해줘\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"season\"],\n",
    "    template=\"{season}에 들으면 좋은 K-pop 3곡 추천해줘\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "겨울에 들으면 좋은 K-pop 3곡 추천해줘\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(season=\"겨울\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. EXO - 'Universe' \n",
      "2. SUNMI - 'Gashina' \n",
      "3. BLACKPINK - 'Forever Young'\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(season=\"겨울\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"season\"],\n",
    "    template=\"{season}에 듣기 좋은 노래 3곡 추천해줘\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. 겨울이 오면 - 마마무\n",
      "2. 겨울엔 기분 좋게 - 장덕철\n",
      "3. 불꽃놀이 - 백지영\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"겨울\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain import LLMMathChain\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "\n",
    "tools =[\n",
    "        Tool(\n",
    "            name=\"search\",\n",
    "            func=search.run,\n",
    "            description=\"시사에 관한 질문에 답해야 할 때 유용합니다. 타겟팅된 질문을 해야 합니다.\",\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"calculator\",\n",
    "            func=llm_math_chain.run,\n",
    "            description=\"수학 계산을 할 때 유용합니다.\"\n",
    "        )\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools,llm,agent=\"zero-shot-react-description\",verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m 일론머스크의 나이를 찾아야 합니다.\n",
      "Action: search\n",
      "Action Input: 일론머스크 나이\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m일론 머스크 옆에 있는 여성은 탱고 게임웍스의 개발자이자 지난 e3 2019 베데스다 프리젠테이션에 등장해 강렬한 인상을 주었던 나카무라 이쿠미(中村育美)다. 2020년 5월 2일에는 테슬라의 주가가 너무 높은 것 같다고 트위터에 올렸다. 당연히 테슬라 주가는 -10% ... 일론 머스크/생애 - 나무위키 일론 머스크/생애 최근 수정 시각: 2023-11-18 18:56:00 일론 머스크 인물별/생애 상위 문서: 일론 머스크 1. 초기 생애 [편집] 1.1. 유년기 [편집] 1971년 남아프리카 공화국 프리토리아 에서 엔지니어인 부친 에롤 머스크와 모델 겸 영양사인 모친 메이 머스크 사이에서 2남 1녀 중 장남 으로 태어났다. [1] 아파르트헤이트 시절 남아공으로 이민했던 백인 가정의 후손들 중 다수가 재산이 많았던 것도 있고, 아버지인 에롤이 남아공에서 최연소로 기술사 자격을 취득한 우수한 전기 기술자여서 어린 시절 머스크의 집은 상당히 부유했다고 한다. 나이 '갓 쉰' 머스크, 어쩌다 아이 열 명의 아빠 됐나 입력 : 2023-09-13 15:41ㅣ 수정 : 2023-09-13 16:52 일론 머스크 (52) 테슬라 최고경영자 (CEO)와 시본 질리스 (36) 뉴럴링크 임원과의 사이에 쌍둥이 자녀가 태어난 것은 머스크의 정자 기증을 거쳐서 이뤄졌다는 사실이 공개돼 눈길을 끌고 있다.... 현재 일론 머스크의 나이는 51세 라고 합니다, 50대 초반 나이입니다, 1971년 6월 28일 생입니다, 일론 머스크는 미국이 아닌 남아프리카 공화국 프리토리아에서 태어났다고 합니다, 일론 머스크는 엔지니어인 아버지와 모델인 어머니 사이에서 장남으로 태어나셨습니다 키 많은 분들이 일론 머스크님의 키에 대해서 궁금해하시는 거 같습니다, 인터넷에 알아본 결과 일론 머스크 님의 키는 188cm 라고 합니다, 상당히 키가 큰 분입니다, 실제로 사진을 보면 상당히 남들보다 키가 큰 모습을 볼 수 있습니다, 덩치도 상당히 크신 모습이 뭔가 되게 건장한 남성의 느낌이 납니다 국적 2023년 4월 13일 일론 머스크 트위터 CEO가 12일 (현지시간) 미 캘리포니아주 샌프란시스코의 트위터 본사에서 BBC와 깜짝 인터뷰를 진행했다. 인터뷰에서 머스크 CEO는 자신의 트위터 운영 방식을 옹호했다. 제임스 클레이튼 BBC 북미지역 테크 리포터는 세계에서 2번째로 부유한 인물이기도 한 머스크 CEO를 만나 1시간가량 트위터...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 일론 머스크의 나이를 알아냈습니다.\n",
      "Action: calculator\n",
      "Action Input: 51 * 2\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "51 * 2\u001b[32;1m\u001b[1;3m```text\n",
      "51 * 2\n",
      "```\n",
      "...numexpr.evaluate(\"51 * 2\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m102\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 102\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m 일론 머스크의 나이에 2를 곱한 결과를 알았습니다.\n",
      "Final Answer: 일론 머스크의 나이에 2를 곱하면 102입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'일론 머스크의 나이에 2를 곱하면 102입니다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"일론머스크의 나이에 2를 곱하면 얼마야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_langchainEx2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nchat = ChatOpenAI(\\n    model_name = 'gpt-3.5-turbo-16k',\\n    temperature = self.config.llm.temperature,\\n    openai_api_key = self.config.llm.openai_api_key,\\n    max_tokens=self.config.llm.max_tokens\\n)\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0.8)\n",
    "'''\n",
    "chat = ChatOpenAI(\n",
    "    model_name = 'gpt-3.5-turbo-16k',\n",
    "    temperature = self.config.llm.temperature,\n",
    "    openai_api_key = self.config.llm.openai_api_key,\n",
    "    max_tokens=self.config.llm.max_tokens\n",
    ")\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"assistant는 마케팅 문구 작성 도우미로 동작한다. user의 내용을 참고하여 마케팅 문구를 작성해라\"\n",
    "system_message_prompt = SystemMessage(content=system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['product_desc', 'product_name', 'product_tone_and_manner'], template='제품 이름: {product_name}\\n제품 설명: {product_desc}\\n제품 톤앤매너: {product_tone_and_manner}\\n위 정보를 참조해서 마케팅 문구 만들어줘'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_template = (\"제품 이름: {product_name}\\n\"\n",
    "                  \"제품 설명: {product_desc}\\n\"\n",
    "                  \"제품 톤앤매너: {product_tone_and_manner}\\n\"\n",
    "                  \"위 정보를 참조해서 마케팅 문구 만들어줘\"\n",
    "                  )\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template) #human_template : 유저가 입력하는 것\n",
    "human_message_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['product_desc', 'product_name', 'product_tone_and_manner'], messages=[SystemMessage(content='assistant는 마케팅 문구 작성 도우미로 동작한다. user의 내용을 참고하여 마케팅 문구를 작성해라'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['product_desc', 'product_name', 'product_tone_and_manner'], template='제품 이름: {product_name}\\n제품 설명: {product_desc}\\n제품 톤앤매너: {product_tone_and_manner}\\n위 정보를 참조해서 마케팅 문구 만들어줘'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name = input('제품이름을 입력하세요:')\n",
    "product_desc = input('주요 내용을 입력하세요:')\n",
    "product_tone_and_manner = input('광고 문구 톤과 매너를 방식을 입력하세요(예:신뢰, 유쾌, 엉뚱)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_slogan_list = []\n",
    "for i in range(10):\n",
    "    ad_slogan = chain.run(product_name=product_name,\n",
    "                    product_desc=product_desc,\n",
    "                    product_tone_and_manner=product_tone_and_manner)\n",
    "    ad_slogan_list.append(f\"- {ad_slogan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- \"나이키 운동화, 그 신기함을 경험해보세요!\"',\n",
       " '- \"운동을 새롭게 경험할 수 있는 나이키 운동화. 신선한 디자인과 착용감은 당신을 놀라게 할 것입니다. 함께 신기한 경험을 시작해보세요!\"',\n",
       " '- \"신나는 운동을 위한 최고의 동반자, 나이키 운동화! 신기한 디자인과 편안한 착용감으로 당신의 운동을 한층 더 특별하게 만들어줄거에요.\"',\n",
       " '- \"신나는 운동을 위한 최고의 선택, 나이키 운동화! 신기한 디자인과 편안한 착용감으로 당신의 운동을 더욱 높여줄 거예요. 뛰어난 품질과 스타일리시한 룩까지, 나이키 운동화와 함께라면 어떤 운동도 가능합니다. 자신감과 멋을 담아 신고, 세상에 독특한 당신을 보여주세요!\"',\n",
       " '- \"나이키의 신기한 운동화로 더욱 독특한 스타일을 완성하세요!\"',\n",
       " '- \"신나는 운동의 시작, 나이키 운동화! 신기한 디자인과 탁월한 성능으로 더욱 즐거운 운동을 경험해보세요.\"',\n",
       " '- \"신나고 모던한 나이키 운동화로 스타일을 완성해보세요! 뛰어난 톤앤매너와 함께 더욱 신기한 운동 경험을 선사합니다. 자신감을 품고 도전하는 당신을 위한 최고의 선택, 나이키 운동화입니다.\"',\n",
       " '- \"나이키 운동화, 신기함이 담긴 톤앤매너로 더 빠르고 더 멋진 운동을 즐겨보세요!\"',\n",
       " '- \"나이키 운동화, 신기함을 경험하세요!\"',\n",
       " '- \"나이키 운동화, 신기함을 느껴보세요!\"']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad_slogan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \"나이키 운동화, 그 신기함을 경험해보세요!\"\n",
      "- \"운동을 새롭게 경험할 수 있는 나이키 운동화. 신선한 디자인과 착용감은 당신을 놀라게 할 것입니다. 함께 신기한 경험을 시작해보세요!\"\n",
      "- \"신나는 운동을 위한 최고의 동반자, 나이키 운동화! 신기한 디자인과 편안한 착용감으로 당신의 운동을 한층 더 특별하게 만들어줄거에요.\"\n",
      "- \"신나는 운동을 위한 최고의 선택, 나이키 운동화! 신기한 디자인과 편안한 착용감으로 당신의 운동을 더욱 높여줄 거예요. 뛰어난 품질과 스타일리시한 룩까지, 나이키 운동화와 함께라면 어떤 운동도 가능합니다. 자신감과 멋을 담아 신고, 세상에 독특한 당신을 보여주세요!\"\n",
      "- \"나이키의 신기한 운동화로 더욱 독특한 스타일을 완성하세요!\"\n",
      "- \"신나는 운동의 시작, 나이키 운동화! 신기한 디자인과 탁월한 성능으로 더욱 즐거운 운동을 경험해보세요.\"\n",
      "- \"신나고 모던한 나이키 운동화로 스타일을 완성해보세요! 뛰어난 톤앤매너와 함께 더욱 신기한 운동 경험을 선사합니다. 자신감을 품고 도전하는 당신을 위한 최고의 선택, 나이키 운동화입니다.\"\n",
      "- \"나이키 운동화, 신기함이 담긴 톤앤매너로 더 빠르고 더 멋진 운동을 즐겨보세요!\"\n",
      "- \"나이키 운동화, 신기함을 경험하세요!\"\n",
      "- \"나이키 운동화, 신기함을 느껴보세요!\"\n"
     ]
    }
   ],
   "source": [
    "content = \"\\n\".join(ad_slogan_list)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_langchainEx3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=a5713ed02a57c43011754d632c3a30b826706687c482bf296de112e35f7d9d8c\n",
      "  Stored in directory: /Users/kakao/Library/Caches/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.5\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: requests in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests) (2023.7.22)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pandas in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Downloading tiktoken-0.5.2-cp310-cp310-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pydantic in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: duckduckgo-search in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (3.9.11)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from duckduckgo-search) (23.2.1)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\n",
      "Requirement already satisfied: lxml>=4.9.3 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from duckduckgo-search) (4.9.3)\n",
      "Requirement already satisfied: httpx>=0.25.1 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (0.25.2)\n",
      "Requirement already satisfied: anyio in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (4.1.0)\n",
      "Requirement already satisfied: brotli in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.1.0)\n",
      "Requirement already satisfied: socksio==1.* in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (4.0.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages (from anyio->httpx>=0.25.1->httpx[brotli,http2,socks]>=0.25.1->duckduckgo-search) (1.0.4)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install tiktoken\n",
    "!pip install pydantic\n",
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import tiktoken\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.8)\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "search.region = 'kr-kr'\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summarizer(llm):\n",
    "    system_message = \"assistant는 user의 내용을 bullet point 3줄로 요약하라. 영어인 경우 한국어로 번역해서 요약하라.\"\n",
    "    system_message_prompt = SystemMessage(content=system_message)\n",
    "\n",
    "    human_template = \"{text}\\n---\\n위 내용을 bullet point로 3줄로 한국어로 요약해\"\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        human_template)\n",
    "\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,\n",
    "                                                    human_message_prompt])\n",
    "\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens=3000):\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) <= max_tokens:  # 토큰 수가 이미 3000 이하라면 전체 텍스트 반환\n",
    "        return text\n",
    "    return enc.decode(tokens[:max_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(url):\n",
    "    response = requests.get(url, verify=False)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    text = ' '.join(soup.stripped_strings)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = build_summarizer(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(search_result):\n",
    "    title = search_result['title']\n",
    "    url = search_result['link']\n",
    "    snippet = search_result['snippet']\n",
    "\n",
    "    content = clean_html(url)\n",
    "    full_content = f\"제목: {title}\\n발췌: {snippet}\\n전문: {content}\"\n",
    "\n",
    "    full_content_truncated = truncate_text(full_content, max_tokens=3500)\n",
    "\n",
    "    summary = summarizer.run(text=full_content_truncated)\n",
    "\n",
    "    result = {\"title\": title,\n",
    "              \"url\": url,\n",
    "              \"content\": content,\n",
    "              \"summary\": summary\n",
    "              }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = input('토픽 키워드를 입력하세요:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': '실제로 많은 전문가들은 올해 금 전망을 긍정적으로 바라보고 있습니다. 시장조사기관 피치 솔루션은 올해 금값 평균 전망치를 온스당 1850 달러에서 1950 달러로 상향 조정했습니다. 또한, \"몇 주 내 금값이 2075달러까지 오를 것\"이라고 전망했습니다. 금융 ...',\n",
       "  'title': '2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프',\n",
       "  'link': 'https://kr.mitrade.com/commodities/gold-investment/trade-gold-commodity'},\n",
       " {'snippet': '2023년 금 시세 동향 살펴보기 EU, 러시아산 금 수입 금지, 금 값 동향 [금 관련 수혜주, 관련주] EU, 러시아산 금 수입 금지, 최대 은행 자산 동결 22년 하반기 국내 무역수지, 국고채 금리, 달러환율, 유류세, 금값 동향을 살펴보자. 1. 국내무역수지 넉달째 적자 금융위기 이후 14년만에 무역수 qlehfl0321 ...',\n",
       "  'title': '2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까?',\n",
       "  'link': 'https://qlehfl0321.tistory.com/entry/2023년-금-시세-동향-살펴보기-금투자-아직-메리트-있을까'},\n",
       " {'snippet': '국제 금시세 확인 국제 금 현물시장에서는 런던 금시장 현 협회에서 결정되는 금 시세가 국제 표준 금 시세로 인정되고 있습니다. 런던 금시장연합회는 런던에 있는 1987년에 설립된 금 거래 업자들의 조직으로 글로벌 은행 등의 주요 금시장 참여자 100여 개 ...',\n",
       "  'title': '금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - Btcc',\n",
       "  'link': 'https://www.btcc.com/ko-KR/academy/financial-investment/gold-price-forecast-will-gold-be-3000-in-2023-what-about-gold-investment'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchResults = search.results(topic, max_results=3)\n",
    "searchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'kr.mitrade.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프', 'url': 'https://kr.mitrade.com/commodities/gold-investment/trade-gold-commodity', 'content': \"2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 Mitrade 홈페이지로 가기 Mitrade 웹으로 거래 Mitrade 앱으로 거래 Mitrade 앱으로 거래 QR코드 스캔하여 다운로드 편집 정책 회사소개 인사이트 핫이슈 더보기 FX 지수 원자재 미국주식 암호화폐 2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 작성자 Mitrade 팀 | 갱신시간 2023-3-30 07:38 36914 글 목록 2023년 금값 전망 금시세 전망 금시세의 영향 요인 국제 금값 금시세 그래프 / 국내 금값 금시세 금투자 방법 - 원자재 투자 금투자 해야 하는 이유 -실시간 금값 시세 그래프 확인하기: -금테크 코로나19\\r\\n 이후, 역사적인 재정부양책에 금융시장에 유동성이 넘쳐나며 거의 모든 자산의 가격이 올랐습니다. 하지만 투자 환경은 시시각각 \\r\\n바뀌곤 합니다. 예를 들어, 중앙은행이 계속해서 양적완화를 지속해도 경제가 성장하지 못하면 갑자기 긴축정책으로 전환할 수도 \\r\\n있습니다. 따라서 하나의 포트폴리오만 가져가기보다는 여러 자산에 투자해 두고, 변곡점을 맞을 때마다 포트폴리오 내 비중을 조정하는\\r\\n 것이 가장 이상적인 투자법입니다. 원자재 상품 중 금은\\r\\n 투자자가 많이 찾는 상품이며, 역사적으로 대표적인 안전 자산 및 분산 투자의 핵심 자산으로 선호되어 왔습니다. ‘헤지펀드의 \\r\\n대부’로 불리는 레이달리오는 금이 역사적으로 포트폴리오의 균형을 맞추는데 일조해왔음을 강조, 자산 포트폴리오에서 금에 대한 비중을\\r\\n 8% 정도 유지하는 것이 이상적이라고 했습니다. 최근\\r\\n 화제가 되었던 코인 투자를 보시면, 사실 실체가 없는 것들입니다. 주식도 마찬가지입니다. 회사의 가치, 혹은 코인의 가치에 \\r\\n투자를 하지만 직접 그것을 보유할 수는 없습니다. 이러한 투자 종목들은 기본적으로 불안정한 가격 변동을 가지고 있습니다. 그러나 금은 현물거래 입니 다. 그런 만큼 가격 변동성이 크지 않아 안전자산으로 불리고 있기도 합니다. 최근 코로나 사태나 경제의 몰락 등 많은 변수들이 걱정되신다면 금과 같은 안전 자산을 현물거래 하는 것도\\xa0좋은 방법 중 하나라고 생각합니다. 최대 등락 상품 외환 원자재 외환 거래 주식 상품명 매수 매도 가격등락 2023년 금값 전망 금시세 전망 ▲2023/01/22 기준 금 펀드 수익률 (이미지 출처 : 머니투데이) - 금 시세 올 한해 금테크는 전문가들이 추천한 투자 중 하나인데요. 금은 대표적인 안전자산으로서 경기가 불안정할 때 수요가 늘어나기 때문입니다. 실제로 올해 1월 말 국제 금값이 20% 이상 급등하면서 작년 6월 16일 이후 6개월 만에 최고 가격을 기록했습니다. 이처럼 금값이 계속 상승하는 이유는 주식시장의 부진, 경기 침체 우려 확대, 미국 금리인상 속도 조절에 대한 기대감, 각국 중앙은행의 탈달러 현상 등의 이유가 복합적으로 작용한 것으로 분석됩니다. 지난 3월 22일(현지 시간)에는 온스당 1960 달러 선에서 거래되며 사상 최고치를 돌파했습니다. 최근 미국 실리콘밸리은행(SVB), 스위스 크레디트스위스(CS) 사태로 금융 시장의 불확실성이 확대되면서 안전 자산이 금으로 투자 수요가 몰린 것입니다. 세계 금 협회에 따르면 2022년 금 수요는 전년 대비 18% 증가하면서 11년 만에 최고치를 기록했습니다. 특히 각국 중앙은행의 금 매입 규모가 55년 만 최고 수준인 1136톤에 달하면서 전체 수요도 많이 늘어났습니다. 3월 20일에는 장중 온스당 2000 달러를 웃돌면서 1년 만 최고치에서는 다소 하락한 수준이긴 하지만 추가 상승에 대한 기대가 상당한데요. - 2023년 금값 전망 실제로 많은 전문가들은 올해 금 전망을 긍정적으로 바라보고 있습니다. 시장조사기관 피치 솔루션은 올해 금값 평균 전망치를 온스당 1850 달러에서 1950 달러로 상향 조정했습니다. 또한, “몇 주 내 금값이 2075달러까지 오를 것”이라고 전망했습니다. 금융서비스업체 CMC마켓은 미 연준의 금리인상 중단 시기가 빨라지면 금값은 또 급등할 것으로 보고, 올해 금값 전망치를 온스당 2500~2600 달러로 제시했습니다. 미국 경제 전문 뉴스 CNBC는 현재 중앙은행이 계속해서 금을 매입하는 것 역시 금값의 추가 상승 여력을 뒷받침한다고 덧붙였습니다. 대표적인 금 ETF 중 하나인 AuAg ESG 골드마이닝 상장지수펀드(ETF)를 운용하는 에릭은 올해 금값 전망이 역대 최고가를 찍을 수 있다고 봤습니다. 그는 온스당 2100 달러 돌파를 예상했는데요. 그는 “중앙은행의 금리 인상 속도 조절 가능성이 커지고 있고, 이는 몇 년 안에 폭발적인 금값 상승으로 이어질 것”이라고 설명했습니다. 스탠다드차타드 스탠다드차타드는 최근 발행한 보고서에서 올해 금 가격에 대해 ‘현재보다 30% 높은 온스당 2250달러까지 오를 가능성이 있다’고 내다봤습니다. 글로벌 투자정보기업 스위스아시아캐피탈의 수장을 맡고 있는 쥬르그 키네르 최고투자책임자(CIO)는 작년 말 2023년 금값을 온스당 4000 달러로 전망하기도 했습니다. 귀금속거래기업 MKS PAMP에 소속된 한 애널리스트는 외신 인터뷰에서 ‘미국 연준의 금리 인상 속도 조절 여부와는 상관없이 금값이 오를 것’이라고 언급했습니다. 김진영 키움증권 연구원은 최근 미 연준의 금리 인상 속도 조절에 대한 기대감을 나타나면서 달러 약세가 지속되고 있으며, 통상 실질금리와 달러와 반대로 움직이는 금이 이 영향으로 상승하고 있다고 현재의 금 상승 배경을 설명했습니다. 그러면서도 경기 불확실성이 계속 되면서 달러 및 금리는 현재의 수준을 크게 벗어나지 않을 것으로 분석했습니다. ‘리스크 및 인플레 헤지 수단으로서 금의 매력도가 달러 및 금리 안정화와 함께 다시 부각될 것’이라며 이와 동시에 현물 투자보다 더 용이한 금 ETF 투자 인기가 높아질 것으로 판단했습니다. 하지만 낙관적인 전망만 있는 것은 아닙니다. 김소현 대신증권 연구원은 금 상승세가 일시적이라는 데 무게를 실었습니다. 현재 금이 급상승한 이유는 안전자산 선호, 달러 약세, 실질금리 하락, ETF 자금 유입 등이 복합적으로 작용했기 때문이며, 전고점을 넘길 수 있는 재료가 부족하다고 판단했기 때문입니다. 그는 2020년 8월 온스당 2063 달러를 기록한 과거와 지금을 비교하면서 ‘현재는 SVB 사태 이후 은행 파산 우려가 있는데다 연준 정책의 불확실성이 커지면서 달러 강세 압력이 커졌다고 볼 수 있다’고 언급했습니다. 다시 강달러로 돌아서면서 상대적으로 금 가격 상승이 제한될 것이라는 얘깁니다. 금시세의 영향 요인 국제 금 시세/금값은 금이 기본적으로 미 달러화로 거래되므로 미국 실질금리, 달러 가치 그리고 수급요인이 결합하여 결정된다고 볼 수 있습니다. 이렇게 결정된 국제 금 시세에 달러 원 환율, 국내 수급요인이 더해져 국내 금 시세가 결정됩니다. 이 중 가장 큰 영향력을 주는 3가지 결정요인에 대해서 알아보겠습니다. ▶ 미국 실질금리： 금은 다른 상품과는 달리 이자를 지급하지 않는 실물 자산입니다. 따라서\\r\\n 금 투자 시 가장 큰 기회비용은 미국 실질금리이기 때문에 이는 국제 금 시세와 반대로 움직입니다. 여기서 실질금리는 명목금리에서\\r\\n 예상 인플레이션을 차감한 개념인데, 이 실질금리가 오르면 (명목금리 하락 혹은 물가 상승의 경우) 금 가격이 상승합니다. 미 \\r\\n국채와 금은 둘 다 최후의 안전자산이라는 인식이 투자자들 사이에 아직도 있기에 서로 대체재로 작용하기도 합니다. 즉,\\r\\n 미국 금리가 하락하면 미국채 투자 매력이 낮아지고, 금의 매력이 상대적으로 높아지는 효과가 있습니다. 또한, 물가 상승 시 \\r\\n인플레이션 헤지 용도로 금 투자 수요가 증가합니다. 실제로 코로나19의 미국 내 확산이 본격화되던 2020년 3월부터 미국 \\r\\n실질금리는 본격적인 마이너스 구간에 진입했고, 이에 금 가격은 2019년 초 1,283달러/온스에서 지난해 8월 역사적 고점인 \\r\\n2,067달러/온스까지 약 61% 상승했습니다. ▶ 미 달러 가치: 금은 달러 표시 자산으로 결국, 금 가격은 달러화에 대한 상대가치를 나타냅니다. 금이 변치 않는 절대가치를 갖고 있다고 가정할 경우 달러 표시 금 가격은 미 달러 가치와 반비례할 수밖에 없고 역사적으로도 금값과 달러 가치는 반비례의 상관관계가 있습니다. ▶ 수요와 공급 요인: 금은 공급이 거의 일정하고 변수가 없으며 수요의 경우 그 출처는 크게 귀금속 제품의 재료, 투자자산, 반도체 등 전자제품의 소재, 각국 중앙은행의 매수 및 보유로 나눌 수 있습니다. 금의\\r\\n 귀금속 수요는 가격과 역의 상관관계를 갖고 있으며 전자제품 및 중앙은행 수요 역시 전체 수요 내에서 차지하는 비중이 상대적으로 \\r\\n작거나, 그 기간이 단기라 가격 결정에 큰 영향을 미치지는 않습니다. 실질적으로 금의 가격을 결정짓는 것은 투자 자산 수요입니다. 국부\\r\\n 금펀드, 연기금, 개인투자자 등 다양한 투자자들이 매크로 지표를 기반으로 자산 배분 차원의 대규모 매매를 하다 보니 금에 대한 \\r\\n가격 결정력이 높은 편입니다. 이는 실물 시장 고유의 소유와 공급 현황이 가격과 상당히 밀접한 관계를 갖는 다른 원자재와 금이 \\r\\n차별되는 점입니다. 국제 금값 금시세 그래프 / 국내 금값 금시세 매일 변동하는 국제 및 국내 금 시세의 가격 결정 방법과 실시간 금값을 확인할 수 사이트, 그리고 금값 확인 시 주의해야 할 점을 알아보겠습니다. ▶ 국제 금시세 결정 방법 및 실시간 국제 금시세 확인 국제\\r\\n 금 현물시장에서는 런던 금시장 현 협회에서 결정되는 금 시세가 국제 표준 금 시세로 인정되고 있습니다. 런던 금시장연합회는 \\r\\n런던에 있는 1987년에 설립된 금 거래 업자들의 조직으로 글로벌 은행 등의 주요 금시장 참여자 100여 개 회원사가 참가하는 \\r\\n기구입니다. 런던의 금시장은 그 거래량이 세계에서 가장 많은 곳으로 런던 금시장연합회가 금시장의 거래 동향과 국제 금값을 \\r\\n좌우한다고 볼 수 있습니다. 협회\\r\\n 내 5개의 가격 결정 회원들이 각각 가격을 발표하고, 런던 금시장연합회는 이를 기준으로 수량과 평균 가격을 참고해 국제 금값을 \\r\\n결정하여 발표합니다. 발표는 매일 오전 10시 30분, 그리고 오후 3시에 이뤄집니다. 이렇게 발표된 금값은 국제 대량거래 등에서\\r\\n 기준으로 적용됩니다. 런던\\r\\n 금시장협회 회원들은 24시간 전 세계 시장에서 실시간으로 거래하며, 이때 거래된 가격은 블룸버그통신 등의 정보제공업체를 통하여 전\\r\\n 세계 시장 참여자들에게 전달됩니다. 이렇게 전달된 실시간 국제 금 시세는 한국금거래소, 한국금거래소 쓰리엠, 삼성 금거래소 등의\\r\\n 거래소 외에도 구글, 네이버, 인제 스팅 닷컴, 블룸버그 등의 포털사이트에서도 확인할 수 있습니다. ▶ 국내 금시세 결정 방법과 실시간 국내 금 시세 확인 국내\\r\\n 금 시세의 경우에는 국제 금시세 및 당일 환율을 기준으로 지식경제부의 인가를 얻은 한국귀금속유통협회에서 매일 오전 협회 기준가를\\r\\n 결정해 발표합니다. 이 기준 가는 매일 협회 사무국이 취합한 협회 이사 회원사의 기준 금 시세 중, 최고가와 최저가를 제외한 \\r\\n나머지 금 시세들의 평균으로 결정됩니다. 이때, 발표된 협회 기준가를 기준으로 이사 회원사들은 당일 중 국제 금값과 연동하여 \\r\\n회원사 간 거래를 진행하고, 또한 일정의 프리미엄을 더하여 이사 회원사의 소매시세를 결정하게 됩니다. 이와\\r\\n 같이 결정된 국내 금 시세는 한국 표준 금거래소에서 매일 오전 10시 30분 정도에 확인할 수 있습니다. 또한 실시간 국내 금 \\r\\n시세는 삼성 금거래소, 한국 표준 금거래소, 종로금거래소 등의 거래소 외에도 네이버, 다음 등의 포털사이트에서 확인할 수 \\r\\n있습니다. 거래소나 사이트에 따라서 해당 무게에 따른 금값도 알 수 있으므로 투자자가 사용하기 편한 사이트를 찾아서 이용하면 \\r\\n됩니다. ▶ 국제 금시세 국내 금 시세 확인 시 주의할 점 (무게 단위) 투자자들이\\r\\n 거래소 및 사이트에서 현재 금값을 확인할 때 한 가지 주의할 부분이 있습니다. 국제 금시장에서는 관행적으로 금의 무게를 \\r\\n‘온스(ounce)’라는 단위로 표기하지만, 이는 실제로는 ‘트라이온스'를 의미합니다. 1트라이온스는 31.1034768g입니다.\\r\\n 반면, 온스가 무게를 나타낼 때, 1온스는 28.349523ｇ이므로, 1트라이온스가 1온스보다 9.7% 더 무겁습니다. 국제 금시장에서는 1g, 1kg, 1ounce(트라이온스를 의미)에 대한 가격을 보여주며 국내 금시장에서는 무게에 따른 금 값 및 금 한 돈(3.75g)에 따른 금 가격을 보여줍니다. 금투자 방법 - 원자재 투자 투자자는\\r\\n 여러 가지 경로를 통해 금 투자할 수 있습니다. 그중 초보투자자들도 들어보셨을 방법은 골드 바(금괴) 구입일 것입니다. 금괴는\\r\\n 은행이나 금은방에서 구입 가능하며, 상속 증여 등을 위해 오래전부터 꾸준히 이어져 온 방식입니다. ▶ 첫 번째 방법은 바로 금 ETF 투자방법 및 금 \\xa0펀드입니다. -먼저 ETF에 대해서 소개 드리겠습니다. ETF는 주식시장에 상장되어 있는 금 인덱스펀드를 시가대로 매입과 매도를 합니다. 금 가격에 연동되지만 펀드로 운용됩니다. 또한\\r\\n 금 \\xa0ETF는 주식 매매와 같이 실시간으로 거래가 가능하죠. 그래서 금 \\xa0ETF는 주식 투자를 병행하시는 분이나 적극적인 \\r\\n투자자분들이 하시기에 적합합니다. 대표적인 투자 방법으로는 미래에셋ETF 에서 할 수 있는 타이거 \\xa0ETF가 있습니다. 자산\\r\\n 규모는 약 74억이고 2019년에 상장한 금 \\xa0ETF입니다. 타이거 ETF는 자산의 거의 대부분을 금 현물이 아닌 금 선물에 \\r\\n투자하는 상품으로서 수수료는 연 0.39%이고 일일 거래량이 비슷한 방법인 KODEX 골드 선물보다 적지만 총보수가 약 40%가량\\r\\n 더 저렴해서 미래에셋 ETF를 이용하는 것이 많이들 찾으시는 방법입니다. 무엇보다 금 ETF 투자가 좋은 방법인 이유는 적은 \\r\\n돈으로도 손쉽게 투자가 가능하기 때문입니다. 하지만 괴리율이 발생할 수 있다는 것이 단점으로 꼽힙니다. -다음은 금 펀드입니다. 펀드는\\r\\n ETF와 같이 금에 직접 투자하는 방법은 아닙니다. 금 펀드는 금 관련 사업, 금광회사 등의 주식에 투자하는 상품입니다. 사실 \\r\\n이렇기에 금 시세의 변동에 따라 함께 움직이는 경향이 크지만 항상 그런 것은 아니기 때문에 조금 다릅니다. 하지만 주식을 보유하는\\r\\n 것이기 때문에 주가 상승이나 배당금 등의 수익을 기대해 볼 수도 있고 안정적으로 분산투자나 적립식 투자가 가능하다는 것이 큰 \\r\\n장점입니다. ▶ 그다음\\xa0 금통장입니다. 하지만,\\r\\n 일반 투자자들은 금괴 구입보다 금통장을 개설해 골드뱅킹을 하는 것이 훨씬 편리합니다. 금통장은 투자자의 계좌잔액이 국제 금 \\r\\n시세에 연동되어 움직이는 간접투자 상품입니다. 투자자가 금통장 개설 후, 본인 계좌에 예금을 넣어두면 국제 금 시세에 따라 계좌 \\r\\n잔액이 자동으로 움직입니다. 여기서, 은행은 고객 예금으로 직접 금을 사들이는 대신 외국 은행에 달러로 예치합니다. 따라서, \\r\\n투자자는 원화를 예금하지만 투자자의 계좌 보유금은 국제 금 시세와 환율에 연동돼 자동으로 바뀌는 것입니다. 흔히들\\r\\n 골드 뱅킹이라고 불리기도 합니다. 현재 국민은행과 신한은행, 그리고 우리은행에는 개설이 가능하며 현금을 입금하면 그날의 금 \\r\\n시세에 맞는 금이 통장 안에 적립이 되는 구조로 이루어져 있습니다. 이 개념이 흔히 알고 계신 개념과는 많이 달라 헷갈리실 것 \\r\\n같아서 예를 들어보겠습니다. 예를 들어서 현재의 금 시세가 1g당 65000원이라고 가정합니다. 금통장 안에 65000원을 \\r\\n입금한다면 통장 안에 1g의 금이 채워지게 되는 것입니다. 이후\\r\\n 출금을 원할 때는 출고 당일의 시세에 맞춰 현금이 인출되게 됩니다. 만약 금 시세가 70000원이라면 5000원의 수익이 \\r\\n생겨나는 것입니다. 물론 거래 누적에 따른 은행 수수료와 소득세가 발생하긴 합니다만 아주 적은 돈으로도 가능하고 굉장히 간단하고 \\r\\n쉬운 투자 방법 중 하나입니다. 투자자는\\r\\n 은행을 통해 빠르게 금통장을 개설할 수 있으며, 소단위인 0.01g 단위로 금을 구입해서 통장에 적립할 수 있습니다. 또한, \\r\\n환매가 쉽고 실물거래가 없지만 현금 인출과 금 현물 인출이 가능해 비교적 자유롭게 금 투자에 참여할 수 있다는 장점이 있습니다. \\r\\n현재 시중 여러 은행들이 금통장 상품을 보유하고 있으며 가입대상과 기한, 금액에 대한 제한이 없습니다. 다만, 금통장을 통해 거래\\r\\n 시, 투자자는 해당 은행에 거래 수수료 및 매매차익에 대한 세금을 내야 합니다. 이 외에도 투자자는 한국거래소를 통하거나 금 현선물 가격에 따라 수익률이 정해지는 상장지수펀드에 가입, 카드사를 통한 금 거래 등도 가능합니다. ▶ 마지막으로는 CFD차액결재거래입니다. CFD（contract for difference）차액결제거래는 파생 상품에서 자산의 시가와 종가의 차이를 이용한 계약입니다. 선물과 비슷한 개념입니다. 실제 기초자산을 보유하지 않고 진입 가격과 청산 가격의 차액을 현금으로 결제하는 상품이라고 생각하시면 됩니다. 이 CFD는 다양한 원자재에서도\\r\\n 가능한데 금도 예외는 아닙니다. 만약 투자자분이 금 가격이 앞으로 상승할 것이라고 생각한다면 이 금 CFD를 구매하시면 됩니다.\\r\\n 시세가 오른다면 그 차익을 얻어낼 수 있습니다. 하지만 선물과 비슷하게 투자 원금에 대한 소실 위험이 크고 변동성이 큰 \\r\\n상품입니다. -금값 시세 실시간 그래프： 지금까지\\r\\n 금 현물거래와 금통장, 금 ETF, 금 CFD 그리고 금 펀드까지 다양한 금 투자 방법을 알아보았습니다. 안전 투자의 대표격인 \\r\\n상품인 만큼 다양한 투자 방법이 존재했습니다. 그렇다면 지금부터는 금 시세가 어떤 상황인지, 그리고 앞으로의 금 시세는 어떻게 \\r\\n될지 알아보겠습니다. Ad 금투자 해야 하는 이유 최근에는\\r\\n 시장의 변동성 증가와 인플레이션 헤지 수요가 몰리면서 미 달러화와 골드 가격이 동반 상승하기도 했습니다. 여기에 보태 금을 \\r\\n거래하는 사람들의 심리에 큰 영향을 받기도 합니다. 아시다시피 연말에는 많은 뉴스들이 있었습니다. 코로나의 변종인 오마 크론의 \\r\\n유행과 미국의 물가 상승, 연방 준비 위원회의 테이퍼링 정책, 원자재 가격의 상승, 국제 유가의 상승, 달러 가치의 변화와 금 \\r\\n생산지 생산량의 변화, 가상화폐 시장의 불안정성 증가 등등 많은 변수들이 현재의 금 시세를 만들어 냈다고 생각하시면 됩니다. 그중에서\\r\\n 가장 주목을 받는 요인은 바로 인플레이션입니다. 시장에서도 물가가 앞으로 더욱 오를 것이라고 전망하고 있습니다. 어쨌든 \\r\\n국제적으로 불안감을 증폭시키는 요인들로 인해 불안정성이 높아지고 이로 인해 안전 자산이 주목을 받기 시작한 것입니다. 많은\\r\\n 변수들이 금 시세를 만들어가고 있습니다. 하지만 대부분의 전문가들은 앞으로도 금값이 오를 것이라고 예상하고 있습니다. 가장 큰 \\r\\n이유는 역시 인플레이션입니다. 이 인플레이션이 국제적으로 계속해서 일어나고 있고 테이퍼링 정책까지 쓴다는 뉴스에도 시장은 믿고 \\r\\n있지 않습니다. 이러한 인플레이션은 불안정성을 만들어내고, 이는 안전 자산에 대한 관심으로 이어지게 됩니다. 현재 금 시세를 올린\\r\\n 원인이 내년에도 계속될 것이라고 예상하는 것입니다. 시장을\\r\\n 예측하기 힘든 요즘 같은 상황에서 주식, 채권, 외환, 원자재 등 투자자는 여러 자산이 적절히 배분된 포트폴리오를 구성해야 \\r\\n안정적인 수익을 낼 수 있습니다. 원자재 투자에서 금은 역사적으로 주식 및 국채와 낮은 상관관계를 지니고 있어 유용한 포트폴리오 \\r\\n분산 수단입니다. 따라서 투자자는 미 금리의 방향, 위험자산이지만 금의 대체재로 거론되는 비트코인 가격 움직임 및 인플레이션 우려\\r\\n 등을 종합적으로 감안해, 증시 조정을 방어하는 차원에서 일정 비중의 금 투자는 반드시 필요합니다. 장기투자의 관점에서 가격 \\r\\n조정은 분할매수 시기이니, 투자자들에게는 지금이 그 기회가 될 것입니다. 지금까지\\r\\n 금 투자 방법과 금 시세 전망에 대해서 알아보았습니다. 앞으로 불안정성이 커진다면 금 시세가 오른다는 것이 조금 아이러니하기는 \\r\\n합니다. 안전한 투자 방법으로는 사실 최고입니다. 장기 투자를 좋아하시는 분들이라면 꼭 추천드립니다. 많은 분들이 전에는 금을 \\r\\n단지 현물로서 보관하고 투자하는 방법만 아셨다면 지금은 더욱 다양한 방법이 존재한다는 것을 아실 겁니다. 무엇보다 안정적이고 \\r\\n다양한 투자 방법을 가지고 있는 금을 한번 투자해 보시는 것도 좋다고 생각합니다. Ad 면책사항: 본문의 내용은 편집자의 개인관점이며, Mitrade의 공식입장을 대표하지 않으며, 투자 권유 또는 제안의 목적이 아닙니다. 글의 내용은 단지 참고용이며, 독자는 본문의 내용을 어떠한 투자의 근거로 삼아서는 안됩니다. Mitrade는 이 글에 근거한 어떠한 거래 결과에 대해서도 책임을 지지 않습니다. Mitrade는 이 글의 정확성을 보증하지 않습니다. 투자 결정을 하기 전에 반드시 위험을 숙지할 수 있도록 독립적인 재무 상담사의 조언을 구해야 합니다. 차액 결제 거래(CFD)는 레버리지 상품이며, 귀하의 투자 원금 손실이 발생할 수 있습니다. CFDs 거래는 모든 사람에게 적합하지 않을 수 있습니다. 신중하게 투자하시기 바랍니다. Mitrade 팀 Mitrdae 분석 팀 구성원은 모두 전문적인 경제 지식과 긴 업계 경력을 지닌 전문가로, Mitrdae 투자 클래스에서 여러분의 투자 성공에 도움이 될 만한 투자 및 거래에 대한 다양한 지식과 경험을 공유합니다. 작성자 홈페이지 > 인기글 가장 많이 읽음 최신 발표 오리지널 거래 분석 가장 많이 읽음 최신 발표 데이터가 없습니다 글 목록 2023년 금값 전망 금시세 전망 금시세의 영향 요인 국제 금값 금시세 그래프 / 국내 금값 금시세 금투자 방법 - 원자재 투자 금투자 해야 하는 이유 핫이슈 더보기 금투자 금테크 금 투자 금 ETF 금시세 금값 시세 금 통장 금 테크 귀금속 투자 선물 투자 Ad 인사이트 글로벌 투자자에게 다양한 양질의 컨텐츠 제공 회사소개 핫이슈 더보기 FX 지수 원자재 미국주식 암호화폐 위험 고지 : 트레이딩 결과 전액 원금 손실이 발생할 수 있습니다. 장외파생상품이 모든 투자자에게 적합한 것은 아닙니다. Mitrade의 서비스를 사용하시기 전에 법적 공시 문서를 살펴보고 관련 위험을 숙지하십시오. 여러분은 기초 자산에 대한 소유권이나 이해 관계가 없습니다. 더보기\", 'summary': '- 2023년 금값은 긍정적으로 전망되고 있으며, 전문가들은 금값 상승을 예상하고 있다.\\n- 올해 금값 평균 전망치는 온스당 1850 달러에서 1950 달러로 조정되었으며, 몇 주 내에 2075달러까지 상승할 것으로 전망된다.\\n- 금은 안전 자산으로 여겨지며, 금테크를 포함한 다양한 투자 방법이 제시되고 있다.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'qlehfl0321.tistory.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까?', 'url': 'https://qlehfl0321.tistory.com/entry/2023년-금-시세-동향-살펴보기-금투자-아직-메리트-있을까', 'content': \"2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까? 본문 바로가기 검색 관리 글쓰기 로그인 로그아웃 메뉴 홈 '재테크' 가 money? 2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까? by 돈이Money? 2023. 1. 8. 2023년 금 시세 동향 살펴보기 EU, 러시아산 금 수입 금지, 금 값 동향 [금 관련 수혜주, 관련주] EU, 러시아산 금 수입 금지, 최대 은행 자산 동결 22년 하반기 국내 무역수지, 국고채 금리, 달러환율, 유류세, 금값 동향을 살펴보자. 1. 국내무역수지 넉달째 적자 금융위기 이후 14년만에 무역수 qlehfl0321.tistory.com 지난해 몇 차례 금 가격 상승에 대한 얘기를 한 적이 있다. 지난해 초 러시아의 우크라 침략과 더불어 미국의 고강도 긴축에 따라 투자자들이 뭉칫돈을 싸들고 앞다퉈 금을 사들이면서 가격이 고공행진했지만, 그동안 킹달러 현상과 경기침체 공포가 뒤를 이으며 금 가격이 줄곧 하락하는 모양새를 보였다. 하지만 지난 지난\\xa07월 23일 금 값 동향에 대한 글을 쓰면서 강달러와 기타 사안들로 인해 투자처를 잃은 투자자들이 금으로 눈을 돌릴 것 같다는 것이 현재 개인적인 판단이라는 얘기를 했던 적이 있었는데,\\xa0오늘자 금시세 가격을 보니 당시보다 꽤 많이 오른 모습이다. 반응형 물론 어쩌다 때려 맞춘 것이나 다름없고, 2022년 08월 12일부터 큰 하락을 겪기도 했지만, 더 싸질때 샀던 나에게는 다시 꾸준히 오름세를 보이고 있는 것은 미리 금 투자를 준비했던 나에게 개인적으로 호재다. 2022-07-22일자부터-금시세-변동-현황 금투자, 아직 메리트가 있을까? 이처럼 최근 들어 금 가격이 다시 상승하고 있다. 금투자, 아직 메리트가 있는 것일까? 금 가격은 지난해 초 러시아의 우크라이나 침공이 시작되면서부터 크게 뛰었다. 6일 금융투자업계에 따르면 지난 6일 지난해 6월 10일(1875.5달러) 이후 국제 금 가격이 급등하며, 7개월 만에 최고치를 기록 하며 장을 마감했다. 이 같은 금 가격의 최근 상승세는 달러화 약세와 밀접한 관련이 있다. 고강도 긴축으로 인한 유동성 축소로 달러화 가치가 떨어지게 되면서 주요 6개국 통화 대비 달러화 가치를 나타내는 달러 인덱스는 지난해 110포인트를 웃돌다가 올 들어 104포인트 선까지 떨어졌다. 중앙은행-금-보유량-추이 또한 최근 들어 국제 금 가격 상승은 일반 투자자들 뿐만 아니라. 각 국의 중앙은행들까지 앞다퉈 금을 사들이고 있는 상황도 한 몫하고 있다고 볼 수 있는데, 이러한 원인은 경기침체 현실화 우려에 대한 금투자만의 매력이 더 반짝일 것이라는 예측 때문일 것이다. 이렇게 금 가격을 결정짓는 요인으로는 달러화 가치뿐만 아니라. 중앙은행의 금 보유량과 실질금리 변화 등에 여러 이유가 함께 따라온다. 위에 중앙은행 금 보유량 추이 그래프를 보면 지난해 3분기부턴 각국 중앙은행들이 금 보유량을 폭발적으로 늘렸다는 것을 알 수 있다. 세계 금 협회(WGC)등에 따르면 지난해 3분기 각국 중앙은행들의 금 보량은 399.3톤으로 전년 동기(90.6톤)보다 약 341% 증가했다고 하는데 주로 튀르키예 31.1톤, 우즈베키스탄 26.1톤, 인도 17.5톤, 카타르 14.8톤 등 신흥국 중앙은행들이 금을 사들이고 있다고 한다. 이로 인해 각 금융투자업계들에서는 주로 신흥국 시장에서의 금 매수 확대는 인플레이션 헷지(위험 회피) 수요와 자국 통화가치 불안으로 인한 안전자산 수요로 인한 것으로 올해까지 이 같은 흐름이 지속될 가능성이 높다고 분석하고 있다. 이는 통상 금값과 반대되는 흐름을 보이는 실질금리가 여전히 높은 상황으로 미국의 금리인상 사이클이 아직 종료되지 않았지만, 추후 실질금리, 달러화 가치가 하락하면서 안전자산으로서의 금의 매력이 더 부각될 것으로 보여서다. 통상 대표적인 실질금리 지표인 미국국채 10년물 금리와 금값을 비교할 때에 과거 지표들을 보면 미 국채금리가 떨어지면 금 값이 오르고 국채금리가 올라가면 금값은 떨어지는 경향을 보여왔다. 채권을 이해하면 경제가 보인다. 출처: https://qlehfl0321.tistory.com/ [돈이 money? 의 돈 공부 거기에 일상까지 한 스푼 더합니다.] 본 포스팅의 내용은 무단 전재 배포를 원하지 않습니다. 채권을 이해하면 경제가 보인다. 채권(Bond)이란 qlehfl0321.tistory.com ↑ 채권에 대해 궁금한 내용은 아래 링크를 통해 글을 읽으신다면 도움이 될 것으로 생각됩니다. 금 값 급등, 아직 안 끝났다? 지난 금값이 급등했던 1970년대 후반과 2008년 금융위기 당시를 회상해 보면, 경기상황이 모두 불투명했다는 공통점이 있다. 이론적으로 금은 공급제한성을 가진 실물자산이자 내재가치가 있는 화폐자산으로 본질 가치를 회복하기 위해 가격이 상승하는 것이 보통인데 여기에 인플레이션 헷지 수요가 유입되면 그 수준 이상으로 가격이 오를 가능성이 충분히 존재한다. 이 같은 이유들을 빌어 요즘 여러 국내외 증권사들의 분석으로는 현재도 금값이 오르고 있는 상황이지만, 상승 랠리는 아직 시작도 안 했다는 분석을 내어놓고 있는데, 그 예로 외국계 투자사인 골드만삭스와 뱅크오브아메리카는 올 2분기부터 금 가격이 본격적으로 올라갈 것으로 예상하기도 했다. 뱅크오브아메리카의 분석에 따르면 미국의 매파적인 금리인상이 올해 정점을 찍고 중앙은행이 정책을 선회한다면 금 투자자들이 새롭게 시장으로 진입할 것으로 분석하고 나섰다. 또한\\xa0골드만삭스는 미국 경기가 경착륙 하면 온스당 최대 2250달러까지 올라갈 수 있다고 보고 있다는데, 이런 골드만삭스의 전망대로 금 가격이 상승하면 사상 최고치를 기록하는 셈이다. 골드만삭스의 분석으로는 미국이 강한 경기침체를 겪게 된다면 금 가격이 온스당 2250달러까지 올라갈 수 있지만, 중앙은행이 계속해서 매파적인 정책을 내놓는다면 1500달러까지 하락할 수 있다며 신흥국 중앙은행의 수요가 금 가격의 하방경직성을 지지해줄 것이라는 의견을 보이기도 했다. 출처: https://qlehfl0321.tistory.com/ [돈이 money? 의 돈 공부 거기에 일상까지 한 스푼 더합니다.] 본 포스팅의 내용은 무단 전재 배포를 원하지 않습니다. 늘 말씀드리지만, 투자의 책임은 온전히 본인에게 있는 것입니다. 지극히 개인적인 주관적 관점이 아주 많이 담겨있으며,\\xa0 포스팅 내용은 참고만 하시고 무리한 투자는 자제하시기 바랍니다. 반응형 공유하기 게시글 관리 구독하기 돈이money? 의 돈 공부 거기에 일상까지 한 스푼 더합니다. 저작자표시 비영리 변경금지 ' '재테크' 가 money? ' 카테고리의 다른 글 종합자산관리계좌(ISA) 보다 좋은 '비과세 종합 저축'이란!? (21) 2022.12.15 신용부도스와프 (CDS) 프리미엄이란? (16) 2022.11.08 미친 금리에 요즘은 흔해진 4%대 정기예금 은행별 비교,  은행 예/적금 주의점 (37) 2022.10.10 태그 2023년 금 시세 동향 살펴보기 , 골드만삭스의 분석 , 국제 금 값 7개월 만에 최고치를 기록 , 금 값 급등 , 금투자 아직 메리트가 있을까? , 뱅크오브아메리카의 분석 , 신흥국 중앙은행들의 금 사재기 , 아직 안 끝났다? , 인플레이션 헷지 수요 유입 관련글 종합자산관리계좌(ISA) 보다 좋은 '비과세 종합 저축'이란!? 신용부도스와프 (CDS) 프리미엄이란? 미친 금리에 요즘은 흔해진 4%대 정기예금 은행별 비교,  은행 예/적금 주의점 글로벌 증시는 암흑기, 일본 증시는 강세? 엔화 환테크 상황정리 댓글 28 비밀글 등록 분류 전체보기 My 스토리 '정책과지원'이 money? '재테크' 가 money? '주식'이 money? '부동산'이 money? '통계 & 지표' 가 money? 1억을 모으는데 10년이 걸렸다. 공감, 이슈, 관심 최근댓글 너무 궁금해요 구독자님 안녕하세요~ 흥미로운 포스팅 감사합니다. 구독⋯ 댓글 감사합니다. 티스토리 자동 맞춤법 검사기로 마지막⋯ wheat를 한국어와 헷갈려\\nmeal 이라고 표기한 걸⋯ TOP TEL. 02.1234.5678 / 경기 성남시 분당구 판교역로 © Kakao Corp. var checkAdsenseAdsFlag = true;\\nvar checkAdsenseAdsTimer = 0;\\nvar checkAdsenseAdsCnt = 0;\\nfunction checkAdsenseAds() {\\n    if(checkAdsenseAdsFlag) {\\n        if(checkAdsenseAdsTimer != 0) {\\n            clearTimeout(checkAdsenseAdsTimer);\\n            checkAdsenseAdsTimer = 0;\\n        }\\n        checkAdsenseAdsTimer = setTimeout(function() {\\n            var insAdsbygoogle = $('ins.adsbygoogle');\\n            if(insAdsbygoogle.length > 0) {\\n                var cnt = 0;\\n                for(var i=0; i 5) insAdsbygoogle.eq(i).remove();\\n                    }\\n                }\\n                if(cnt == 0) checkAdsenseAdsFlag = false;\\n                checkAdsenseAdsCnt++;\\n            }\\n        }, 500);\\n    }\\n} 티스토리툴바 돈이money? 의 돈 공부 거기에 일상까지 한 스푼 더합니다. 구독하기\", 'summary': '- 2023년 금 시세 동향을 살펴보자. EU와 러시아에서는 금 수입을 금지했고, 최대 은행 자산도 동결되었다.\\n- 국내 무역수지는 넉달째 적자이며, 국고채 금리, 달러환율, 유류세 등과 함께 금값 동향을 살펴봐야 한다.\\n- 금 가격은 최근 상승세를 보이고 있으며, 투자자들은 안전자산으로서의 금에 눈돌릴 가능성이 높다.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakao/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/urllib3/connectionpool.py:1061: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.btcc.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - Btcc', 'url': 'https://www.btcc.com/ko-KR/academy/financial-investment/gold-price-forecast-will-gold-be-3000-in-2023-what-about-gold-investment', 'content': '금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - BTCC BTCC 아카데미 추천 추천 암호화폐 입금 BTCC 계정으로 암호화폐 즉시 입금 USDT 무기한 선물 USDT로 주문하는 선물 거래 VIP 수수료 할인 VIP 등급에 따라 다양한 수수료 할인 혜택을 누리세요. 원화 입금 몇 초 만에 코인 구매 파트너 제휴 친구 추천하고 보상 받기 코인 교환 암호화폐를 실시간으로 다른 코인으로 교환 도움 센터 자주 묻는 질문 소식센터 공식 공지 목록 BTCC 아카데미 블록체인 및 암호화폐 튜토리얼 뉴스 암호화폐 시장의 최신 동향 최신 이벤트 친구 초대 이벤트 로그인 회원 가입 BTCC 아카데미 투자 가이드 금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 2023/10/11 글쓴이 : 연말을 맞아 금융기관별 내년 경기 전망 발표가 이어지는 가운데, 덴마크 투자은행(IB) 삭소방크 측이 내년 금값이 60% 넘게 오를 가능성 등 다소 황당해 보이는 전망을 내놨습니다.더 나아가 국제 금 가격이 내년 최대 4천 달러까지 치솟을 수 있다는 전망이 나왔습니다. 또한 최근 미국 벤치마크 금리인 10년물 국채금리 급락 속에 비수익성 자산인 금 등의 가격이 상승한 것도 눈길을 끌었습니다. 이스라엘-하마스 충돌이 지속되는 가운데 안전자산인 금과 은값이 오름세를 지속한 것도 주목받았습니다. 금 소개 금 투자를 위해 BTCC 계정을 등록하십시오. 거래 URL: https://www.btcc.com/ko-KR/trade/perpetual/GOLDUSDT 금(XAU/ GOLD )은 희귀한 귀금속으로 주로 매장량 및 투자용으로 사용되며 산업자재 및 장신구로도 널리 사용됩니다. 국제적으로는 일반적으로 온스(온스)를 단위로 사용합니다. 세계에서 가장 인기 있는 투자 상품 중 하나인 금은 주로 금 시장의 높은 투명성과 큰 거래량에 기인하므로 금 시장은 세계에서 가장 공정한 거래 시장이라고 할 수 있습니다. 금 가격을 결정할 수 있는 “은행가” 국가도 금 가격의 추세에 거의 영향을 미치지 않습니다. 금 값은 일반적으로 다양한 국제 정치, 경제, 전쟁, 자연 재해 및 비상 사태의 영향을 받으며 큰 변동이 있을 수 있으며 투자자는 변동 시 가격 차이를 사용하여 금을 거래하고 이익을 얻을 수 있습니다. 예를 들어, 2019년 말부터 전 세계적으로 확산되고 있는 코로나19 바이러스의 영향으로 세계 경제가 급격히 악화되어 원유 가격이 폭락하고 각국의 통화 정책이 완화되면서 물가가 2020년 8월 금 가격은 사상 최고치를 기록했습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 안드로이드 버전 다운로드 iOS 버전 다운로드 신규 유저 한정 이벤트(입금 및 거래 시 10,055USDT 보상) <<<< 금값 시세 및 최신 뉴스 이스라엘·하마스 간 무력 충돌 이슈 이스라엘·하마스 간 무력 충돌 이슈가 불거지자 안전자산에 대한 관심이 커지며 금이 다시 주목받고 있습니다. 10일 한국거래소에 따르면 한국투자신탁운용의 ‘ACE 골드선물 레버리지(합성 H)’ 상장지수펀드(ETF)가 이날 4.38% 상승했습니다. ‘KODEX 골드선물(H)’과 ‘TIGER 골드선물(H)’도 이날 각각 2.15%, 2.17% 올랐습니다. 금 시세가 전 거래일보다 1000~1500원 상승하면서 국내에 상장된 금 선물·현물 ETF가 일제히 올라섰습니다다. 디지털 자산 플랫폼 VDX 연구 책임자 그레타 유안(Greta Yuan)은 매체에 보낸 메일을 통해 “시장에서는 이스라엘-하마스 갈등이 인근 산유국으로 확산될 것을 우려한다. 이에, 투자자가 민감한 반응을 보이고 있다. 주말 사이에 중동의 지정학적 상황이 악화되고, 시장 위험 회피 심리가 상승하면서 금의 현물 가격이 약 1% 상승했다”라고 설명했습니다. 이달 들어 금 가격은 미국발 고금리 기조가 강화되는 분위기에서 올해 3월 이후 7개월 만에 최저치를 찍는 등 하락세였습니다. 하지만 전문가들은 전쟁 발발과 고금리 등 증시 변동성이 확대되는 상황에서 안전자산인 금에 대한 수요가 다시 증가할 것으로 내다봤습니다. 최근 금 가격이 떨어진 데 따른 ETF 하락세가 반등 계기를 마련했다는 것입니다. 실제로 전날 뉴욕상품거래소(COMEX)에서 금 선물 12월물은 1% 상승한 1864.30달러를 기록했고 이날 오후에는 더 뛰어 1875.35달러에 거래됐습니다. 美국채금리 ‘뚝’ 국내 금시세는 24K 순금 한돈에 부가세를 포함한 34만 7천원대로 미국 채권 수익률 발표 이후 급락했다가 지난 2거래일 동안 소폭 반등의 기미를 보이고 있습니다. 연준에 따르면 전날 로리 로건 댈러스 연방준비은행(연은) 총재, 필립 제퍼슨 연방준비제도(연준) 부의장 등 주요 연준 인사들이 “미국 국채금리 고공행진 시 추가 금리인상 요인이 줄어들 수 있다”는 온건적인 발언을 하고 이날엔 라파엘 보스틱 애틀란타 연은 총재가 “중앙은행은 더이상 금리를 올릴 필요가 없다”고 연설한 가운데\\xa0이날 미국 달러가치가 하락하고 미국 국채금리가 크게 떨어졌습니다. 달러가치 하락 속에 달러의 단기대체재인 금값이 올랐고 은값도 더불어 상승했습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 금 시세 전망, 2023년 금값 전망 4월 26일(현지시간) 덴마크 투자은행(IB) 삭소방크 측이 내년 금값이 60% 넘게 오를 가능성 등 전망했습니다. 이러한 견해가 삭소방크의 공식적인 견해는 아니며 소속 전략가들의 견해지만, 투자자들이 각국의 정책 결정에 따른 세계 경제 영향 등 시나리오를 점검하는 차원에서 의의가 있다는 게 CNBC 설명입니다. 삭소방크의 원자재 전략 부문장인 올레 한센은 현재 온스당 1,800달러인 금 현물 가격이 내년 중 67%가량 상승해 3,000달러를 넘길 가능성이 있다고 봤습니다.그러면서 세계화가 저물고 자급자족이 중시되는 ‘전시 경제’ 심리가 퍼지면서 개별 국가 입장에서 외국 화폐 보유보다 금이 더 매력적일 가능성이 있다고 봤습니다. 또 국가안보상의 우선순위에 대한 투자 확대, 전 세계 유동성 증가 등도 금의 매력도를 높이는 요인이라는 것입니다. 삭소방크의 최고투자책임자(CIO) 스테인 야콥센은 “원자재가 주도하는 경제 상황에서 더 나은 대안이 없는 만큼 금 수요가 늘어나도 놀랍지 않을 것”이라면서 “금값이 날아갈 것”이라고 말했습니다. 원자재시장 분석업체 CRU그룹의 키릴 키릴렌코 애널리스트는 미국 연방준비제도(Fed·연준)의 정책에 큰 영향을 받겠지만 온스당 1,900달러 수준을 예상했다는 것입니다. 또한 주르크 키에너 스위스 아시아 캐피탈 최고투자책임자(CIO)는 현지시간 22일\\xa0CNBC와의 인터뷰에서 “금 값이 내년 중에 적게는 온스당 2천500달러, 높게는 4천 달러까지 뛸 수 있다”고 전망했습니다. 현재 금 값은 온스당 1천820달러 수준에서 거래되고 있습니다. 키에너 CIO는 “내년도에 금 가격이 커다란 상승 움직임을 보일 가능성이 높다”며 “단지 10%나 20% 정도 오르는 그런 상승세가 아니라, 신고가를 경신할 수 있을 정도의 흐름을 보일 것”이라고 내다봤습니다. 또한 “금은 모든 중앙은행들이 보유하고 있는 유일한 자산이기도 하다”고 덧붙였습니다. 안드로이드 버전 다운로드 iOS 버전 다운로드 신규 유저 한정 이벤트(입금 및 거래 시 10,055USDT 보상) <<<< 금값 전망…월가 “2300달러도 가능” 일부 전문가들은 빠른 경기회복이 쉽지 않은 만큼 금 가격의 추가 상승을 전망하고 있습니다.월가 전문가들은 이같은 매크로 환경에 큰 변화가 없는 한 금 가격 랠리는 당분간 이어질 것으로 예상하고 있습니다. 씨티그룹 분석가들은 “금 가격은 경기침체 가능성 증가를 포함해 몇몇 매크로 요인에 힘입어 단기적으로 강세를 유지할 것으로 전망된다”면서 “온스당 2300달러까지 상승할 것으로 예상된다”고 말했습니다. CPM 그룹의 상품 컨설턴트들 역시 “경기침체를 연상시키는 경제 상황으로 인해 금 수요는 계속 증가할 것으로 보인다”며 “우리는 2024년 경기침체가 도래할 가능성이 높다고 보고 있다”고 설명했습니다. 기술적으로 보더라도 금 가격이 온스당 2050달러를 돌파할 경우 가파른 강세 흐름으로 이어질 가능성이 있다는 의견도 나옵니다. 헤라우스의 귀금속 딜러인 알렉산더 줌페는 “기술적인 관점에서 금 가겨은 강세를 유지하고, 혹은 더 높은 수준에서 안정될 가능성이 높다”며 “중요한 저항선인 2050달러를 넘어설 경우 금 가격은 사상 최고치까지 빠르게 치솟을 수 있다”고 분석했습니다. 엘컴텍은 몽골 현지에서 금이 매장된 광구 탐사권을 보유하고 있어 금 관련 주로 분류됩니다. 금값이 역대 최고치를 기록하는 등 강세를 이어가면서 엘컴텍의 주가에 매수세가 몰리는 것으로 풀이됩니다. 금 가격이 랠리를 지속한다면 금광 회사의 주가 또한 상승 흐름이 이어질 수 있습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 금 VS 비트코인, 금에 대한 수요 증가 미국 투자 대기업 골드만삭스는 장기적으로 금이 비트코인을 능가하는 것으로 보고 있다고 전했다.골드만삭스는 투자자들의 인플레이션(물가 상승)과 달러화의 가치 하락에 대비한 수단으로 금이 사용되고 있으며 실수요도 뒷받침되고 있다면서 이같이 분석했습니다. 반면 비트코인은 ‘고위험 기술주’와 같이 인식된다는 것입니다. 금은 투기성이 강한 비트코인과는 달리 비투기적이고, 긴축적 금융 환경의 영향을 덜 받아 투자 포트폴리오 다각화라는 측면에서 장기적으로 금이 비트코인보다 낫다고 골드만삭스는 평가했습니다. 반면 비트코인은 미래 실 사용처의 범위가 넓을 수 있다는 점으로 그간 가치가 평가됐는데, 금보다는 변동성이 크고 투기적이라고 설명했다. 또한 탈중앙화된 화폐를 찾으려는 투자자들의 요구가 비트코인 채택을 도왔으나, 앞으로의 금융 환경은 비트코인에 불리해질 것으로 내다봤습니다. 영국의 다국적 은행인 스탠다드차타드(Standard Charted)의 경우에도 최근 금과 비트코인의 관계를 조사했습니다. 스탠다드차타드는 내년 중 비트코인에 대한 수요가 금으로 몰릴 가능성이 존재한다고 짚었다. ‘디지털 금’으로 표현되는 비트코인에 대한 수요가 금으로 향할 경우, 시세가 30%가량 오를 수 있다는 게 스탠다드차타드의 견해였습니다. 스탠다드차타드는 “금은 가상화폐 생태계에 대한 갑작스러운 신뢰 하락과 문제로부터 앞으로 이익을 얻을 것이다”라며 “가상화폐 생태계는 몇몇 거래소가 인력 감축을 시행하는 등 계속 축소되고 있다”라고 언급했습니다. 또한 미국 벤치마크 금리인 10년물 국채금리 급락 속에 비수익성 자산인 금 등의 가격이 상승한 것도 눈길을 끌었습니다. 이스라엘-하마스 충돌이 지속되는 가운데 안전자산인 금과 은값이 오름세를 지속한 것도 주목받았습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 안드로이드 버전 다운로드 iOS 버전 다운로드 신규 유저 한정 이벤트(입금 및 거래 시 10,055USDT 보상) <<<< 금 값 금 시세 영향의 요인 1.미국 통화 정책 및 미국 달러 지수 금은 달러와 같은 안전자산이지만, 달러의 대체재이기도 합니다. 여기서 대체재란 다음과 같은 뜻을 가지고 있습니다. 대체재: 같은 효용을 주는 서로 다른 재화. 예를 들어 소고기와 돼지고기, 버터와 마가린은 대체재 관계임. 한 재화의 가격이 상승하여 수요가 줄어들 경우, 대체재의 수요는 증가함. 금은 달러의 대체재이므로, 달러의 안전성이 높아질수록 금의 매력은 떨어집니다. 또한 금은 달러로 표시되는 실물자산입니다. 따라서 금값에 가장 큰 영향을 미치는 것은 미국 연방준비제도(Fed) 이사회가 통제하는 통화정책입니다.즉, 국제 금시장에서 금은 달러화로 거래되고 있습니다. 만약 달러의 가치가 하락하면 다른 통화가치는 올라갈 것이고, 다른 통화를 보유한 사람들은 같은 금액으로 더 많은 금을 살 수 있으므로 금 수요가 늘어나서 금값이 올라가는 것입니다. 동시에 미국 달러 지수의 추세는 금 가격에도 영향을 미치고 금의 시세는 미국 달러와 음의 상관관계를 나타냅니다. 다른 요인이 고정적일 때에는 달러와 금의 가치는 반비례로 움직이는 경향을 보입니다. 그래서 달러가 오르면 금값이 떨어지고, 달러가 떨어지면 금값이 오릅니다. 2.금리 금은 안전자산으로써 채권과 대체재의 성격을 띠기도 합니다. 예를 들어, 미국 국채(미국 정부가 발행한 채권으로, 미국이 망하지 않는 한 손해를 보지 않는 채권) 금리가 올라간다고 생각합니다. 금을 보유한다고 해서 이자가 들어오진 않지만, 비슷한 안전자산인 미국 국채를 보유하고 있는데 많은 이자를 받을 수 있다면 기존에 금을 보유하고 있던 사람들도 금을 팔고 채권을 사려할 것입니다. 즉, 금리가 오르면 금보다 채권 수요가 늘기 때문에 금값은 떨어지는 경향 을 보이게 되는 것입니다. 금리변동에 인한 유동성으로도 금 가격 변동에 대해 설명할 수 있습니다. 만약 금리가 내려가서 시장에 돈이 많이 풀리게 되면 사람들은 주식, 부동산 같은 위험자산에도 투자하겠지만 금에도 일정 수요가 몰릴 것입니다. 3.수요와 공급 단순한 수요와 공급의 경제학도 실물 금 가격에 영향을 미칩니다. 권위있는 기관인 메탈스 포커스(Metals Focus)의 “골드 포커스 2022″에 따르면 금 생산의 총 유지 비용은 온스당 1,068달러입니다. 현재 금값이 여전히 높은 것으로 볼 때 생산자들은 여전히 상당한 이윤을 남길 수 있지만 새로운 광산 채굴은 단번에 할 수 있는 일이 아니며 공급을 늘리기도 쉽지 않습니다. 4. 국제환경 국제 환경도 금 가격에 큰 영향을 미칩니다. 예를 들어 일부 지역에서는 갑작스러운 전쟁이 발생하거나 일부 국가에서는 갑작스러운 파산이 발생하거나 일부 국가에서는 갑작스러운 주요 좋은 소식이나 나쁜 소식이 발생할 수 있습니다. 이것들은 금 가격에 큰 영향을 미칠 것입니다. 결국 금은 위험 회피 정서입니다. 특히 전쟁이 터지면 금값은 빠른 속도로 오를 수 있습니다. 5.투자심리 투자 심리도 금 가격에 어느 정도 영향을 미칠 것입니다 우리 투자 시장에서 많은 투자자들이 금 투자에 참여하기 시작했습니다. 투자 과정에서 대다수의 사람들은 예상 범위 내에서 목표를 설정합니다. 예를 들어, 특정 시점에 많은 사람들은 금이 급등할 것이라고 생각할 수 있습니다. 그 결과 많은 사람들이 금을 사기 시작했고 금을 공매도하려는 사람은 거의 없었습니다. 따라서 금값은 서서히 오르게 되는데 금값이 지속적으로 하락하기 시작하는 것을 많은 사람들이 보면 금값이 계속해서 하락할 것이라고 생각하는 사람들이 많다. 금 가격은 계속 하락했습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 금시세 확인 방법 국제 금시세 확인 국제 금 현물시장에서는 런던 금시장 현 협회에서 결정되는 금 시세가 국제 표준 금 시세로 인정되고 있습니다. 런던 금시장연합회는 런던에 있는 1987년에 설립된 금 거래 업자들의 조직으로 글로벌 은행 등의 주요 금시장 참여자 100여 개 회원사가 참가하는 기구입니다. 런던의 금시장은 그 거래량이 세계에서 가장 많은 곳으로 런던 금시장연합회가 금시장의 거래 동향과 국제 금값을 좌우한다고 볼 수 있습니다. 협회 내 5개의 가격 결정 회원들이 각각 가격을 발표하고, 런던 금시장연합회는 이를 기준으로 수량과 평균 가격을 참고해 국제 금값을 결정하여 발표합니다. 발표는 매일 오전 10시 30분, 그리고 오후 3시에 이뤄집니다. 이렇게 발표된 금값은 국제 대량거래 등에서 기준으로 적용됩니다. 런던 금시장협회 회원들은 24시간 전 세계 시장에서 실시간으로 거래하며, 이때 거래된 가격은 블룸버그통신 등의 정보제공업체를 통하여 전 세계 시장 참여자들에게 전달됩니다. 이렇게 전달된 실시간 국제 금 시세는 한국금거래소, 한국금거래소 쓰리엠, 삼성 금거래소 등의 거래소 외에도 구글, 네이버, 인제 스팅 닷컴, 블룸버그 등의 포털사이트에서도 확인할 수 있습니다. 국내 금시세 확인 국내 금 시세의 경우에는 국제 금시세 및 당일 환율을 기준으로 지식경제부의 인가를 얻은 한국귀금속유통협회에서 매일 오전 협회 기준가를 결정해 발표합니다. 이 기준 가는 매일 협회 사무국이 취합한 협회 이사 회원사의 기준 금 시세 중, 최고가와 최저가를 제외한 나머지 금 시세들의 평균으로 결정됩니다. 이때, 발표된 협회 기준가를 기준으로 이사 회원사들은 당일 중 국제 금값과 연동하여 회원사 간 거래를 진행하고, 또한 일정의 프리미엄을 더하여 이사 회원사의 소매시세를 결정하게 됩니다. 이와 같이 결정된 국내 금 시세는 한국 표준 금거래소에서 매일 오전 10시 30분 정도에 확인할 수 있습니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 안드로이드 버전 다운로드 iOS 버전 다운로드 신규 유저 한정 이벤트(입금 및 거래 시 10,055USDT 보상) <<<< 금 투자하는 방법 가상자산(암호화폐) 주식과 같은 자산에 대한 위험을 해지하기 위해서라도 금이나 달러에 대한 분산투자는 포트폴리오 상 필요합니다. 물론 금은방에 가서 금을 직접 사는 방법이 있다. 금을 현물로 사는 것이다. 그러나 보관하는 방법도 어렵고, 무엇보다 살 때 부가세 10%를 내야 합니다. 그렇기에 투자 목적으로 시세차익을 노리기 위해서는 굳이 금을 현물로 살 필요는 없습니다. 1)금 통장 만들기 은행의 금통장은 여러 금투자 수단 가운데 일반 금융소비자들이 가장 쉽게 접근할 수 있습니다. 일반 예·적금 통장에 돈을 입금하듯이 투자금을 예치해 두면 되는 간단한 구조인 이유에서다. 특히 금 실물을 인수할 필요가 없고 0.01g 단위로 거래되므로 소액 투자가 가능해 진입장벽이 비교적 낮습니다. 다만 금통장 역시 결국에는 투자 상품인 만큼 일반 예·적금 통장보다 복잡하고 위험성도 있습니다. 거래금액의 기준가가 국제 금 시세와 원달러 환율을 감안해 은행이 고시하는 1그램당 원화기준 금 가격에 의해 결정되기 때문입니다. 환율과 금 시세를 동시에 고려해야 하므로 아무래도 일반인에게는 조금 까다로운 투자입니다. 은행에서 판매하는 상품이지만 예금자 보호가 되지 않는다. 아울러 매매차익에 대해서는 배당소득세 15.4%를 부담해야 합니다. 또 수익이 2000만원을 초과하면 금융소득 종합 과세 대상에도 포함됩니다. 여기에 은행이 받아가는 1~2%가량의 수수료도 있다. 접근성은 좋지만 저렴한 투자 수단은 아닌 셈입니다. 2) KRX 금현물 계좌 개설 증권사에서 KRX(한국금거래소) 금현물 계좌를 개설해 직접 거래를 하는 방법이 있습니다. 거래 종목은 두 가지입니다. 1킬로그램 단위 상품과 100그램 단위 상품이 있습니다. 두 종목 중 하나를 선택하면 됩니다. 이는 직접 현물로 찾을 때 필요한 것이고, 현물로 찾지 않을 경우 두 종목을 선택하는 의미는 없습니다. 단지 가격 측면에서 1킬로그램 단위로 구매하는 게 좀 더 싸다. 종목을 선택했다면, 그 다음부터는 1그램 단위로 투자가 가능하다. 위탁 수수료는 0.2%~0.4% 정도가 붙습니다. KRX 금현물 계좌가 은행 금 통장보다 나은 점은 환율과 금 시세를 동시에 고려할 필요 없이 순수하게 금 시세만 보고 투자할 수 있다는 점입니다. 투자의 복잡성이 없습니다. KRX 금현물 계좌의 가장 큰 장점은 매매차익에 대해 비과세라는 점입니다.\\xa0매매차익이 금융소득 종합 과세에 합산되지 않는다는 것도 KRX 금현물 계좌의 장점입니다. 금과 같은 안전자산에 대한 투자는 소액으로 조금씩 꾸준히 투자하는 게 좋습니다. 소액투자로 금 투자를 해볼 의향이 있다면 현재로서는 KRX 금현물 계좌를 통한 투자가 가장 좋은 방법이라 할 수 있습니다. 3)금 펀드에 가입 금펀드는 금을 채굴하는 기업의 주식과 금 관련 상품에 투자하는 상품이다. 금펀드는 금값 이외에도 환율과 펀드 대상 기업실적 등의 영향을 받는다. 장점: 소액으로 투자할 수 있다. 금이 강세일 때 금값 상승률보다 더 많이 오를 수 있다 단점: 금값 외의 회사 경영 등의 이슈 리스크가 있다. 펀드수수료를 내야 한다. 매매차익에 배당소득세(15.4%)의 세금을 내야 한다 ?\\xa0 강력한 금값 상승이 예상될 때, 리스크를 감수하고 금값 수익률보다 더 많은 수익률을 내고 싶을 때 사용하는 방법이다. 4)금ETF 매입 금을 직접 사는 게 아니라 금값을 추종하는 ETF 상품 을 사는 방식이다. 금ETF 상품은 현물가격을 따라가는 상품과 선물가격을 따라가는 상품이 있다. 선물 ETF는 금이 아닌 선물이 기초자산이기 때문에 금을 보유하고 있지 않아요. 대신 롤오버 비용이 들어갑니다. 롤오버 비용은 선물 상품의 만기일을 연장하면서 내는 일종의 수수료라고 이해하면 된다. ⇒ 금 ETF란 무엇입니까? 국내 및 미국 금 ETF 소개, 금 구매 방법은? – BTCC 금ETF 장단점 •장점 증권사 계좌로 살 수 있다 연금계좌에 넣을 수 있는 ETF의 경우 비과세 혜택을 받을 수 있다 환헷지 옵션션에 따라 투자할 수 있고 인버스나 레버리지 투자가 가능하다. •단점:\\xa0매매차익에 배당소득세 15.4%가 붙고 보수수수료 및 거래수수료를 내야 한다. ?연금 통장을 통한 비과세 혜택을 받고 싶은 경우 적절한 방식이다. 5) BTCC에서 금 선물 투자 BTCC 계정에 USDT를 보유하는 것만으로 사용자는 금 , 은과 같은 가장 인기 있는 상품과 애플(Apple) , 메타(Meta) 및 마이크로소프트(Microsoft)와 같은 주식을 거래할 수 있습니다. 토큰화 선물은 토큰화 주식 및 토큰화 상품 선물 등을 포함하여 BTCC가 개발한 혁신적인 제품입니다. 사용자는 주식 및 상품 거래를 위해 다른 전통적인 주식이나 금거 거래소에서 계좌를 개설하지 않고도 USDT를 사용하여 BTCC에서 주식 및 금 상품을 사고 팔 수 있습니다. GOLD USDT 무기한 GOLDUSDT는 국제 금 가격과 1:1 비율로 고정되어 있습니다. 1 GOLD = 1 Gold 토큰 = 1 온스(oz)의 가격입니다. USDT로 GOLD를 매수 또는 매도할 수 있으며, 최대 150배 레버리지를 지원합니다. BTCC에 가입방법 BTCC는 모바일 사용자에게 개방되어 있는 오픈형 어플리케이션으로 QR 코드를 스캔하고 링크를 통해 iOS 및 안드로이드 앱을 다운로드할 수 있습니다. 애플 시스템 다운로드 QR 코드 / 안드로이드 시스템 다운로드 QR 코드 / BTCC에서는 비트코인선물거래로 데일리, 위클리, 무기한거래로 진입할 수 있습니다.자세한 거래 가이드는 BTCC에서 암호화폐 선물 계약 거래하는 방법 글을 참조하시면 됩니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! BTCC에서 금 선물 계약을 거래하는 절차 단계 1 : BTCC 앱을 다운로드하고 BTCC 계정에 로그인합니다. BTCC 거래소에 계정이 없으시다면 아래 링크를 통해 생성하실 수 있습니다. (신규 사용자 가입시 무조건\\xa010\\xa0USDD 보너스를 받을 수 있습니다) BTCC 가입하기 단계 2 :로그인 후 BTCC 앱에서 하단 메뉴에서 “시장”을 선택하고 원하는 선물 계약을 선택합니다. 신규 사용일 경우 선물 계약 투자하려면 BTCC 계정에 입금하셔야 합니다. 자세한 입금 절차는 BTCC 입금 안내ㅣ크게 2가지 방법을 알려 드립니다! – BTCC 을 참고해 주세요. 초보자일 경우 먼저 모의거래 부터 연습하는 것이 좋습니다. 단계 3 : 선물 계약 페이지를 클릭하면 코인의 추세 차트를 볼 수 있습니다. 매수/매도, 시장 가격, 레버리지 배수, 손실 정지 리스트를 설정한 후, 아래의 “매수”/”매도” 버튼을 눌러 거래 진행됩니다. 단계4: 목표 계약을 선택하여 매수 (롱) 또는 매도 (숏)를 합니다. 단계5: 시장가 주문으로 매수하고 적절한 레버리지 비율을 선택하고 필요한 로트(lot) 크기를 입력합니다. 또한 이익 실현 및 스탑리밋 예상가격을 설정할 수 있습니다. 단계6: 매입 후 개시 성공한 화면을 보게 됩니다. 금에 투자하시려면 👇 BTCC 가입하고 15,055USDT 받으세요 📢 BTCC 가입만으로 10 USDT 획득! 안드로이드 버전 다운로드 iOS 버전 다운로드 신규 유저 한정 이벤트(입금 및 거래 시 10,055USDT 보상) <<<< BTCC 거래소 선택한 이유 BTCC 거래소는 2011년 설립되어 세계에서 가장 오래 운영 중인 암호화폐 거래소입니다. 영국, 홍콩 등에 본부를 두어 설립 후 단 한차례도 해킹 피해 사례가 없는 강력한 보안을 자랑해 투자자가 보다 더 안전하게 거래할 수 있는 환경을 제공하고 있습니다. 미국 금융감독국(MSB) , 캐나다 금용감독국 (MSB), 새롭게 리투아니아의 암호화페 라이센스 를 확득했습니다.라이센스를 취득 할 만큼 믿고 신뢰할 수 있는 거래소입니다.또한 고객 자산관리에 측면은 BTCC는 다중 서명 콜드 월렛을 사용합니다. 모든 사용자의 자산은 다중 서명 콜드 월렛에 저장되어 제3자, 사이버 해커 또는 도둑의 무단 접근을 방지합니다.업계 최고의 자산관리로 믿음직한 거래소입니다. 암호화폐 외에 토큰화 주식 및 상품 BTCC거래소 에 토큰화 주식 및 상품이 출시되었습니다. BTCC 계정에 USDT를 보유하는 것만으로 사용자는 금, 은과 같은 가장 인기 있는 상품과 테슬라(Telsa) , 구글(Google) 애플(Apple) , 메타(Meta) 및 마이크로소프트(Microsoft) 와 같은 주식을 거래할 수 있습니다. 이러한 토큰화 선물은 토큰화 주식 및 토큰화 상품 선물 등을 포함하여 BTCC가 개발한 혁신적인 제품입니다. 사용자는 주식 및 상품 거래를 위해 다른 전통적인 주식 거래소에서 계좌를 개설하지 않고도 USDT를 사용하여 BTCC에서 주식 및 상품을 사고 팔 수 있습니다. 풍부한 유동성 및 호가창(오더북) 거래소 선택의 중요한 포인트는 유동성입니다. 소형거래소들의 경우는 호가창이 얇아서 매수, 매도가 원하는 호가에서 원활치 않은 경우가 있습니다. BTCC 는 코인게코 기준 세계 3위의 거래소로 풍부한 유동성을 자랑합니다. 또한 BTCC 는 코인 시세를 확인할 수 있는 호가창을 추가하며 풍부한 유동성을 확인하실 수 있습니다. 거래소의 거래량이 많을 때 거래자는 제품의 현재 가격에 가까운 가격으로 포지션이 오픈될 가능성이 큼 거래소의 거래량은 주문가격 편차와 보류 주문량을 통해 평가할 수 있습니다. 호가창에 있는 각 가격 간의 가격 차이가 작을수록, 사용자의 주문이 최상의 가격으로 체결될 때 사용자의 거래 손실이 줄어들 수 있습니다. 보류 중인 주문량이 클수록 거래소는 사용자로부터 모든 시장의 주문이 체결될 수 있습니다. BTCC는 최고의 거래량을 자랑합니다. 대량 주문도 최상의 가격으로 체결 가능합니다! 사용자는 모의 거래 기능을 사용하여 BTCC의 거래량 테스트가 가능합니다. 더 많은 내용은 BTCC 뉴스 및 BTCC 아카데미 에 확인 할 수 있습니다. BTCC 가입혜택 BTCC는 신규 유저들을 위해서 가입하면\\xa0보너스를 받을 수 있는 이벤트를 준비합니다. 지금 가입 및 거래하면 최대 15,055USDT 를 받을 수 있습니다. 또한 입금 금액에 대비 VIP 등급 높을 수 있습니다 ( 입금↑\\xa0 VIP↑ ) . VIP 가 되면 거래 수수료 할인 또 다른 헤택을 누릴 수 있습니다. BTCC 가입하고 보너스 받으세요 BTCC거래소 12년 보안 무사고 암호화페 라이센스를 확득한 믿음직한 거래소 전세계 규제 준수 세계 최저 수수료 (0.03%) 최대 150 배 레버리지 제공 선물 거래량 기준 세계 5 위 무료 모의 거래 가능 원화입금 지원 24×7 시간 한국어 고객서비스 제공 최신 시장 동향 및 투자 교육 제공 PC/모바일 거래 가능 다양한 이벤트 금과 관련페이지: 골드만삭스 “장기적으로 금이 비트코인 능가”…비트코인 횡보세 – BTCC 금 시세 전망,약세 보이는 금 시장…올해 4분기 금 시세 반등 가능할까? – BTCC 11조 달러 세계 金 시장, 블록체인으로 디지털화 모색 중 – BTCC BTCC에서 선물계약 하기: BTCC에서 암호화폐 선물 계약 거래하는 방법 – BTCC 암호화폐 선물 거래란? BTCC 4가지 선물계약 유형 소개! – BTCC 레버리지/ 마진을 통해 비트코인을 늘리는 방법 (뉴비) (btcc.com) 비트코인 레버리지/마진거래 하는법 및 관련 거래소 순위 – BTCC 비트코인 마진 (거래소) 란? 그의 거래하는 방법(레버리지) 소개 – BTCC 마진거래 이해하기,마진 및 레버리지 어떤 관계? 마지건래 진행하면? – BTCC 레버리지/ 마진을 통해 비트코인을 늘리는 방법 (뉴비) (btcc.com) 시바이누 코인 가격 시세 전망(예측) 2022-2030 – BTCC [2022] 글로벌 Top 10 암호화폐 비트코인 거래소 순위 소개 및 추천 – BTCC 투자 가이드: 구글 알파벳A,C 주가 및 투자 궁금이라면,여기 구글 주식 전면 소개 테슬라 주식 사도 됩니까? 테슬라 주기 미래 전망 및 투자 방법 소개 – BTCC 미국 3대 주가 지수: 다우,나스닥, S&P500 지수 소개 – BTCC 달러 인덱스(DXY, 달러지수)란?그는 코스피, 암호화폐와 어떤 관계인가? – BTCC 투자 교육 더 보기: 도지코인(DOGE) 시세 분석 및 가격 전망 2023~2030 2023년 에이다(ADA) 코인 가격 전망 (예측) 및 시세 분석 솔라나 SOL 가격 전망(예측) 분석 2023~2030…솔라나 좋은 투자입니까? 도지코인(DOGE) 시세 분석 및 가격 전망 2023~2030 이오스 가격 시세 분석 및 전망 2023 년 이오스(EOS)란 무엇입니까?ㅣ 코인 소개 DYDX 코인이란? 어떤 용도에 사용합니까? (feat. dydx 거래소) 테라 코인 전망, 테라 루나 클래식 LUNC 코인 전망 2023년 알고랜드(ALGO)란? 이는 좋은 투자인가요? 향후 전망은? 메인넷 출시한 앱토스(Aptos)란 무엇입니까? 코스모스(ATOM)란?ㅣ코인 소개 바이낸스 코인(BNB) 가격 시세 및 전망 2023~2030 시바이누 코인 시세 분석, SHIB 코인 가격 전망 2022-2030 바이너리엑스(BinaryX)란? 인기 게임 보기…BNX 코인 구매 방법은? 셀러네트워크(CELR)란? CELR 코인 시세 및 향후 전망 살펴보기 셴투(Shentu)란? CTK 코인 정보 및 향후 전망 살펴보기 카르테시(Cartesi)란? CTSI 코인 시세 및 향후 전망은? 프랙스 쉐어(FXS)란? FXS 코인 시세 분석 및 가격 전망 2023 기프토(Gifto)란? GFT 코인 정보 및 향후 전망 알아보기 지엠엑스(GMX)란? GMX 거래소 및 GMX 코인 정보에 대한 알아보기 재스미(JasmyCoin)란? JASMY 코인 정보 및 가격 전망 2023 트레이더 조(JOE)란? JOE 코인 정보 및 향후 전망은? 리도다오(LDO)란? LDO 코인 정보, 시세 및 향후 전망 2023 리니어파이낸스(LINA)란? LINA 코인 소개 및 향후 전망 보기 리퀴티(LQTY)란? LQTY 코인 정보 및 향후 시세 전망 보기 메이커(MKR; Maker)코인이란? MKR 코인 시세 및 전망 살펴보기 아이젝(iExec RLC)이란? RLC 코인 정보 및 전만 알아보기 로켓풀(Rocket Pool)란? RPL 코인 시세, 스테이킹 및 전망 알아보기 스타링크(Starlink)란? STARL 코인 정보 및 가격 전망 2023 스타게이트 파이낸스(STG)란? STG 코인 시세 및 가격 전망 2023 스톰엑스(StormX)란? STMX 코인 시세 분석 및 가격 전망 2023 텔러(Tellor)란? TRB 코인 시세 및 가격 전망 2023 테라클래식USD란? USTC 코인 300% 급등…미래 전망은? 뱅코르(Bancor)란? BNT 코인 가격 시세 및 향후 전망 보기 스몰 러브 포션(SLP)란? SLP 시세 분석 맟 가격 전망 2023 스텝앱(Step App)이란? FITFI 코인 및 스텝앱 가입 방법 알아보기 뱃저다오(Badger DAO)란? BADGER 코인 90% 급등…향후 전망은? 퍼페츄얼프로토콜(PERP)이란? PERP 코인 시세 및 가격 전망은? 하모니(Harmony)란? ONE 코인 정보 및 가격 전망 2023 코티(COTI)란? COTI 코인 정보 및 가격 전망 2023 바이코노미(Biconomy)란? BICO 코인 정보 및 향후 전망은? 도라팩토리(DoraFactory)란? DORA 코인 정보 및 가격 전망 보기 갓즈 언체인드(GODS)게임이란? GODS 코인 시세 및 전망 보기 스웨트코인(SWEAT)이란? 스웨트코인 사용법 및 SWEAT 코인 가격 전망은? 스펠 토큰(SPELL)란? 아브라카다브라 머니 및 SPELL 코인 가격 전망 보기 플레어(Flare)란? FLR 코인 시세 및 향후 전망은? 문빔(Moonbeam)란? GLMR 코인 시세, 향후 전망 살펴보기 가입 하고 암호화폐 투자를 시작하세요 App Store 또는 Google Play 를 통해 BTCC 앱 다운로드 BTCC 소셜 미디어 팔로우 스캔하여 다운로드 PDF 다운로드 금 통장 금값 시황 금 값 시세 금 시세 확인 이스라엘·하마스 전쟁 금 투자 금시세 금값 금 시세 금 값 전망 금 현물 댓글 더 보기 댓글 달기 귀하의 이메일 주소는 공개되지 않습니다. *는 필수 항목입니다. 댓글 내용* 이름* 이메일 주소* 제출 인기 선물 RSS 한국어RSS Exchange for a better future 제품 무기한 계약 카피트레이딩 서비스 친구 초대 VIP 프로그램 실시간 거래 데이터 마켓 가이드 공식 블로그 도움 센터 뉴스 BTCC 아카데미 NFT 마켓 BTCC 정보 BTCC 정보 BTCC 업데이트 소식센터 BTCC 및 보안 이용 약관 고객센터 온라인 고객 센터 문제 보고 & 보상 [email\\xa0protected] 커뮤니티 빠른 링크 비트 코인 (BTC) 선물 구매 이더 리움 (ETH) 선물 구매 라이트 코인 (LTC) 선물 구매 비트 코인 캐시 (BCH) 선물 구매 이오스 (EOS) 선물 구매 리플 (XRP) 선물 매수 에이다 (ADA) 선물 구매 스텔라 (XLM) 선물 구매 대시 (DASH)선물 구매 위험 알림: 암호화폐 거래는 전망이 밝은 산업으로 수익을 크게 낼 수도 있지만 그만큼 위험이 따릅니다. 특히 레버리지 거래는 이익이 확대되는 동시에 리스크도 수반해 커지니 암호화폐 시장, 레버리지 거래, 거래 규칙 등을 충분히 이해하고 거래하시기 바랍니다. 거래할 때는 감당이 가능한 리스크 범위 내에서 거래하는 것을 추천합니다. 세계에서 가장 오래 운영한 거래소, 안정적 운영 12주년 © 2011- 2023 BTCC.com. All rights reserved', 'summary': '- 금값이 하락하던 추세에서 다시 상승할 가능성이 나타나고 있음. \\n- 2023년에는 금값이 60% 이상 상승할 수 있다는 전망이 나왔으며, 최대 4천 달러까지 치솟을 수도 있다고 예측됨.\\n- 금은 안전자산으로서 금리 하락과 경제 불안 요소에 영향을 받아 가격이 크게 변동할 수 있으며, 투자자는 변동 시 차익을 얻을 수 있음.'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for s in searchResults:\n",
    "  result = task(s)\n",
    "  print(result)\n",
    "  record = {\"title\":result['title'],\n",
    "            \"url\":result['url'],\n",
    "            \"summary\":result['summary'],\n",
    "            \"content\":result['content']}\n",
    "  results.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프',\n",
       " 'url': 'https://kr.mitrade.com/commodities/gold-investment/trade-gold-commodity',\n",
       " 'summary': '- 2023년 금값은 긍정적으로 전망되고 있으며, 전문가들은 금값 상승을 예상하고 있다.\\n- 올해 금값 평균 전망치는 온스당 1850 달러에서 1950 달러로 조정되었으며, 몇 주 내에 2075달러까지 상승할 것으로 전망된다.\\n- 금은 안전 자산으로 여겨지며, 금테크를 포함한 다양한 투자 방법이 제시되고 있다.',\n",
       " 'content': \"2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 Mitrade 홈페이지로 가기 Mitrade 웹으로 거래 Mitrade 앱으로 거래 Mitrade 앱으로 거래 QR코드 스캔하여 다운로드 편집 정책 회사소개 인사이트 핫이슈 더보기 FX 지수 원자재 미국주식 암호화폐 2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 작성자 Mitrade 팀 | 갱신시간 2023-3-30 07:38 36914 글 목록 2023년 금값 전망 금시세 전망 금시세의 영향 요인 국제 금값 금시세 그래프 / 국내 금값 금시세 금투자 방법 - 원자재 투자 금투자 해야 하는 이유 -실시간 금값 시세 그래프 확인하기: -금테크 코로나19\\r\\n 이후, 역사적인 재정부양책에 금융시장에 유동성이 넘쳐나며 거의 모든 자산의 가격이 올랐습니다. 하지만 투자 환경은 시시각각 \\r\\n바뀌곤 합니다. 예를 들어, 중앙은행이 계속해서 양적완화를 지속해도 경제가 성장하지 못하면 갑자기 긴축정책으로 전환할 수도 \\r\\n있습니다. 따라서 하나의 포트폴리오만 가져가기보다는 여러 자산에 투자해 두고, 변곡점을 맞을 때마다 포트폴리오 내 비중을 조정하는\\r\\n 것이 가장 이상적인 투자법입니다. 원자재 상품 중 금은\\r\\n 투자자가 많이 찾는 상품이며, 역사적으로 대표적인 안전 자산 및 분산 투자의 핵심 자산으로 선호되어 왔습니다. ‘헤지펀드의 \\r\\n대부’로 불리는 레이달리오는 금이 역사적으로 포트폴리오의 균형을 맞추는데 일조해왔음을 강조, 자산 포트폴리오에서 금에 대한 비중을\\r\\n 8% 정도 유지하는 것이 이상적이라고 했습니다. 최근\\r\\n 화제가 되었던 코인 투자를 보시면, 사실 실체가 없는 것들입니다. 주식도 마찬가지입니다. 회사의 가치, 혹은 코인의 가치에 \\r\\n투자를 하지만 직접 그것을 보유할 수는 없습니다. 이러한 투자 종목들은 기본적으로 불안정한 가격 변동을 가지고 있습니다. 그러나 금은 현물거래 입니 다. 그런 만큼 가격 변동성이 크지 않아 안전자산으로 불리고 있기도 합니다. 최근 코로나 사태나 경제의 몰락 등 많은 변수들이 걱정되신다면 금과 같은 안전 자산을 현물거래 하는 것도\\xa0좋은 방법 중 하나라고 생각합니다. 최대 등락 상품 외환 원자재 외환 거래 주식 상품명 매수 매도 가격등락 2023년 금값 전망 금시세 전망 ▲2023/01/22 기준 금 펀드 수익률 (이미지 출처 : 머니투데이) - 금 시세 올 한해 금테크는 전문가들이 추천한 투자 중 하나인데요. 금은 대표적인 안전자산으로서 경기가 불안정할 때 수요가 늘어나기 때문입니다. 실제로 올해 1월 말 국제 금값이 20% 이상 급등하면서 작년 6월 16일 이후 6개월 만에 최고 가격을 기록했습니다. 이처럼 금값이 계속 상승하는 이유는 주식시장의 부진, 경기 침체 우려 확대, 미국 금리인상 속도 조절에 대한 기대감, 각국 중앙은행의 탈달러 현상 등의 이유가 복합적으로 작용한 것으로 분석됩니다. 지난 3월 22일(현지 시간)에는 온스당 1960 달러 선에서 거래되며 사상 최고치를 돌파했습니다. 최근 미국 실리콘밸리은행(SVB), 스위스 크레디트스위스(CS) 사태로 금융 시장의 불확실성이 확대되면서 안전 자산이 금으로 투자 수요가 몰린 것입니다. 세계 금 협회에 따르면 2022년 금 수요는 전년 대비 18% 증가하면서 11년 만에 최고치를 기록했습니다. 특히 각국 중앙은행의 금 매입 규모가 55년 만 최고 수준인 1136톤에 달하면서 전체 수요도 많이 늘어났습니다. 3월 20일에는 장중 온스당 2000 달러를 웃돌면서 1년 만 최고치에서는 다소 하락한 수준이긴 하지만 추가 상승에 대한 기대가 상당한데요. - 2023년 금값 전망 실제로 많은 전문가들은 올해 금 전망을 긍정적으로 바라보고 있습니다. 시장조사기관 피치 솔루션은 올해 금값 평균 전망치를 온스당 1850 달러에서 1950 달러로 상향 조정했습니다. 또한, “몇 주 내 금값이 2075달러까지 오를 것”이라고 전망했습니다. 금융서비스업체 CMC마켓은 미 연준의 금리인상 중단 시기가 빨라지면 금값은 또 급등할 것으로 보고, 올해 금값 전망치를 온스당 2500~2600 달러로 제시했습니다. 미국 경제 전문 뉴스 CNBC는 현재 중앙은행이 계속해서 금을 매입하는 것 역시 금값의 추가 상승 여력을 뒷받침한다고 덧붙였습니다. 대표적인 금 ETF 중 하나인 AuAg ESG 골드마이닝 상장지수펀드(ETF)를 운용하는 에릭은 올해 금값 전망이 역대 최고가를 찍을 수 있다고 봤습니다. 그는 온스당 2100 달러 돌파를 예상했는데요. 그는 “중앙은행의 금리 인상 속도 조절 가능성이 커지고 있고, 이는 몇 년 안에 폭발적인 금값 상승으로 이어질 것”이라고 설명했습니다. 스탠다드차타드 스탠다드차타드는 최근 발행한 보고서에서 올해 금 가격에 대해 ‘현재보다 30% 높은 온스당 2250달러까지 오를 가능성이 있다’고 내다봤습니다. 글로벌 투자정보기업 스위스아시아캐피탈의 수장을 맡고 있는 쥬르그 키네르 최고투자책임자(CIO)는 작년 말 2023년 금값을 온스당 4000 달러로 전망하기도 했습니다. 귀금속거래기업 MKS PAMP에 소속된 한 애널리스트는 외신 인터뷰에서 ‘미국 연준의 금리 인상 속도 조절 여부와는 상관없이 금값이 오를 것’이라고 언급했습니다. 김진영 키움증권 연구원은 최근 미 연준의 금리 인상 속도 조절에 대한 기대감을 나타나면서 달러 약세가 지속되고 있으며, 통상 실질금리와 달러와 반대로 움직이는 금이 이 영향으로 상승하고 있다고 현재의 금 상승 배경을 설명했습니다. 그러면서도 경기 불확실성이 계속 되면서 달러 및 금리는 현재의 수준을 크게 벗어나지 않을 것으로 분석했습니다. ‘리스크 및 인플레 헤지 수단으로서 금의 매력도가 달러 및 금리 안정화와 함께 다시 부각될 것’이라며 이와 동시에 현물 투자보다 더 용이한 금 ETF 투자 인기가 높아질 것으로 판단했습니다. 하지만 낙관적인 전망만 있는 것은 아닙니다. 김소현 대신증권 연구원은 금 상승세가 일시적이라는 데 무게를 실었습니다. 현재 금이 급상승한 이유는 안전자산 선호, 달러 약세, 실질금리 하락, ETF 자금 유입 등이 복합적으로 작용했기 때문이며, 전고점을 넘길 수 있는 재료가 부족하다고 판단했기 때문입니다. 그는 2020년 8월 온스당 2063 달러를 기록한 과거와 지금을 비교하면서 ‘현재는 SVB 사태 이후 은행 파산 우려가 있는데다 연준 정책의 불확실성이 커지면서 달러 강세 압력이 커졌다고 볼 수 있다’고 언급했습니다. 다시 강달러로 돌아서면서 상대적으로 금 가격 상승이 제한될 것이라는 얘깁니다. 금시세의 영향 요인 국제 금 시세/금값은 금이 기본적으로 미 달러화로 거래되므로 미국 실질금리, 달러 가치 그리고 수급요인이 결합하여 결정된다고 볼 수 있습니다. 이렇게 결정된 국제 금 시세에 달러 원 환율, 국내 수급요인이 더해져 국내 금 시세가 결정됩니다. 이 중 가장 큰 영향력을 주는 3가지 결정요인에 대해서 알아보겠습니다. ▶ 미국 실질금리： 금은 다른 상품과는 달리 이자를 지급하지 않는 실물 자산입니다. 따라서\\r\\n 금 투자 시 가장 큰 기회비용은 미국 실질금리이기 때문에 이는 국제 금 시세와 반대로 움직입니다. 여기서 실질금리는 명목금리에서\\r\\n 예상 인플레이션을 차감한 개념인데, 이 실질금리가 오르면 (명목금리 하락 혹은 물가 상승의 경우) 금 가격이 상승합니다. 미 \\r\\n국채와 금은 둘 다 최후의 안전자산이라는 인식이 투자자들 사이에 아직도 있기에 서로 대체재로 작용하기도 합니다. 즉,\\r\\n 미국 금리가 하락하면 미국채 투자 매력이 낮아지고, 금의 매력이 상대적으로 높아지는 효과가 있습니다. 또한, 물가 상승 시 \\r\\n인플레이션 헤지 용도로 금 투자 수요가 증가합니다. 실제로 코로나19의 미국 내 확산이 본격화되던 2020년 3월부터 미국 \\r\\n실질금리는 본격적인 마이너스 구간에 진입했고, 이에 금 가격은 2019년 초 1,283달러/온스에서 지난해 8월 역사적 고점인 \\r\\n2,067달러/온스까지 약 61% 상승했습니다. ▶ 미 달러 가치: 금은 달러 표시 자산으로 결국, 금 가격은 달러화에 대한 상대가치를 나타냅니다. 금이 변치 않는 절대가치를 갖고 있다고 가정할 경우 달러 표시 금 가격은 미 달러 가치와 반비례할 수밖에 없고 역사적으로도 금값과 달러 가치는 반비례의 상관관계가 있습니다. ▶ 수요와 공급 요인: 금은 공급이 거의 일정하고 변수가 없으며 수요의 경우 그 출처는 크게 귀금속 제품의 재료, 투자자산, 반도체 등 전자제품의 소재, 각국 중앙은행의 매수 및 보유로 나눌 수 있습니다. 금의\\r\\n 귀금속 수요는 가격과 역의 상관관계를 갖고 있으며 전자제품 및 중앙은행 수요 역시 전체 수요 내에서 차지하는 비중이 상대적으로 \\r\\n작거나, 그 기간이 단기라 가격 결정에 큰 영향을 미치지는 않습니다. 실질적으로 금의 가격을 결정짓는 것은 투자 자산 수요입니다. 국부\\r\\n 금펀드, 연기금, 개인투자자 등 다양한 투자자들이 매크로 지표를 기반으로 자산 배분 차원의 대규모 매매를 하다 보니 금에 대한 \\r\\n가격 결정력이 높은 편입니다. 이는 실물 시장 고유의 소유와 공급 현황이 가격과 상당히 밀접한 관계를 갖는 다른 원자재와 금이 \\r\\n차별되는 점입니다. 국제 금값 금시세 그래프 / 국내 금값 금시세 매일 변동하는 국제 및 국내 금 시세의 가격 결정 방법과 실시간 금값을 확인할 수 사이트, 그리고 금값 확인 시 주의해야 할 점을 알아보겠습니다. ▶ 국제 금시세 결정 방법 및 실시간 국제 금시세 확인 국제\\r\\n 금 현물시장에서는 런던 금시장 현 협회에서 결정되는 금 시세가 국제 표준 금 시세로 인정되고 있습니다. 런던 금시장연합회는 \\r\\n런던에 있는 1987년에 설립된 금 거래 업자들의 조직으로 글로벌 은행 등의 주요 금시장 참여자 100여 개 회원사가 참가하는 \\r\\n기구입니다. 런던의 금시장은 그 거래량이 세계에서 가장 많은 곳으로 런던 금시장연합회가 금시장의 거래 동향과 국제 금값을 \\r\\n좌우한다고 볼 수 있습니다. 협회\\r\\n 내 5개의 가격 결정 회원들이 각각 가격을 발표하고, 런던 금시장연합회는 이를 기준으로 수량과 평균 가격을 참고해 국제 금값을 \\r\\n결정하여 발표합니다. 발표는 매일 오전 10시 30분, 그리고 오후 3시에 이뤄집니다. 이렇게 발표된 금값은 국제 대량거래 등에서\\r\\n 기준으로 적용됩니다. 런던\\r\\n 금시장협회 회원들은 24시간 전 세계 시장에서 실시간으로 거래하며, 이때 거래된 가격은 블룸버그통신 등의 정보제공업체를 통하여 전\\r\\n 세계 시장 참여자들에게 전달됩니다. 이렇게 전달된 실시간 국제 금 시세는 한국금거래소, 한국금거래소 쓰리엠, 삼성 금거래소 등의\\r\\n 거래소 외에도 구글, 네이버, 인제 스팅 닷컴, 블룸버그 등의 포털사이트에서도 확인할 수 있습니다. ▶ 국내 금시세 결정 방법과 실시간 국내 금 시세 확인 국내\\r\\n 금 시세의 경우에는 국제 금시세 및 당일 환율을 기준으로 지식경제부의 인가를 얻은 한국귀금속유통협회에서 매일 오전 협회 기준가를\\r\\n 결정해 발표합니다. 이 기준 가는 매일 협회 사무국이 취합한 협회 이사 회원사의 기준 금 시세 중, 최고가와 최저가를 제외한 \\r\\n나머지 금 시세들의 평균으로 결정됩니다. 이때, 발표된 협회 기준가를 기준으로 이사 회원사들은 당일 중 국제 금값과 연동하여 \\r\\n회원사 간 거래를 진행하고, 또한 일정의 프리미엄을 더하여 이사 회원사의 소매시세를 결정하게 됩니다. 이와\\r\\n 같이 결정된 국내 금 시세는 한국 표준 금거래소에서 매일 오전 10시 30분 정도에 확인할 수 있습니다. 또한 실시간 국내 금 \\r\\n시세는 삼성 금거래소, 한국 표준 금거래소, 종로금거래소 등의 거래소 외에도 네이버, 다음 등의 포털사이트에서 확인할 수 \\r\\n있습니다. 거래소나 사이트에 따라서 해당 무게에 따른 금값도 알 수 있으므로 투자자가 사용하기 편한 사이트를 찾아서 이용하면 \\r\\n됩니다. ▶ 국제 금시세 국내 금 시세 확인 시 주의할 점 (무게 단위) 투자자들이\\r\\n 거래소 및 사이트에서 현재 금값을 확인할 때 한 가지 주의할 부분이 있습니다. 국제 금시장에서는 관행적으로 금의 무게를 \\r\\n‘온스(ounce)’라는 단위로 표기하지만, 이는 실제로는 ‘트라이온스'를 의미합니다. 1트라이온스는 31.1034768g입니다.\\r\\n 반면, 온스가 무게를 나타낼 때, 1온스는 28.349523ｇ이므로, 1트라이온스가 1온스보다 9.7% 더 무겁습니다. 국제 금시장에서는 1g, 1kg, 1ounce(트라이온스를 의미)에 대한 가격을 보여주며 국내 금시장에서는 무게에 따른 금 값 및 금 한 돈(3.75g)에 따른 금 가격을 보여줍니다. 금투자 방법 - 원자재 투자 투자자는\\r\\n 여러 가지 경로를 통해 금 투자할 수 있습니다. 그중 초보투자자들도 들어보셨을 방법은 골드 바(금괴) 구입일 것입니다. 금괴는\\r\\n 은행이나 금은방에서 구입 가능하며, 상속 증여 등을 위해 오래전부터 꾸준히 이어져 온 방식입니다. ▶ 첫 번째 방법은 바로 금 ETF 투자방법 및 금 \\xa0펀드입니다. -먼저 ETF에 대해서 소개 드리겠습니다. ETF는 주식시장에 상장되어 있는 금 인덱스펀드를 시가대로 매입과 매도를 합니다. 금 가격에 연동되지만 펀드로 운용됩니다. 또한\\r\\n 금 \\xa0ETF는 주식 매매와 같이 실시간으로 거래가 가능하죠. 그래서 금 \\xa0ETF는 주식 투자를 병행하시는 분이나 적극적인 \\r\\n투자자분들이 하시기에 적합합니다. 대표적인 투자 방법으로는 미래에셋ETF 에서 할 수 있는 타이거 \\xa0ETF가 있습니다. 자산\\r\\n 규모는 약 74억이고 2019년에 상장한 금 \\xa0ETF입니다. 타이거 ETF는 자산의 거의 대부분을 금 현물이 아닌 금 선물에 \\r\\n투자하는 상품으로서 수수료는 연 0.39%이고 일일 거래량이 비슷한 방법인 KODEX 골드 선물보다 적지만 총보수가 약 40%가량\\r\\n 더 저렴해서 미래에셋 ETF를 이용하는 것이 많이들 찾으시는 방법입니다. 무엇보다 금 ETF 투자가 좋은 방법인 이유는 적은 \\r\\n돈으로도 손쉽게 투자가 가능하기 때문입니다. 하지만 괴리율이 발생할 수 있다는 것이 단점으로 꼽힙니다. -다음은 금 펀드입니다. 펀드는\\r\\n ETF와 같이 금에 직접 투자하는 방법은 아닙니다. 금 펀드는 금 관련 사업, 금광회사 등의 주식에 투자하는 상품입니다. 사실 \\r\\n이렇기에 금 시세의 변동에 따라 함께 움직이는 경향이 크지만 항상 그런 것은 아니기 때문에 조금 다릅니다. 하지만 주식을 보유하는\\r\\n 것이기 때문에 주가 상승이나 배당금 등의 수익을 기대해 볼 수도 있고 안정적으로 분산투자나 적립식 투자가 가능하다는 것이 큰 \\r\\n장점입니다. ▶ 그다음\\xa0 금통장입니다. 하지만,\\r\\n 일반 투자자들은 금괴 구입보다 금통장을 개설해 골드뱅킹을 하는 것이 훨씬 편리합니다. 금통장은 투자자의 계좌잔액이 국제 금 \\r\\n시세에 연동되어 움직이는 간접투자 상품입니다. 투자자가 금통장 개설 후, 본인 계좌에 예금을 넣어두면 국제 금 시세에 따라 계좌 \\r\\n잔액이 자동으로 움직입니다. 여기서, 은행은 고객 예금으로 직접 금을 사들이는 대신 외국 은행에 달러로 예치합니다. 따라서, \\r\\n투자자는 원화를 예금하지만 투자자의 계좌 보유금은 국제 금 시세와 환율에 연동돼 자동으로 바뀌는 것입니다. 흔히들\\r\\n 골드 뱅킹이라고 불리기도 합니다. 현재 국민은행과 신한은행, 그리고 우리은행에는 개설이 가능하며 현금을 입금하면 그날의 금 \\r\\n시세에 맞는 금이 통장 안에 적립이 되는 구조로 이루어져 있습니다. 이 개념이 흔히 알고 계신 개념과는 많이 달라 헷갈리실 것 \\r\\n같아서 예를 들어보겠습니다. 예를 들어서 현재의 금 시세가 1g당 65000원이라고 가정합니다. 금통장 안에 65000원을 \\r\\n입금한다면 통장 안에 1g의 금이 채워지게 되는 것입니다. 이후\\r\\n 출금을 원할 때는 출고 당일의 시세에 맞춰 현금이 인출되게 됩니다. 만약 금 시세가 70000원이라면 5000원의 수익이 \\r\\n생겨나는 것입니다. 물론 거래 누적에 따른 은행 수수료와 소득세가 발생하긴 합니다만 아주 적은 돈으로도 가능하고 굉장히 간단하고 \\r\\n쉬운 투자 방법 중 하나입니다. 투자자는\\r\\n 은행을 통해 빠르게 금통장을 개설할 수 있으며, 소단위인 0.01g 단위로 금을 구입해서 통장에 적립할 수 있습니다. 또한, \\r\\n환매가 쉽고 실물거래가 없지만 현금 인출과 금 현물 인출이 가능해 비교적 자유롭게 금 투자에 참여할 수 있다는 장점이 있습니다. \\r\\n현재 시중 여러 은행들이 금통장 상품을 보유하고 있으며 가입대상과 기한, 금액에 대한 제한이 없습니다. 다만, 금통장을 통해 거래\\r\\n 시, 투자자는 해당 은행에 거래 수수료 및 매매차익에 대한 세금을 내야 합니다. 이 외에도 투자자는 한국거래소를 통하거나 금 현선물 가격에 따라 수익률이 정해지는 상장지수펀드에 가입, 카드사를 통한 금 거래 등도 가능합니다. ▶ 마지막으로는 CFD차액결재거래입니다. CFD（contract for difference）차액결제거래는 파생 상품에서 자산의 시가와 종가의 차이를 이용한 계약입니다. 선물과 비슷한 개념입니다. 실제 기초자산을 보유하지 않고 진입 가격과 청산 가격의 차액을 현금으로 결제하는 상품이라고 생각하시면 됩니다. 이 CFD는 다양한 원자재에서도\\r\\n 가능한데 금도 예외는 아닙니다. 만약 투자자분이 금 가격이 앞으로 상승할 것이라고 생각한다면 이 금 CFD를 구매하시면 됩니다.\\r\\n 시세가 오른다면 그 차익을 얻어낼 수 있습니다. 하지만 선물과 비슷하게 투자 원금에 대한 소실 위험이 크고 변동성이 큰 \\r\\n상품입니다. -금값 시세 실시간 그래프： 지금까지\\r\\n 금 현물거래와 금통장, 금 ETF, 금 CFD 그리고 금 펀드까지 다양한 금 투자 방법을 알아보았습니다. 안전 투자의 대표격인 \\r\\n상품인 만큼 다양한 투자 방법이 존재했습니다. 그렇다면 지금부터는 금 시세가 어떤 상황인지, 그리고 앞으로의 금 시세는 어떻게 \\r\\n될지 알아보겠습니다. Ad 금투자 해야 하는 이유 최근에는\\r\\n 시장의 변동성 증가와 인플레이션 헤지 수요가 몰리면서 미 달러화와 골드 가격이 동반 상승하기도 했습니다. 여기에 보태 금을 \\r\\n거래하는 사람들의 심리에 큰 영향을 받기도 합니다. 아시다시피 연말에는 많은 뉴스들이 있었습니다. 코로나의 변종인 오마 크론의 \\r\\n유행과 미국의 물가 상승, 연방 준비 위원회의 테이퍼링 정책, 원자재 가격의 상승, 국제 유가의 상승, 달러 가치의 변화와 금 \\r\\n생산지 생산량의 변화, 가상화폐 시장의 불안정성 증가 등등 많은 변수들이 현재의 금 시세를 만들어 냈다고 생각하시면 됩니다. 그중에서\\r\\n 가장 주목을 받는 요인은 바로 인플레이션입니다. 시장에서도 물가가 앞으로 더욱 오를 것이라고 전망하고 있습니다. 어쨌든 \\r\\n국제적으로 불안감을 증폭시키는 요인들로 인해 불안정성이 높아지고 이로 인해 안전 자산이 주목을 받기 시작한 것입니다. 많은\\r\\n 변수들이 금 시세를 만들어가고 있습니다. 하지만 대부분의 전문가들은 앞으로도 금값이 오를 것이라고 예상하고 있습니다. 가장 큰 \\r\\n이유는 역시 인플레이션입니다. 이 인플레이션이 국제적으로 계속해서 일어나고 있고 테이퍼링 정책까지 쓴다는 뉴스에도 시장은 믿고 \\r\\n있지 않습니다. 이러한 인플레이션은 불안정성을 만들어내고, 이는 안전 자산에 대한 관심으로 이어지게 됩니다. 현재 금 시세를 올린\\r\\n 원인이 내년에도 계속될 것이라고 예상하는 것입니다. 시장을\\r\\n 예측하기 힘든 요즘 같은 상황에서 주식, 채권, 외환, 원자재 등 투자자는 여러 자산이 적절히 배분된 포트폴리오를 구성해야 \\r\\n안정적인 수익을 낼 수 있습니다. 원자재 투자에서 금은 역사적으로 주식 및 국채와 낮은 상관관계를 지니고 있어 유용한 포트폴리오 \\r\\n분산 수단입니다. 따라서 투자자는 미 금리의 방향, 위험자산이지만 금의 대체재로 거론되는 비트코인 가격 움직임 및 인플레이션 우려\\r\\n 등을 종합적으로 감안해, 증시 조정을 방어하는 차원에서 일정 비중의 금 투자는 반드시 필요합니다. 장기투자의 관점에서 가격 \\r\\n조정은 분할매수 시기이니, 투자자들에게는 지금이 그 기회가 될 것입니다. 지금까지\\r\\n 금 투자 방법과 금 시세 전망에 대해서 알아보았습니다. 앞으로 불안정성이 커진다면 금 시세가 오른다는 것이 조금 아이러니하기는 \\r\\n합니다. 안전한 투자 방법으로는 사실 최고입니다. 장기 투자를 좋아하시는 분들이라면 꼭 추천드립니다. 많은 분들이 전에는 금을 \\r\\n단지 현물로서 보관하고 투자하는 방법만 아셨다면 지금은 더욱 다양한 방법이 존재한다는 것을 아실 겁니다. 무엇보다 안정적이고 \\r\\n다양한 투자 방법을 가지고 있는 금을 한번 투자해 보시는 것도 좋다고 생각합니다. Ad 면책사항: 본문의 내용은 편집자의 개인관점이며, Mitrade의 공식입장을 대표하지 않으며, 투자 권유 또는 제안의 목적이 아닙니다. 글의 내용은 단지 참고용이며, 독자는 본문의 내용을 어떠한 투자의 근거로 삼아서는 안됩니다. Mitrade는 이 글에 근거한 어떠한 거래 결과에 대해서도 책임을 지지 않습니다. Mitrade는 이 글의 정확성을 보증하지 않습니다. 투자 결정을 하기 전에 반드시 위험을 숙지할 수 있도록 독립적인 재무 상담사의 조언을 구해야 합니다. 차액 결제 거래(CFD)는 레버리지 상품이며, 귀하의 투자 원금 손실이 발생할 수 있습니다. CFDs 거래는 모든 사람에게 적합하지 않을 수 있습니다. 신중하게 투자하시기 바랍니다. Mitrade 팀 Mitrdae 분석 팀 구성원은 모두 전문적인 경제 지식과 긴 업계 경력을 지닌 전문가로, Mitrdae 투자 클래스에서 여러분의 투자 성공에 도움이 될 만한 투자 및 거래에 대한 다양한 지식과 경험을 공유합니다. 작성자 홈페이지 > 인기글 가장 많이 읽음 최신 발표 오리지널 거래 분석 가장 많이 읽음 최신 발표 데이터가 없습니다 글 목록 2023년 금값 전망 금시세 전망 금시세의 영향 요인 국제 금값 금시세 그래프 / 국내 금값 금시세 금투자 방법 - 원자재 투자 금투자 해야 하는 이유 핫이슈 더보기 금투자 금테크 금 투자 금 ETF 금시세 금값 시세 금 통장 금 테크 귀금속 투자 선물 투자 Ad 인사이트 글로벌 투자자에게 다양한 양질의 컨텐츠 제공 회사소개 핫이슈 더보기 FX 지수 원자재 미국주식 암호화폐 위험 고지 : 트레이딩 결과 전액 원금 손실이 발생할 수 있습니다. 장외파생상품이 모든 투자자에게 적합한 것은 아닙니다. Mitrade의 서비스를 사용하시기 전에 법적 공시 문서를 살펴보고 관련 위험을 숙지하십시오. 여러분은 기초 자산에 대한 소유권이나 이해 관계가 없습니다. 더보기\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>summary</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프</td>\n",
       "      <td>https://kr.mitrade.com/commodities/gold-invest...</td>\n",
       "      <td>- 2023년 금값은 긍정적으로 전망되고 있으며, 전문가들은 금값 상승을 예상하고 ...</td>\n",
       "      <td>2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 Mitrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까?</td>\n",
       "      <td>https://qlehfl0321.tistory.com/entry/2023년-금-시...</td>\n",
       "      <td>- 2023년 금 시세 동향을 살펴보자. EU와 러시아에서는 금 수입을 금지했고, ...</td>\n",
       "      <td>2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까? 본문 바로가기 검색...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - ...</td>\n",
       "      <td>https://www.btcc.com/ko-KR/academy/financial-i...</td>\n",
       "      <td>- 금값이 하락하던 추세에서 다시 상승할 가능성이 나타나고 있음. \\n- 2023년...</td>\n",
       "      <td>금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프   \n",
       "1                2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까?   \n",
       "2  금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://kr.mitrade.com/commodities/gold-invest...   \n",
       "1  https://qlehfl0321.tistory.com/entry/2023년-금-시...   \n",
       "2  https://www.btcc.com/ko-KR/academy/financial-i...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  - 2023년 금값은 긍정적으로 전망되고 있으며, 전문가들은 금값 상승을 예상하고 ...   \n",
       "1  - 2023년 금 시세 동향을 살펴보자. EU와 러시아에서는 금 수입을 금지했고, ...   \n",
       "2  - 금값이 하락하던 추세에서 다시 상승할 가능성이 나타나고 있음. \\n- 2023년...   \n",
       "\n",
       "                                             content  \n",
       "0  2023년 금값 금시세 전망: 실시간 국제 국내 금값 금시세 그래프 Mitrade ...  \n",
       "1  2023년 금 시세 동향 살펴보기 금투자, 아직 메리트 있을까? 본문 바로가기 검색...  \n",
       "2  금 시세 전망 2023: 추락하던 금값, 다시 날개 다나? 금 투자 방법 소개 - ...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# df.to_excel(\"/content/drive/MyDrive/datas/exported.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_novel_makeEx1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_gpt_api(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    max_token: int = 500,\n",
    "    temperature: float = 0.8,\n",
    ") -> str:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_token,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<등장 인물>\\n        - James: 야망에 찬 사업가\\n        - Kong: 생물학 연구소에서 일하는 유명한 박사, James 와 연인관계\\n        - Bab: 질투 많은 경쟁사 사장\\n</등장 인물>\\n\\n<뉴스 기사>\\n기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\\n한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\\n경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\\n휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\\n대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\\n</뉴스 기사>\\n\\n<등장 인물> 과 <뉴스 기사> 를 소재로 새롭고 흥미진진한 스릴러 소설 아이디어를 한 문단으로 작성해줘'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = read_prompt_template('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_text.txt')\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'James는 야망이 불타는 사업가로, 성공을 꿈꾸며 경쟁사와의 싸움에 힘쓰고 있다. 그러나 그의 연인인 Kong은 생물학 연구소에서 일하는 유명한 박사로서, 자신의 일에 집중하고 있다. 그 사이에서 경쟁사인 Bab는 질투에 눈이 멀어 James의 성공을 막기 위해 계속해서 음모를 꾸미고 있다. 그런데 어느 날, James는 뉴스 기사에서 국내 주유소의 휘발유와 경유 가격이 계속해서 하락하고 있는 것을 알게 된다. 이로 인해 James는 새로운 사업 아이디어를 고민하게 되고, 그 아이디어가 경쟁사와의 전쟁에서 이길 수 있는 열쇠가 될지도 모른다. 이렇게 James, Kong, Bab과 함께 펼쳐지는 스릴러 소설이 시작된다.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_gpt_api(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제임스는 기름값이 급격히 하락하는 상황에 새로운 사업 계획을 세우게 된다. 그의 연인인 공과 함께 새롭게 개발된 생물연료 기술을 활용해 기름 가격의 풍선효과를 무너뜨리려 한다. 이 사업이 성공한다면 그는 경제계의 새로운 왕좌에 오를 수 있지만, 경쟁사인 Bab의 질투와 방해가 그의 계획을 위협한다. 특히, 경유 가격이 반등할 것이라는 예상 속에서 제임스와 콩, 그리고 Bab 사이의 강렬한 사업 전쟁이 벌어진다. 이들의 운명은 과연 어떻게 될 것인가? 그리고 이 모든 것은 과연 누구의 손에서 시작된 것일까?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_gpt_api(prompt_template, model='gpt-4', max_token=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<등장 인물>\\n{characters}\\n</등장 인물>\\n\\n<뉴스 기사>\\n{news_text}\\n</뉴스 기사>\\n\\n<등장 인물> 과 <뉴스 기사> 를 소재로 새롭고 흥미진진한 {genre} 소설을 작성해줘'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = read_prompt_template('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_template.txt')\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel(genre, characters, news_text) -> dict[str, str]:\n",
    "    prompt_template = read_prompt_template('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_template.txt')\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        genre=genre,\n",
    "        characters=characters,\n",
    "        news_text=news_text,\n",
    "    )\n",
    "\n",
    "    return {\"results\": request_gpt_api(prompt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GENRE = \"Comedy\"\n",
    "DEFAULT_CHACRACTERS = [\n",
    "    {\"name\": \"James\", \"characteristics\": \"야망에 찬 사업가\"},\n",
    "    {\"name\": \"Kong\", \"characteristics\": \"생물학 연구소에서 일하는 유명한 박사, James 와 연인관계\"},\n",
    "    {\"name\": \"Bab\", \"characteristics\": \"질투 많은 경쟁사 사장\"},\n",
    "]\n",
    "DEFAULT_NEWS_TEXT = \"\"\"기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
    "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
    "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
    "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
    "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': '제임스는 야망에 찬 사업가로, 항상 성공과 돈을 추구하는 사람이었습니다. 그는 경제 뉴스를 즐겨보고, 특히 유가 변동에 민감하게 반응했습니다. 그의 연인인 콩은 생물학 연구소에서 유명한 박사로 일하고 있었습니다. 그녀 또한 제임스와 마찬가지로 돈을 좋아하고, 자신의 연구를 위해 자금이 필요하다는 생각을 가지고 있었습니다.\\n\\n한편, 제임스의 경쟁사인 밥은 질투 많은 경쟁사의 사장이었습니다. 그는 제임스의 성공을 부러워하고, 언제나 그를 따라잡기 위해 악수를 덮어놓는 계획을 세우고 있었습니다. 밥은 유가의 변동에도 민감하게 반응하며, 자신의 경쟁사가 어려움에 처하지 않도록 꼼꼼히 관찰하고 있었습니다.\\n\\n어느 날, 제임스는 뉴스 기사에서 휘발유와 경유의 가격이 계속해서 하락하고 있다는 것을 알게 되었습니다. 그는 이 기회를 이용해 성공적인 사업을 계획하고자 했습니다. 그리고 이를 통해 밥을 이기기 위한 기회를 잡을 수 있다는 생각에 마음이 설레기 시작했습니다.\\n\\n한편, 콩은 제임스의 변덕스러운 성격에 짜증을 내면서도 그를 사랑하고 있었습니다. 그녀는 제임'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_novel(DEFAULT_GENRE, DEFAULT_CHACRACTERS, DEFAULT_NEWS_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_novel_makeEx2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CUR_DIR = os.path.dirname(os.path.abspath('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_template1'))\n",
    "STEP1_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/1_extract_idea.txt\")\n",
    "STEP2_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/2_write_outline.txt\")\n",
    "STEP3_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/3_write_plot.txt\")\n",
    "WRITE_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/4_write_chapter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kakao/vs_code/AI_LLM/AI_LLM'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_template1/1_extract_idea.txt'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP1_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel(genre, characters, news_text, chapter_number) -> dict[str, str]:\n",
    "    \n",
    "    context = {}\n",
    "\n",
    "    #아이디어 뽑기\n",
    "    novel_idea_prompt_template = read_prompt_template(STEP1_PROMPT_TEMPLATE)\n",
    "    novel_idea_prompt = novel_idea_prompt_template.format(\n",
    "        genre=genre,\n",
    "        characters=characters,\n",
    "        news_text=news_text\n",
    "    )\n",
    "    context['novel_idea'] = request_gpt_api(novel_idea_prompt)\n",
    "    print(\"=\"*200)\n",
    "    print('idea:')\n",
    "    print(context['novel_idea'])\n",
    "\n",
    "    #뽑은 아이디어로 아웃라인 작성\n",
    "    novel_outline_prompt_template = read_prompt_template(STEP2_PROMPT_TEMPLATE)\n",
    "    novel_outline_prompt = novel_outline_prompt_template.format(\n",
    "        genre=genre,\n",
    "        characters=characters,\n",
    "        news_text=news_text,\n",
    "        novel_idea=context['novel_idea']\n",
    "    )\n",
    "    context['novel_outline'] = request_gpt_api(novel_outline_prompt)\n",
    "    print(\"=\"*200)\n",
    "    print('outline:')\n",
    "    print(context['novel_outline'])\n",
    "\n",
    "    #아웃라인 소설 플롯 작성\n",
    "    novel_plot_prompt_template = read_prompt_template(STEP3_PROMPT_TEMPLATE)\n",
    "    novel_plot_prompt = novel_plot_prompt_template.format(\n",
    "        genre=genre,\n",
    "        characters=characters,\n",
    "        news_text=news_text,\n",
    "        novel_idea=context['novel_idea'],\n",
    "        novel_outline=context['novel_outline']\n",
    "\n",
    "    )\n",
    "    context['novel_plot'] = request_gpt_api(novel_plot_prompt)\n",
    "    print(\"=\"*200)\n",
    "    print('plot:')\n",
    "    print(context['novel_plot'])\n",
    "\n",
    "    #플롯으로 소설 챕터 작성\n",
    "\n",
    "    context['novel_chapter'] = request_gpt_api(novel_plot_prompt)\n",
    "\n",
    "    write_prompt_template = read_prompt_template(WRITE_PROMPT_TEMPLATE)\n",
    "    contents = write_prompt_template.format(\n",
    "        genre=genre,\n",
    "        characters=characters,\n",
    "        news_text=news_text,\n",
    "        novel_idea=context['novel_idea'],\n",
    "        novel_outline=context['novel_outline'],\n",
    "        novel_plot=context['novel_plot'],\n",
    "        chapter_number=chapter_number\n",
    "    )\n",
    "\n",
    "    return {'results': request_gpt_api(contents)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GENRE = \"Thriller\"\n",
    "DEFAULT_CHACRACTERS = [\n",
    "    {\"name\": \"James\", \"characteristics\": \"야망에 찬 사업가\"},\n",
    "    {\"name\": \"Kong\", \"characteristics\": \"생물학 연구소에서 일하는 유명한 박사, James 와 연인관계\"},\n",
    "    {\"name\": \"Bab\", \"characteristics\": \"질투 많은 경쟁사 사장\"},\n",
    "]\n",
    "DEFAULT_NEWS_TEXT = \"\"\"기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
    "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
    "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
    "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
    "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================================================================================================\n",
      "idea:\n",
      "박사인 Kong은 야망에 찬 사업가인 James와 밀접한 연인 관계에 있습니다. 그러나 경쟁사인 Bab는 James에게 질투를 품고 있습니다. 이때, 휘발유와 경유의 가격이 지속적으로 하락하는 소식이 전해집니다. James는 이를 악용하여 경쟁사에게 큰 타격을 줄 계획을 세우지만, Bab는 질투심을 더해 James와 Kong의 관계를 이용하여 그들의 계획을 망치기 위해 음모를 꾸미고 있습니다. 이와 관련하여 Kong은 생물학 연구소에서 중요한 발견을 했다는 사실을 알게 되고, 그 발견이 경쟁사에게 매우 중요하다는 것을 알게 됩니다. 따라서 James와 Kong은 Bab의 음모와 경쟁사의 악의적인 계획을 막기 위해 힘을 합쳐 사건을 해결해야 합니다. 이 흥미진진한 스릴러는 야망과 질투, 이중간첩과 음모가 얽힌 중대한 사건의 풀이를 그립니다.\n",
      "========================================================================================================================================================================================================\n",
      "outline:\n",
      "아웃라인:\n",
      "1. James는 야망에 찬 사업가로서 경제적인 성공을 꿈꾸고 있다. 그의 비밀 연인인 Kong은 유명한 생물학 연구소에서 일하며 중요한 발견을 했다는 사실을 알게 된다.\n",
      "2. Bab는 경쟁사의 사장으로 질투심이 많아 James와 Kong의 관계를 알게 되고, 그들의 계획을 망치기 위해 음모를 꾸미기 시작한다.\n",
      "3. 휘발유와 경유의 가격이 지속적으로 하락하는 소식이 전해진다. James는 이를 악용하여 경쟁사에 큰 타격을 주기 위한 계획을 세우지만, Bab는 James와 Kong의 관계를 이용하여 그들의 계획을 무너뜨리기 위한 음모를 꾸미고 있다.\n",
      "4. Kong은 중요한 발견이 경쟁사에게 매우 중요하다는 것을 알게 되고, James와 함께 Bab의 음모와 경쟁사의 악의적인 계획을 막기 위해 힘을 합친다.\n",
      "5. 이 스릴러 소설은 야망과 질투, 이중간첩과 음모가 얽힌 중대한 사건의 풀이를 그린다. James와 Kong은 Bab의 음모에 맞설 때마다 대립과 위험한 상황에 직면하며 스토리는 긴장감과 스릴을 유지한다.\n",
      "6. 결말에서 James와 Kong은 Bab와 경쟁사를 속이고, Kong의 발견을 안전하게\n",
      "========================================================================================================================================================================================================\n",
      "plot:\n",
      "Chapter 1: 야망의 시작\n",
      "- James는 야망에 찬 사업가로서 경제적인 성공을 꿈꾸고 있다.\n",
      "- 비밀 연인인 Kong은 유명한 생물학 연구소에서 일하며 중요한 발견을 했다는 사실을 James에게 알리게 된다.\n",
      "- Kong의 발견이 경쟁사에게 매우 중요하다는 것을 James도 인지하게 되고, 그들은 경쟁사에게 큰 타격을 줄 계획을 세우기 시작한다.\n",
      "\n",
      "Chapter 2: 음모의 시작\n",
      "- Bab는 경쟁사인 사장으로 질투심이 많아 James와 Kong의 관계를 알게 된다.\n",
      "- Bab는 James와 Kong의 계획을 망치기 위해 음모를 꾸며 사건을 조작하고, James와 Kong은 Bab의 일탈을 알아차리게 된다.\n",
      "- James는 경쟁사에 대한 악의적인 계획을 세우고, Kong은 Bab의 음모를 막기 위해 힘을 합친다.\n",
      "\n",
      "Chapter 3: 위험한 전개\n",
      "- 휘발유와 경유의 가격이 지속적으로 하락하는 소식이 전해진다.\n",
      "- James는 이를 악용하여 경쟁사에 큰 타격을 주기 위한 계획을 세워 진행하고, Kong은 Bab의 음모를 막기 위해 숨은 발견을 안전하게 보호한다.\n",
      "- 그러나 Bab는 James와 Kong의 관계를 이용하여 그들의 계획을 무너뜨리기 위한 음모를 꾸미고 있다.\n",
      "\n",
      "Chapter 4: 결말과 해결\n",
      "- James와\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': '야망의 시작\\n\\nJames는 머릿속에 항상 야망을 품고 있는 사업가였다. 그는 자신의 경제적인 성공을 꿈꾸며 하루하루를 보내고 있었다. 그러던 어느 날, James는 자신의 비밀 연인인 Kong으로부터 중요한 소식을 전해들었다. Kong은 유명한 생물학 연구소에서 일하며, 최근에 중요한 발견을 했다는 것이었다.\\n\\n이 소식에 James는 가슴이 두근거리며, Kong과 함께 야망적인 계획을 세우기 시작했다. Kong의 발견이 경쟁사에게 매우 중요하다는 것을 James도 인지하게 되었고, 그들은 경쟁사에게 큰 타격을 줄 수 있는 방법을 모색하기 시작했다.\\n\\nJames와 Kong은 서로에게 의지하며, 야망이 가득한 눈빛으로 서로의 비전을 공유했다. James는 경제적인 성공을 통해 자신의 목표를 달성하고, Kong은 자신의 발견이 인정받아 과학의 역사에 이름을 새기는 것을 꿈꾸었다.\\n\\n그러나 이들의 야망과 비전을 간섭하려는 존재가 있었다. 경쟁사의 사장인 Bab는 질투심이 많은 사람으로서 James와 Kong의 관계를 알게 되었다. 자신의 사업을 보호하기 위해 Bab는 James와 Kong의 계획을 망치기 위해 음모를 꾸며 사건을 조작하기 시작했다'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_novel(DEFAULT_GENRE, DEFAULT_CHACRACTERS, DEFAULT_NEWS_TEXT, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_novel_makeEx3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pprint import pprint\n",
    "\n",
    "def generate_novel(genre, characters, news_text) -> dict[str, str]:\n",
    "\n",
    "  writer_llm = ChatOpenAI(temperature=0.1, max_tokens=500, model='gpt-3.5-turbo')\n",
    "  writer_prompt_template = ChatPromptTemplate.from_template(\n",
    "      template=read_prompt_template('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_text.txt'))\n",
    "  writer_chain = LLMChain(llm=writer_llm, prompt=writer_prompt_template)\n",
    "  result = writer_chain(dict(\n",
    "      genre=genre,\n",
    "      characters=characters,\n",
    "      news_text=news_text\n",
    "  ))\n",
    "  pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'characters': [{'characteristics': '야망에 찬 사업가', 'name': 'James'},\n",
      "                {'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계',\n",
      "                 'name': 'Kong'},\n",
      "                {'characteristics': '질투 많은 경쟁사 사장', 'name': 'Bab'}],\n",
      " 'genre': 'Thriller',\n",
      " 'news_text': '기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 '\n",
      "              '하락했습니다.\\n'\n",
      "              '한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 '\n",
      "              '하락한 리터당 1,575.8원을 기록했습니다.\\n'\n",
      "              '경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\\n'\n",
      "              '휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\\n'\n",
      "              '대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 '\n",
      "              '가격이 반등할 가능성이 있다\"고 전망했습니다.',\n",
      " 'text': 'James는 야망에 찬 사업가로, 경쟁사인 Bab에게 질투를 받고 있다. 그러던 중 James는 연인인 생물학 연구소의 '\n",
      "         '유명한 박사인 Kong과 함께 휘발유와 경유 가격이 계속 하락하는 뉴스 기사를 보게 된다. James는 이를 계기로 '\n",
      "         '경쟁사인 Bab에 대항하기 위해 특히 경유 가격이 반등할 가능성을 염두에 두고 야망을 키우는 스릴러 소설 아이디어를 '\n",
      "         '생각해낸다. 이 아이디어는 James와 Kong이 경쟁사와의 치열한 경쟁 속에서 휘발유와 경유 가격의 변동을 이용하여 '\n",
      "         '사업을 성공시키는 과정을 그린다. 이 소설은 경쟁과 야망, 사랑과 배신이 얽힌 스릴러적인 요소를 담고 있어 독자들에게 '\n",
      "         '흥미진진한 이야기를 제공할 것이다.'}\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_GENRE = \"Thriller\"\n",
    "DEFAULT_CHACRACTERS = [\n",
    "    {\"name\": \"James\", \"characteristics\": \"야망에 찬 사업가\"},\n",
    "    {\"name\": \"Kong\", \"characteristics\": \"생물학 연구소에서 일하는 유명한 박사, James 와 연인관계\"},\n",
    "    {\"name\": \"Bab\", \"characteristics\": \"질투 많은 경쟁사 사장\"},\n",
    "]\n",
    "DEFAULT_NEWS_TEXT = \"\"\"기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
    "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
    "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
    "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
    "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\"\"\"\n",
    "\n",
    "generate_novel(DEFAULT_GENRE, DEFAULT_CHACRACTERS, DEFAULT_NEWS_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel(genre, characters, news_text) -> dict[str, str]:\n",
    "\n",
    "  writer_llm = ChatOpenAI(temperature=0.1, max_tokens=500, model='gpt-3.5-turbo')\n",
    "  writer_prompt_template = ChatPromptTemplate.from_template(\n",
    "      template=read_prompt_template('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_text.txt'))\n",
    "  writer_chain = LLMChain(llm=writer_llm, prompt=writer_prompt_template, output_key='output')\n",
    "  result = writer_chain(dict(\n",
    "      genre=genre,\n",
    "      characters=characters,\n",
    "      news_text=news_text\n",
    "  ))\n",
    "\n",
    "  return {'result' :result['output']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 'James는 야망에 찬 사업가로, 경쟁사인 Bab에게 질투를 받고 있다. 그러던 중 James는 연인인 생물학 연구소의 유명한 박사인 Kong과 함께 휘발유와 경유 가격이 계속해서 하락하는 뉴스 기사를 보게 된다. 이를 계기로 James는 Bab에 대항하기 위해 휘발유와 경유 가격 하락을 이용하여 자신의 사업을 성공시키기 위한 야망을 키우는데, Kong과의 연인관계를 이용하여 그들의 연구소를 통해 혁신적인 에너지 솔루션을 개발하고 경쟁사를 압도하는 스릴러 소설 아이디어가 탄생한다.'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_novel(DEFAULT_GENRE, DEFAULT_CHACRACTERS, DEFAULT_NEWS_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1211_novel_makeEx4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CUR_DIR = os.path.dirname(os.path.abspath('/Users/kakao/vs_code/AI_LLM/AI_LLM/prompt_template1'))\n",
    "STEP1_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/1_extract_idea.txt\")\n",
    "STEP2_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/2_write_outline.txt\")\n",
    "STEP3_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/3_write_plot.txt\")\n",
    "WRITE_PROMPT_TEMPLATE = os.path.join(CUR_DIR, \"prompt_template1/4_write_chapter.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(llm, template_path, output_key):\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=ChatPromptTemplate.from_template(\n",
    "            template=read_prompt_template(template_path),\n",
    "        ),\n",
    "        output_key=output_key,\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "def generate_novel(genre, characters, news_text) -> dict[str, str]:\n",
    "\n",
    "  writer_llm = ChatOpenAI(temperature=0.1, max_tokens=300, model=\"gpt-3.5-turbo-16k\")\n",
    "\n",
    "  # 아이디어 뽑기 체인 생성\n",
    "  novel_idea_chain = create_chain(writer_llm, STEP1_PROMPT_TEMPLATE, \"novel_idea\")\n",
    "\n",
    "  # 아웃라인 작성 체인 생성\n",
    "  novel_outline_chain = create_chain(writer_llm, STEP2_PROMPT_TEMPLATE, \"novel_outline\")\n",
    "\n",
    "  # 플롯 작성 체인 생성\n",
    "  novel_plot_chain = create_chain(writer_llm, STEP3_PROMPT_TEMPLATE, \"novel_plot\")\n",
    "\n",
    "  # 소설 챕터 작성 체인 생성\n",
    "  novel_chapter_chain = create_chain(writer_llm, WRITE_PROMPT_TEMPLATE, \"output\")\n",
    "\n",
    "  preprocess_chain = SequentialChain(\n",
    "      chains=[\n",
    "          novel_idea_chain,\n",
    "          novel_outline_chain,\n",
    "          novel_plot_chain,\n",
    "      ],\n",
    "      input_variables=[\"genre\", \"characters\", \"news_text\"],\n",
    "      output_variables=[\"novel_idea\", \"novel_outline\", \"novel_plot\"],\n",
    "      verbose=True,\n",
    "  )\n",
    "\n",
    "  context = dict(\n",
    "      genre=genre,\n",
    "      characters=characters,\n",
    "      news_text=news_text\n",
    "  )\n",
    "  context = preprocess_chain(context)\n",
    "\n",
    "  context[\"novel_chapter\"] = []\n",
    "  for chapter_number in range(1, 3):\n",
    "      context[\"chapter_number\"] = chapter_number\n",
    "      context = novel_chapter_chain(context)\n",
    "      context[\"novel_chapter\"].append(context[\"output\"])\n",
    "\n",
    "  contents = \"\\n\\n\".join(context[\"novel_chapter\"])\n",
    "  return {\"results\": contents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <등장 인물>\n",
      "[{'name': 'James', 'characteristics': '야망에 찬 사업가'}, {'name': 'Kong', 'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계'}, {'name': 'Bab', 'characteristics': '질투 많은 경쟁사 사장'}]\n",
      "</등장 인물>\n",
      "\n",
      "<뉴스 기사>\n",
      "기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
      "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
      "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
      "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
      "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\n",
      "</뉴스 기사>\n",
      "\n",
      "<등장 인물> 과 <뉴스 기사> 를 소재로 새롭고 흥미진진한 Thriller 소설 아이디어를 한 문단으로 작성해줘\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <등장 인물>\n",
      "[{'name': 'James', 'characteristics': '야망에 찬 사업가'}, {'name': 'Kong', 'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계'}, {'name': 'Bab', 'characteristics': '질투 많은 경쟁사 사장'}]\n",
      "</등장 인물>\n",
      "\n",
      "<뉴스 기사>\n",
      "기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
      "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
      "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
      "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
      "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\n",
      "</뉴스 기사>\n",
      "\n",
      "<아이디어>\n",
      "James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 많이 품고 있다. 그러던 중 James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 연인관계를 맺게 된다. 그리고 이 때문에 James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다. 그러나 이와 동시에 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있었다. James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타격을 주기로 결심한다. 그리고 그 다음 주부터는 경유 가격이 반등할 가능성이 있다는 전망이 나오는데, James는 이를 이용하여 Bab에게 더 큰 위기를 초래하려고 계획한다. 이렇게 James와 Kong,\n",
      "</아이디어>\n",
      "\n",
      "<context>\n",
      "아웃라인 단계에서는 주요 이벤트와 결말을 고려하세요. \n",
      "여기서 중요한 것은, 이 단계에서 구체적인 디테일에 매몰되기보다는 스토리의 큰 그림에 집중하는 것입니다.\n",
      "</context>\n",
      "\n",
      "<등장 인물> 과 <아이디어> 를 소재로 새롭고 흥미진진한 Thriller 소설의 아웃라인을 작성해줘\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <등장 인물>\n",
      "[{'name': 'James', 'characteristics': '야망에 찬 사업가'}, {'name': 'Kong', 'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계'}, {'name': 'Bab', 'characteristics': '질투 많은 경쟁사 사장'}]\n",
      "</등장 인물>\n",
      "\n",
      "<아이디어>\n",
      "James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 많이 품고 있다. 그러던 중 James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 연인관계를 맺게 된다. 그리고 이 때문에 James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다. 그러나 이와 동시에 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있었다. James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타격을 주기로 결심한다. 그리고 그 다음 주부터는 경유 가격이 반등할 가능성이 있다는 전망이 나오는데, James는 이를 이용하여 Bab에게 더 큰 위기를 초래하려고 계획한다. 이렇게 James와 Kong,\n",
      "</아이디어>\n",
      "\n",
      "<아웃라인>\n",
      "아웃라인 단계에서는 주요 이벤트와 결말을 고려하여 스토리의 큰 그림을 그려보겠습니다.\n",
      "\n",
      "1. James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있다.\n",
      "2. James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 연인관계를 맺는다.\n",
      "3. James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다.\n",
      "4. 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있다.\n",
      "5. James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타격을 주기로 결심한다.\n",
      "6. 경유 가격이 반등할 가능성이 있는 다음 주에, James는 Bab에게 더 큰 위기를 초래하기 위한 계획을\n",
      "<아웃라인>\n",
      "\n",
      "<context>\n",
      "세부 플롯 개발에서는 캐릭터의 발전, 각 장면의 연속성, 그리고 이벤트들이 어떻게 스토리를 전진시키는지에 초점을 맞추세요. \n",
      "또한, 캐릭터 간의 상호작용과 각 장면이 전체 플롯에 어떻게 기여하는지를 고려하면서 플롯을 세부화해 나가야 합니다.\n",
      "</context>\n",
      "\n",
      "<등장 인물>, <아이디어>, <아웃라인>을 가지고 흥미진진한 Thriller 소설의 세부 플롯을 4개의 Chapter로 나눠서 작성해줘.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <persona>\n",
      "너는 베스트셀러 작가야. 사람들에게 항상 새롭고 흥미진진한 소설을 작성하는 것으로 잘 알려져 있어\n",
      "</persona>\n",
      "\n",
      "<등장 인물>\n",
      "[{'name': 'James', 'characteristics': '야망에 찬 사업가'}, {'name': 'Kong', 'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계'}, {'name': 'Bab', 'characteristics': '질투 많은 경쟁사 사장'}]\n",
      "</등장 인물>\n",
      "\n",
      "<아웃라인>\n",
      "아웃라인 단계에서는 주요 이벤트와 결말을 고려하여 스토리의 큰 그림을 그려보겠습니다.\n",
      "\n",
      "1. James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있다.\n",
      "2. James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 연인관계를 맺는다.\n",
      "3. James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다.\n",
      "4. 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있다.\n",
      "5. James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타격을 주기로 결심한다.\n",
      "6. 경유 가격이 반등할 가능성이 있는 다음 주에, James는 Bab에게 더 큰 위기를 초래하기 위한 계획을\n",
      "<아웃라인>\n",
      "\n",
      "<플롯>\n",
      "Chapter 1: 질투의 불꽃\n",
      "\n",
      "- James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있다.\n",
      "- James는 Bab의 성공을 부러워하며, 자신의 사업을 키우기 위해 악순환에 빠져있다.\n",
      "- James는 우연히 생물학 연구소에서 일하는 유명한 박사인 Kong과 만나게 된다.\n",
      "- James와 Kong은 서로에게 끌리며 연인관계를 맺게 된다.\n",
      "- James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다.\n",
      "\n",
      "Chapter 2: 가격 하락의 음모\n",
      "\n",
      "- 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있다.\n",
      "- James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타\n",
      "</플롯>\n",
      "\n",
      "\n",
      "<등장 인물>, <아웃라인>을 가지고 <플롯>의 Chapter 1을 소설로 작성해\n",
      "\n",
      "Chapter 1:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <persona>\n",
      "너는 베스트셀러 작가야. 사람들에게 항상 새롭고 흥미진진한 소설을 작성하는 것으로 잘 알려져 있어\n",
      "</persona>\n",
      "\n",
      "<등장 인물>\n",
      "[{'name': 'James', 'characteristics': '야망에 찬 사업가'}, {'name': 'Kong', 'characteristics': '생물학 연구소에서 일하는 유명한 박사, James 와 연인관계'}, {'name': 'Bab', 'characteristics': '질투 많은 경쟁사 사장'}]\n",
      "</등장 인물>\n",
      "\n",
      "<아웃라인>\n",
      "아웃라인 단계에서는 주요 이벤트와 결말을 고려하여 스토리의 큰 그림을 그려보겠습니다.\n",
      "\n",
      "1. James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있다.\n",
      "2. James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 연인관계를 맺는다.\n",
      "3. James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다.\n",
      "4. 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있다.\n",
      "5. James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타격을 주기로 결심한다.\n",
      "6. 경유 가격이 반등할 가능성이 있는 다음 주에, James는 Bab에게 더 큰 위기를 초래하기 위한 계획을\n",
      "<아웃라인>\n",
      "\n",
      "<플롯>\n",
      "Chapter 1: 질투의 불꽃\n",
      "\n",
      "- James는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있다.\n",
      "- James는 Bab의 성공을 부러워하며, 자신의 사업을 키우기 위해 악순환에 빠져있다.\n",
      "- James는 우연히 생물학 연구소에서 일하는 유명한 박사인 Kong과 만나게 된다.\n",
      "- James와 Kong은 서로에게 끌리며 연인관계를 맺게 된다.\n",
      "- James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획한다.\n",
      "\n",
      "Chapter 2: 가격 하락의 음모\n",
      "\n",
      "- 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있다.\n",
      "- James는 이 가격 하락을 이용하여 자신의 사업에 도움을 주기 위해 경쟁사인 Bab에게 더 큰 타\n",
      "</플롯>\n",
      "\n",
      "\n",
      "<등장 인물>, <아웃라인>을 가지고 <플롯>의 Chapter 2을 소설로 작성해\n",
      "\n",
      "Chapter 2:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'results': '질투의 불꽃\\n\\nJames는 야망에 찬 사업가로, 경쟁사인 Bab에 대한 질투를 품고 있었다. Bab의 성공을 보면서 자신의 사업이 부족하다는 생각이 들었고, 그로 인해 악순환에 빠져있었다. 그러던 어느 날, 우연히 James는 생물학 연구소에서 일하는 유명한 박사인 Kong과 만나게 되었다.\\n\\nJames와 Kong은 서로에게 끌리며 연인관계를 맺게 되었다. Kong은 James에게 새로운 시각과 통찰력을 제공해주었고, James는 Kong에게 안정과 사랑을 주었다. 그러나 James는 Kong과의 관계를 이용하여 Bab에게 악몽 같은 복수를 계획하게 되었다.\\n\\nJames는 Bab에게 자신의 사업을 망치기 위해 Kong을 이용하기로 결심했다. 그는 Kong에게 Bab\\n\\n가격 하락의 음모\\n\\nJames는 경쟁사인 Bab에게 더 큰 타격을 주기 위해 국내 주유소에서 휘발유와 경유의 가격이 계속해서 하락하고 있는 상황을 이용하기로 결심했다. 그는 이 가격 하락을 자신의 사업에 도움을 주기 위한 음모로 사용하기로 마음먹었다.\\n\\nJames는 다음 주에 경유 가격이 반등할 가능성이 있는 것을 알고 있었다. 그래서 그는 Bab에게 더 큰 위기를 초래하기 위한 계획을 세우기 시작했다. 그의 계획은 경유 가격이 다시 상승할 때 Bab의 주유소에 대량으로 경유를 공급하여 가격을 인위적으로 높이는 것이었다.\\n\\nJames는 Kong과 함께 이 계획을 실행하기로 했다. Kong은 생물학 연구소에서 일하는 유명한 박사로서'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_GENRE = \"Thriller\"\n",
    "DEFAULT_CHACRACTERS = [\n",
    "    {\"name\": \"James\", \"characteristics\": \"야망에 찬 사업가\"},\n",
    "    {\"name\": \"Kong\", \"characteristics\": \"생물학 연구소에서 일하는 유명한 박사, James 와 연인관계\"},\n",
    "    {\"name\": \"Bab\", \"characteristics\": \"질투 많은 경쟁사 사장\"},\n",
    "]\n",
    "DEFAULT_NEWS_TEXT = \"\"\"기름값 하락 계속…휘발유 6.6원·경유 5.9원↓ 이번 주에도 국내 주유소 휘발유와 경유 판매 가격이 동반 하락했습니다.\n",
    "한국석유공사 유가정보시스템 오피넷에 따르면 6월 셋째주 전국 주유소 휘발유 평균 판매 가격은 전주보다 6.6원 하락한 리터당 1,575.8원을 기록했습니다.\n",
    "경유 판매 가격 역시 8.7원 내린 1,387.6원으로 집계됐습니다.\n",
    "휘발유 가격은 8주째, 경유 가격은 9주 연속 내림세입니다.\n",
    "대한석유협회 관계자는 \"다음 주에도 휘발유·경유 가격은 하향 안정세를 보이겠지만, 그 다음 주부터는 특히 경유 가격이 반등할 가능성이 있다\"고 전망했습니다.\"\"\"\n",
    "\n",
    "generate_novel(DEFAULT_GENRE, DEFAULT_CHACRACTERS, DEFAULT_NEWS_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    filename: str\n",
    "    page: int\n",
    "\n",
    "\n",
    "class Document(BaseModel):\n",
    "    id: str\n",
    "    text: str\n",
    "    metadata: Metadata\n",
    "\n",
    "\n",
    "class File(BaseModel):\n",
    "    path: str\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    filename: str\n",
    "    query: str\n",
    "    top_k: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient()\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\",\"\"),\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "\n",
    "class Datastore:\n",
    "    def __init__(self, collection_name):\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        try:\n",
    "            self.collection = client.get_collection(\n",
    "                name=collection_name, embedding_function=openai_ef)\n",
    "        except ValueError as e:\n",
    "            self.collection = client.create_collection(\n",
    "                name=collection_name, embedding_function=openai_ef)\n",
    "\n",
    "    def upsert(self, doc: Document):\n",
    "        self.collection.add(\n",
    "            ids=[doc.id],\n",
    "            documents=[doc.text],\n",
    "            metadatas=[doc.metadata.dict()],\n",
    "        )\n",
    "\n",
    "        return doc\n",
    "\n",
    "    def delete(self, id: str):\n",
    "        self.collection.delete(ids=[id])\n",
    "\n",
    "    def query(self, filename: str, query: str, top_k: int) -> list[Document]:\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=top_k,\n",
    "            where={\"filename\": filename},\n",
    "        )\n",
    "\n",
    "        docs = []\n",
    "        for i in range(len(results[\"ids\"][0])):\n",
    "            doc = Document(\n",
    "                id=results[\"ids\"][0][i],\n",
    "                text=results[\"documents\"][0][i],\n",
    "                metadata=results[\"metadatas\"][0][i],\n",
    "                distance=results[\"distances\"][0][i],\n",
    "            )\n",
    "\n",
    "            docs.append(doc)\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "db = Datastore(\"chatpdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    filename = '/Users/kakao/vs_code/AI_LLM/AI_LLM/sample.pdf'\n",
    "    fulltext = \"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf = PdfReader(f)\n",
    "        print(pdf)\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            chunks = [text[i:i+1000] for i in range(0, len(text), 900)]\n",
    "            pageNum = pdf.get_page_number(page)\n",
    "            fulltext += text\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc = Document(\n",
    "                    id=f\"{filename.split('/')[-1]}-p{pageNum}-{i}\",\n",
    "                    text=chunk,\n",
    "                    metadata={\"filename\": filename.split('/')[-1], \"page\": pageNum}\n",
    "                )\n",
    "\n",
    "                print(\"Upserting: \", doc.id)\n",
    "                db.upsert(doc)\n",
    "\n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"total_pages\": len(pdf.pages),\n",
    "        \"text\": fulltext\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PyPDF2._reader.PdfReader object at 0x1800a3100>\n",
      "Upserting:  sample.pdf-p0-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/h7_976t54qs_mw32z98y8w180000gq/T/ipykernel_35380/2770969260.py:24: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  metadatas=[doc.metadata.dict()],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting:  sample.pdf-p0-1\n",
      "Upserting:  sample.pdf-p0-2\n",
      "Upserting:  sample.pdf-p0-3\n",
      "Upserting:  sample.pdf-p0-4\n",
      "Upserting:  sample.pdf-p1-0\n",
      "Upserting:  sample.pdf-p1-1\n",
      "Upserting:  sample.pdf-p1-2\n",
      "Upserting:  sample.pdf-p1-3\n",
      "Upserting:  sample.pdf-p1-4\n",
      "Upserting:  sample.pdf-p1-5\n",
      "Upserting:  sample.pdf-p2-0\n",
      "Upserting:  sample.pdf-p2-1\n",
      "Upserting:  sample.pdf-p2-2\n",
      "Upserting:  sample.pdf-p2-3\n",
      "Upserting:  sample.pdf-p2-4\n",
      "Upserting:  sample.pdf-p2-5\n",
      "Upserting:  sample.pdf-p3-0\n",
      "Upserting:  sample.pdf-p3-1\n",
      "Upserting:  sample.pdf-p3-2\n",
      "Upserting:  sample.pdf-p3-3\n",
      "Upserting:  sample.pdf-p3-4\n",
      "Upserting:  sample.pdf-p4-0\n",
      "Upserting:  sample.pdf-p4-1\n",
      "Upserting:  sample.pdf-p4-2\n",
      "Upserting:  sample.pdf-p4-3\n",
      "Upserting:  sample.pdf-p4-4\n",
      "Upserting:  sample.pdf-p5-0\n",
      "Upserting:  sample.pdf-p5-1\n",
      "Upserting:  sample.pdf-p5-2\n",
      "Upserting:  sample.pdf-p5-3\n",
      "Upserting:  sample.pdf-p5-4\n",
      "Upserting:  sample.pdf-p6-0\n",
      "Upserting:  sample.pdf-p6-1\n",
      "Upserting:  sample.pdf-p6-2\n",
      "Upserting:  sample.pdf-p6-3\n",
      "Upserting:  sample.pdf-p6-4\n",
      "Upserting:  sample.pdf-p6-5\n",
      "Upserting:  sample.pdf-p7-0\n",
      "Upserting:  sample.pdf-p7-1\n",
      "Upserting:  sample.pdf-p7-2\n",
      "Upserting:  sample.pdf-p7-3\n",
      "Upserting:  sample.pdf-p7-4\n",
      "Upserting:  sample.pdf-p8-0\n",
      "Upserting:  sample.pdf-p8-1\n",
      "Upserting:  sample.pdf-p8-2\n",
      "Upserting:  sample.pdf-p8-3\n",
      "Upserting:  sample.pdf-p9-0\n",
      "Upserting:  sample.pdf-p9-1\n",
      "Upserting:  sample.pdf-p9-2\n",
      "Upserting:  sample.pdf-p9-3\n",
      "Upserting:  sample.pdf-p9-4\n",
      "Upserting:  sample.pdf-p10-0\n",
      "Upserting:  sample.pdf-p10-1\n",
      "Upserting:  sample.pdf-p10-2\n",
      "Upserting:  sample.pdf-p10-3\n",
      "Upserting:  sample.pdf-p10-4\n",
      "Upserting:  sample.pdf-p10-5\n",
      "Upserting:  sample.pdf-p11-0\n",
      "Upserting:  sample.pdf-p11-1\n",
      "Upserting:  sample.pdf-p11-2\n",
      "Upserting:  sample.pdf-p11-3\n",
      "Upserting:  sample.pdf-p11-4\n",
      "Upserting:  sample.pdf-p11-5\n",
      "Upserting:  sample.pdf-p12-0\n",
      "Upserting:  sample.pdf-p12-1\n",
      "Upserting:  sample.pdf-p12-2\n",
      "Upserting:  sample.pdf-p13-0\n",
      "Upserting:  sample.pdf-p13-1\n",
      "Upserting:  sample.pdf-p13-2\n",
      "Upserting:  sample.pdf-p13-3\n",
      "Upserting:  sample.pdf-p14-0\n",
      "Upserting:  sample.pdf-p14-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': '/Users/kakao/vs_code/AI_LLM/AI_LLM/sample.pdf',\n",
       " 'total_pages': 15,\n",
       " 'text': 'Lost in the Middle: How Language Models Use Long Contexts\\nNelson F. Liu1∗Kevin Lin2John Hewitt1Ashwin Paranjape3\\nMichele Bevilacqua3Fabio Petroni3Percy Liang1\\n1Stanford University2University of California, Berkeley3Samaya AI\\nnfliu@cs.stanford.edu\\nAbstract\\nWhile recent language models have the ability\\nto take long contexts as input, relatively little\\nis known about how well the language models\\nuselonger context. We analyze language model\\nperformance on two tasks that require identify-\\ning relevant information within their input con-\\ntexts: multi-document question answering and\\nkey-value retrieval. We find that performance is\\noften highest when relevant information occurs\\nat the beginning or end of the input context,\\nand significantly degrades when models must\\naccess relevant information in the middle of\\nlong contexts. Furthermore, performance sub-\\nstantially decreases as the input context grows\\nlonger, even for explicitly long-context models.\\nOur analysis provides a better understanding\\nof how language models use their input con-\\ntext and provides new evaluation protocols for\\nfuture long-context models.\\n1 Introduction\\nLanguage models have become an important and\\nflexible building block in a variety of user-facing\\nlanguage technologies, including conversational\\ninterfaces, search and summarization, and collabo-\\nrative writing. These models perform downstream\\ntasks primarily via prompting: all relevant task\\nspecification and data to process is formatted as\\na textual context, and the model returns a gener-\\nated text completion. These input contexts can\\ncontain thousands of tokens, especially when using\\nlanguage models on lengthy inputs (e.g., legal or\\nscientific documents, conversation histories, etc.)\\nor augmenting them with external information (e.g.,\\nrelevant documents from a search engine, database\\nquery results, etc; Petroni et al., 2020; Ram et al.,\\n2023; Shi et al., 2023; Mallen et al., 2023; Schick\\net al., 2023, inter alia ).\\nHandling these use-cases requires language mod-\\nels to successfully operate over long sequences.\\n*Work partially completed as an intern at Samaya AI.\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer5560657075Accuracy\\n20 T otal Retrieved Documents\\ngpt-3.5-turbo-0613 (open-book)\\ngpt-3.5-turbo-0613 (closed-book)Figure 1: Changing the location of relevant information\\n(in this case, the position of the passage that answers an\\ninput question) within the language model’s input con-\\ntext results in a U-shaped performance curve—models\\nare better at using relevant information that occurs at the\\nvery beginning or end of its input context, and perfor-\\nmance degrades significantly when models must access\\nand use information located in the middle of its input\\ncontext. For example, GPT-3.5-Turbo’s open-book per-\\nformance on the multi-document question task when\\nrelevant information is placed in the middle of its input\\ncontext is lower than its performance when predicting\\nwithout any documents (i.e., the closed-book setting;\\n56.1%). See Figure 5 for full results.\\nLanguage models are generally implemented with\\nTransformers, which scale poorly to long sequences\\n(e.g., since self-attention complexity is quadratic\\nwith the input sequence length). As a result, lan-\\nguage models are typically trained with relatively\\nsmall context windows. Recent improvements in\\nhardware (e.g., faster GPUs with more memory)\\nand algorithms (Dai et al., 2019; Dao et al., 2022;\\nPoli et al., 2023; Rubin and Berant, 2023, inter\\nalia) have resulted in language models with larger\\ncontext windows, but it remains unclear how thesearXiv:2307.03172v1  [cs.CL]  6 Jul 2023extended-context language models make use their\\ninput contexts when performing downstream tasks.\\nWe empirically investigate this question via\\ncontrolled experiments with a variety of state-of-\\nthe-art open (MPT-30B-Instruct, LongChat-13B\\n(16K)) and closed (OpenAI’s GPT-3.5-Turbo and\\nAnthropic’s Claude) language models in settings\\nthat require accessing and using information within\\nan input context. We first experiment with multi-\\ndocument question answering, which requires mod-\\nels to reason over provided documents to find rele-\\nvant information and use it to answer a given ques-\\ntion; this task mimics the retrieval-augmented gen-\\neration setup underlying many commercial gener-\\native search and question answering applications\\n(e.g., Bing Chat). We make controlled changes to\\nthe input context size and the position of the rele-\\nvant information within the input context and study\\ntheir effects on model performance. In particular,\\nwe can increase the input context length by adding\\nmore documents to the input context (akin to re-\\ntrieving more documents in retrieval-augmented\\ngeneration), and modify the position of the relevant\\ninformation within the context by changing the or-\\nder of the documents in the input context to place\\nthe relevant document at the beginning, middle or\\nend of the context.\\nWe observe a distinctive U-shaped performance,\\nwhich can be clearly visualized in Figure 1, as\\nwe vary the position of the relevant information\\n—language model performance is highest when rel-\\nevant information occurs at the very beginning or\\nend of its input context, and performance signifi-\\ncantly degrades when models must access and use\\ninformation in the middle of their input context\\n(§3.3). For example, when relevant information\\nis placed in the middle of its input context, GPT-\\n3.5-Turbo’s performance on the multi-document\\nquestion task is lower than its performance when\\npredicting without any documents (i.e., the closed-\\nbook setting; 56.1%). In addition, we find that\\nmodel performance steadily degrades on longer\\ncontexts (§3.3), and that extended-context models\\nare not necessarily better at using their input con-\\ntext (§3.3).\\nGiven that language models struggle to retrieve\\nand use relevant information in the multi-document\\nquestion answering task, to what extent can lan-\\nguage models even retrieve from their input con-\\ntexts? We study this question with a synthetic key-\\nvalue retrieval task, which is designed to be a mini-mal testbed for the basic ability to retrieve matching\\ntokens from the input context. In this task, models\\nare given a collection of JSON-formatted key-value\\npairs, and must return the value associated with a\\nspecific key. Similar to the multi-document QA\\ntask, the key-value retrieval task also admits con-\\ntrolled changes to the input context length (adding\\nmore key-value pairs) and the position of relevant\\ninformation. We observe a similar U-shaped perfor-\\nmance curve in this setting; many models struggle\\nto simply retrieve matching tokens that occur in the\\nmiddle of their input context.\\nTo better understand why language models strug-\\ngle to access and use information in the middle\\nof their input contexts, we conduct preliminary\\ninvestigations into the role of model architecture\\n(decoder-only vs. encoder-decoder), query-aware\\ncontextualization, and instruction fine-tuning (§5).\\nWe find that encoder-decoder models are relatively\\nrobust to changes in the position of relevant in-\\nformation within their input context when evalu-\\nated on sequences within its training-time sequence\\nlength, but they show a U-shaped curve when eval-\\nuated on sequences longer than those seen during\\ntraining (§5.1). In addition, query-aware contex-\\ntualization (placing the query before andafter the\\ndocuments or key-value pairs) enables models to\\nperform the synthetic key-value task perfectly, but\\nminimally changes trends in multi-document QA\\n(§5.2). Finally, even base language models (i.e.,\\nwithout instruction fine-tuning) show a U-shaped\\nperformance curve as we vary the position of rele-\\nvant information in the input context.\\nLastly, we perform a case study with retriever-\\nreader models on open-domain question answering\\nto better understand the trade-off between adding\\nmore information to an input context and increas-\\ning the amount of content that the model must rea-\\nson over (§6)—in contrast to our controlled multi-\\ndocument QA task, where the context always con-\\ntains exactly one document that answers the ques-\\ntion, none or many of the top kdocuments may\\ncontain the answer in the open-domain QA sett-\\nting. When retrieving from Wikipedia to answer\\nqueries from NaturalQuestions-Open, we find that\\nmodel performance saturates long before retriever\\nrecall levels off, indicating that models fail to effec-\\ntively use additional retrieved documents—using\\nmore than 20 retrieved documents only marginally\\nimproves performance ( ∼1.5% for GPT-3.5-Turbo\\nand∼1% for claude-1.3).Our analysis provides a better understanding of\\nhow language models use their input context and\\nintroduces new evaluation protocols for future long-\\ncontext models. To facilitate further work on un-\\nderstanding and improving how language models\\nuse their input context, we release our code and\\nevaluation data at nelsonliu.me/papers/lost-in-the-\\nmiddle.\\n2 Language Models\\nWe study language models as functions that take\\na textual input context and return a textual out-\\nput. Modern language models are most commonly\\nimplemented with Transformers (Vaswani et al.,\\n2017). Transformer language models encode in-\\nput contexts with self-attention, whose time and\\nmemory complexity is quadratic in the length of\\nthe input, limiting their application to very long\\nsequences. As a result, language models are gen-\\nerally pre-trained with relatively small amount of\\nprior context (its context window ), which accord-\\ningly also limits the maximum length of their input\\ncontexts.\\nIncreasing language model maximum context\\nlength. Recent advances in hardware (e.g., faster\\nGPUs with more memory) and algorithms (e.g.,\\nFlashAttention; Dao et al., 2022) have driven a\\nrapid increase in language model maximum context\\nlength. OpenAI’s GPT-4 model (released in March\\n2023) has a maximum context window of 32K to-\\nkens; in May 2023, Claude’s context window was\\nexpanded from 8K tokens to 100K tokens. In June\\n2023, OpenAI announced an extended-context ver-\\nsion of its GPT-3.5-Turbo model, increasing its\\ncontext from 4K to 16K tokens. A variety of open-\\nsource long context language models have also\\nbeen recently released: MPT-30B has a maximum\\ncontext length of 8K tokens, and LongChat-7B has\\na maximum context length of 16K tokens. Finally,\\na variety of recently-proposed architectures model\\nsequences with millions of tokens, raising the po-\\ntential of further dramatic increases in language\\nmodel maximum context length (Gu et al., 2022;\\nFu et al., 2023; Poli et al., 2023; Yu et al., 2023,\\ninter alia ).\\n3 Multi-Document Question Answering\\nOur goal is to better understand how language mod-\\nels use their input context. To this end, we analyze\\nmodel performance on multi-document questionanswering, which requires models to find relevant\\ninformation within an input context and using it to\\nanswer the question. In particular, we make con-\\ntrolled changes to the length of the input context\\nand the position of the relevant information and\\nmeasure changes in task performance.\\n3.1 Experimental Setup\\nOur multi-document question answering task\\nclosely parallels the retrieval-augmented genera-\\ntion setup underlying commercial search and ques-\\ntion answering applications (e.g., Bing Chat). In\\nthese experiments, the model inputs are (i) a ques-\\ntion to answer and (ii) kdocuments (e.g., passages\\nfrom Wikipedia), where exactly one the documents\\ncontains the answer to the question and k−1“dis-\\ntractor” documents do not. Performing this task\\nrequires the model to access the document that con-\\ntains the answer within its input context and use\\nit to answer the question. Figure 2 presents an\\nexample.\\nWe instantiate this task with data from the\\nNaturalQuestions benchmark (Kwiatkowski et al.,\\n2019), which contains historical queries issued to\\nthe Google search engine and human-annotated an-\\nswers extracted from Wikipedia. Specifically, we\\nfirst take queries from NaturalQuestions-Open (Lee\\net al., 2019), an open domain question answering\\nbenchmark that is derived from NaturalQuestions.\\nUse use passages (chunks of at most 100 tokens)\\nfrom Wikipedia as documents within our input con-\\ntexts. For each of these queries, we need a docu-\\nment that contains the answer and k−1distractor\\ndocuments that do not contain the answer. To ob-\\ntain a document that answers the question, we use\\nthe Wikipedia paragraph that contains the answer\\nfrom the NaturalQuestions annotations. To col-\\nlectk−1distractor documents that do not contain\\nthe answer, we use the Contriever retrieval system\\n(Izacard et al., 2021) to retrieve the k−1Wikipedia\\nchunks that are most relevant to the question and do\\nnot contain any of the NaturalQuestions-annotated\\nanswers.1In the input context, the distractor doc-\\numents are presented in order of decreasing rele-\\nvance.2\\n1Ambiguity in NaturalQuestions-Open means that a small\\nnumber of distractor passages may contain a reasonable an-\\nswer. We additionally run experiments on subset of unam-\\nbiguous questions, finding similar results and conclusions; see\\nAppendix A.\\n2Since there might be a prior over “search results” appear-\\ning in ranked order, we explored randomly ordering the k−1\\ndistractor documents and mentioning that the documents areWrite a high-quality answer for the given question using only the provided search \\nresults (some of which might be irrelevant). \\nDocument [1](Title: Asian Americans in science and technology) Prize in physics for \\ndiscovery of the subatomic particle J/ψ. Subrahmanyan Chandrasekhar shared... \\nDocument [2](Title: List of Nobel laureates in Physics) The first Nobel Prize in \\nPhysics was awarded in 1901 to Wilhelm Conrad Röntgen, of Germany, who received... \\nDocument [3](Title: Scientist) and pursued through a unique method, was essentially \\nin place. Ramón y Cajal won the Nobel Prize in 1906 for his remarkable... \\nQuestion: who got the first nobel prize in physics \\nAnswer: Input Context \\nWilhelm Conrad Röntgen Desired Answer Figure 2: Example of the multi-document question answering task, with an input context and the desired model\\nanswer. The relevant document for correctly answering the request is bolded within the input context.\\nWrite a high-quality answer for the given question \\nusing only the provided search results (some of \\nwhich might be irrelevant). \\nDocument [1](Title: Asian Americans in science and \\ntechnology) ... \\nDocument [2](Title: List of Nobel laureates in \\nPhysics) ... \\nDocument [3](Title: Scientist) ... \\nDocument [4](Title: Norwegian Americans) ... \\nDocument [5](Title: Maria Goeppert Mayer) ... \\nQuestion: who got the first nobel prize in physics \\nAnswer: Input Context Input Context \\nWilhelm Conrad Röntgen Desired Answer \\nFigure 3: Modulating the input context length of the\\nmulti-document question answering example presented\\nin Figure 2. Adding additional documents that do not\\ncontain the answer increases the length of the input con-\\ntext, but does not affect the desired output. The relevant\\ndocument pair for correctly answering the request is\\nbolded within the input context.\\nFollowing Kandpal et al. (2022) and Mallen et al.\\n(2023), we use accuracy as our primary evaluation\\nmetric, judging whether any of the correct answers\\n(as taken from the NaturalQuestions annotations)\\nappear in the predicted output.\\nTo modulate the input context length in this task,\\nwe increase or decrease the number of retrieved\\ndocuments that do not contain the answer (Fig-\\nure 3). To modulate the position of relevant infor-\\nmation within the input context, we adjust the order\\nof the documents in the input context to change the\\nposition of the document that contains the answer\\n(Figure 4).\\nrandomly ordered in the task description, but found the same\\ntrends. See Appendix B for more details.\\nWrite a high-quality answer for the given question \\nusing only the provided search results (some of \\nwhich might be irrelevant). \\nDocument [1](Title: List of Nobel laureates in \\nPhysics) ... \\nDocument [2](Title: Asian Americans in science and \\ntechnology) ... \\nDocument [3](Title: Scientist) ... \\nQuestion: who got the first nobel prize in physics \\nAnswer: Input Context \\nWilhelm Conrad Röntgen Desired Answer Figure 4: Modulating the position of relevant informa-\\ntion within the input context for the multi-document\\nquestion answering example presented in Figure 2. Re-\\nordering the documents in the input context does not\\naffect the desired output. The relevant document for cor-\\nrectly answering the request is bolded within the input\\ncontext.\\n3.2 Models\\nWe analyze several state-of-the-art open and closed\\nmodels. We use greedy decoding when generating\\noutputs and leave exploration of other decoding\\nmethods to future work. We use a standard set of\\nprompts for each model (depicted in Figure 2).\\nOpen models. We experiment with MPT-30B-\\nInstruct, which has a maximum context length of\\n8192 tokens. The model was initially pre-trained\\non 1 trilion tokens using 2048-token sequences,\\nfollowed by an additional sequence length adapta-\\ntion pre-training phase on 50B tokens using 8192-\\ntoken sequences. We also evaluate LongChat-13B\\n(16K) (Li et al., 2023), which builds on LLaMA-\\n13B (original maximum context window from\\n2048; Touvron et al., 2023) and extends its con-\\ntext window to 16384 by using condensed rotary1st 5th 10th\\nPosition of Document with the Answer505560657075Accuracy\\n10 T otal Retrieved Documents\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer505560657075Accuracy\\n20 T otal Retrieved Documents\\n1st 5th 10th 15th 20th 25th 30th\\nPosition of Document with the Answer505560657075Accuracy\\n30 T otal Retrieved Documents\\nclaude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mpt-30b-instruct longchat-13b-16kFigure 5: The effect of changing the position of relevant information (document containing the answer) on multi-\\ndocument question answering performance. Lower positions are closer to the start of the input context. Performance\\nis generally highest when relevant information is positioned at the very start or very end of the context, and rapidly\\ndegrades when models must reason over information in the middle of their input context.\\nembeddings before fine-tuning with 16384-token\\nsequences.\\nClosed models. We use the OpenAI API to ex-\\nperiment with GPT-3.5-Turbo and GPT-3.5-Turbo\\n(16K).3GPT-3.5-Turbo has a maximum context\\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\\nversion with an extended maximum context length\\nof 16K tokens. We evaluate claude-1.3 and claude-\\n1.3-100k with the Anthropic API; claude-1.3 has a\\nmaximum context length of 8K tokens, and claude-\\n1.3-100k has an extended context length of 100K\\ntokens.4\\n3.3 Results and Discussion\\nWe experiment with input contexts containing 10,\\n20, and 30 documents (2.7K examples each). Fig-\\nure 5 presents multi-document question answering\\nperformance when the position of relevant informa-\\ntion within the input context. To better understand\\nthe realistic lower- and upper-bounds on perfor-\\nmance, we also evaluate performance on the closed-\\nbook and oracle settings. In the closed-book setting,\\nmodels are not given any documents in their input\\ncontext, and must rely on their parametric memory\\nto generate the correct answer. On the other hand,\\nin the oracle setting, language models are given\\nthe single document that contains the answer and\\nmust use it to answer the question. GPT-3.5-Turbo\\nand GPT-3.5-Turbo (16K) have the highest closed-\\n3We use the 0613 model revisions for all OpenAI API\\nexperiments.\\n4We also use the OpenAI API to evaluate GPT-4 on a\\nsubset of multi-document QA experiments, finding similar\\nresults and trends as other models (though with higher absolute\\nperformance). Evaluating GPT-4 on the full multi-document\\nQA and key-value retrieval experiments would cost upwards\\nof $6000. See Appendix C for results and discussion.book (55%) and oracle (88%) performance; see\\nAppendix D for full closed-book and oracle results\\non all models.\\nModel performance is highest when relevant in-\\nformation occurs at the beginning or end of its\\ninput context. As the position of relevant infor-\\nmation is changed, we see a distinctive U-shaped\\ncurve in model performance—models are much\\nbetter at identifying and using relevant informa-\\ntion that occurs at the very beginning and very\\nend of contexts, and suffer degraded performance\\nwhen forced to use information within the middle\\nof its input context. For example, GPT-3.5-Turbo’s\\nmulti-document QA performance can drop by more\\nthan 20%—at its nadir, performance in 20- and\\n30-document settings is lower than performance\\nwithout anyinput documents (i.e., closed-book per-\\nformance; 56.1%). These results indicate that cur-\\nrent models cannot effectively reason over their en-\\ntire context window when performing downstream\\ntasks, and that models have an easier time retriev-\\ning and using information at the very start or end\\nof their input contexts.\\nModel performance substantially decreases as\\ninput contexts grow longer. On both tasks,\\nmodel performance degrades as the contexts grow\\nlonger, indicating that models struggle to retrieve\\nand use relevant information from long input con-\\ntexts (Figure 6).\\nThis trend continues when comparing models\\nwith their corresponding extended-context versions.\\nFor example, GPT-3.5-Turbo’s lowest performance\\nin the 20-document setting is 52.9% (when the doc-\\nument containing the answer is positioned 10th\\nout of 20). The input contexts of the 30-document5 10 20 30\\nNumber of Documents in Input Context55606570Accuracy\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613\\nclaude-1.3claude-1.3-100k\\nmpt-30b-instruct\\nlongchat-13b-16kFigure 6: Language model performance (averaged\\nacross position of relevant information) on the multi-\\ndocument question answering task decreases as the input\\ncontext grows longer.\\nsetting are too long for GPT-3.5-Turbo, but using\\nits extended-context counterpart GPT-3.5-Turbo\\n(16K) also results in performance decrease (49.5%\\nwhen the relevant document is positioned 10th out\\nof 30)—although extended-context models can pro-\\ncess longer input contexts, they may not be better\\nat reasoning over the information within its context\\nwindow.\\nExtended-context models are not necessarily bet-\\nter at using input context. In settings where the\\ninput context fits in the context window of both\\na model and its extended-context counterpart, we\\nsee that performance between them is nearly iden-\\ntical. For example, the results for GPT-3.5-Turbo\\nand GPT-3.5-Turbo (16K) are nearly superimposed\\n(solid green series and dashed red series, respec-\\ntively). These results indicate that models with\\nlonger maximum context windows are not neces-\\nsarily better at using this extended context.\\n4 How Well Can Language Models\\nRetrieve From Input Contexts?\\nGiven that language models struggle to retrieve\\nand use information from the middle of their input\\ncontexts in the multi-document question answer-\\ning task, to what extent can they simply retrieve\\nfrom input contexts? We study this question with\\na synthetic key-value retrieval task to isolate and\\nstudy the basic ability of matching and retrieving\\nrelevant information from input contexts.4.1 Experimental Setup\\nIn our synthetic key-value retrieval task, the inputs\\nare (i) a string-serialized JSON object with kkey-\\nvalue pairs, where each of the keys and values are\\nunique, randomly-generated UUIDs and (ii) a par-\\nticular key within the aforementioned JSON object.\\nThe goal is to return the value associated with the\\nspecified key. Thus, each JSON object contains\\none relevant key-value pair (where the value is to\\nbe retrieved), and k−1irrelevant “distractor” key-\\nvalue pairs. Figure 7 provides an example input\\ncontext and its corresponding desired output. We\\nuse accuracy as our evaluation metric, assessing\\nwhether the correct value appears in the predicted\\noutput.\\nOur synthetic key-value retrieval task is designed\\nto provide a minimal testbed for the basic abil-\\nity to retrieve matching tokens from an input con-\\ntext. This task shares similar goals with the Little\\nRetrieval Test of Papailiopoulos et al. (2023) and\\nthe closely-related fine-grained line retrieval task\\nof Li et al. (2023), but we explicitly seek to dis-\\ntill and simplify the task by removing as much\\nnatural language semantics as possible (using ran-\\ndom UUIDs instead), since language features may\\npresent potential confounders (e.g., because Trans-\\nformer language models may have varying sensi-\\ntivity to different linguistic features in their input\\ncontext; O’Connor and Andreas, 2021).\\nTo modulate the input context length in this task,\\nwe change the number of input JSON key-value\\npairs kby adding or removing random keys, chang-\\ning the number of distractor key-value pairs (Fig-\\nure 8). To modulate the position of relevant in-\\nformation within the input context, we change the\\nposition of the key to retrieve within the serialized\\nJSON object (Figure 9).\\n4.2 Results and Discussion\\nFigure 10 presents key-value retrieval performance;\\nWe experiment with input contexts containing\\n75, 140, and 300 key-value pairs (500 examples\\neach). We use the same set of models as the multi-\\ndocument question answering experiments, see\\n§3.2 for more details.\\nAlthough the synthetic key-value retrieval task\\nonly requires identifying exact match within\\nthe input context, not all models achieve high\\nperformance—claude-1.3 and claude-1.3-100k do\\nnearly perfectly on all evaluated input context\\nlengths, but other models struggle, especially whenExtract the value corresponding to the specified key in the JSON object below. \\nJSON data: \\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\", \\n \"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\", \\n \"9f4a92b9-5f69-4725-ba1e-403f08dea695 \": \"703a7ce5-f17f-4e6d-b895-5836ba5ec71c\", \\n \"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-b9823991ebeb\", \\n \"f4eb1c53-af0a-4dc4-a3a5-c2d50851a178\": \"d733b0d2-6af3-44e1-8592-e5637fdb76fb\"} \\nKey: \" 9f4a92b9-5f69-4725-ba1e-403f08dea695 \"\\nCorresponding value: Input Context \\n703a7ce5-f17f-4e6d-b895-5836ba5ec71c Desired Output Figure 7: Example of the key-value retrieval task, with an input context and the desired model output. All keys\\nand values are 128-bit UUIDs, and the goal of the task is to return the value associated with the specified key. The\\nrelevant key-value pair for correctly answering the request is bolded within the input context.\\nExtract the value corresponding to the specified key in \\nthe JSON object below. \\nJSON data: \\n{\"2a8d601d-...-8ad825a74195\": \"bb3ba2a5-...-a88bb9fa7289\", \\n \"a54e2eed-...-3624e77d6684\": \"d1ff29be-...-0cea716be3d4\", \\n \"f9130258-...-232e92d369c9\": \"6fcd02c0-...-16464ce76a13\", \\n \"56e00398-...-4cbdd6c87b53\": \"bf4700be-...-7ccd57c9df91\", \\n \"85352c2d-...-9edbe756efca\": \"307d52f4-...-cdd939438915\", \\n \"9f4a92b9- ...-403f08dea695 \": \"703a7ce5-...-5836ba5ec71c\", \\n \"7202d68f-...-e352844671fe\": \"145e4450-...-d8e4576d9a8e\", \\n \"1dc736e1-...-f3296b586348\": \"43da98d6-...-1544f95782a2\", \\n \"dd52c4b0-...-7a167fbdf8b4\": \"c88ad889-...-c0f76b4afa42\", \\n \"52a9c80c-...-4a4901bc2ac3\": \"b2f8ea3d-...-b9823991ebeb\", \\n \"f4eb1c53-...-c2d50851a178\": \"d733b0d2-...-e5637fdb76fb\"} \\nKey: \" 9f4a92b9-5f69-4725-ba1e-403f08dea695 \"\\nCorresponding value: Input Context \\nFigure 8: Modulating the input context length of the key-\\nvalue retrieval example presented in Figure 7. Adding\\nrandom key-value pairs (128-bit UUIDs) increases\\nlength of the input context, but does not affect the de-\\nsired output. The relevant key-value pair for correctly\\nanswering the request is bolded within the input context.\\nretrieving keys from 140 or more key-value pairs.\\nThe results on the key-value retrieval task have\\nlargely similar trends to the results on the multi-\\ndocument question-answering task (excepting mod-\\nels with perfect performance on the key-value re-\\ntrieval task). In particular, we see the U-shaped\\nperformance curve again; model performance is\\nlowest when they must access key-value pairs in\\nthe middle of their input context. Furthermore,\\nmodel performance in this setting generally also\\ndecreases on longer input contexts. LongChat-13B\\n(16K) in the 140 key-value setting is a notable out-\\nlier; when the relevant information is at the start\\nof the input context, it tends to generate code to\\nretrieve the key, rather than outputting the value\\nitself.\\nExtract the value corresponding to the specified key in \\nthe JSON object below. \\nJSON data: \\n{\"9f4a92b9-...-403f08dea695 \": \"703a7ce5- ...-5836ba5ec71c\", \\n \"2a8d601d- ...-8ad825a74195\": \"bb3ba2a5- ...-a88bb9fa7289\", \\n \"a54e2eed- ...-3624e77d6684\": \"d1ff29be- ...-0cea716be3d4\", \\n \"52a9c80c- ...-4a4901bc2ac3\": \"b2f8ea3d- ...-b9823991ebeb\", \\n \"f4eb1c53- ...-c2d50851a178\": \"d733b0d2- ...-e5637fdb76fb\"} \\nKey: \" 9f4a92b9-5f69-4725-ba1e-403f08dea695 \"\\nCorresponding value: Input Context Figure 9: Modulating the position of relevant informa-\\ntion within the input context for the key-value retrieval\\nexample presented in Figure 7. Re-ordering the key-\\nvalue pairs does not affect the desired output. All keys\\nand values are random 128-bit UUIDs. The relevant\\nkey-value pair for correctly answering the request is\\nbolded within the input context.\\n5 Why Do Language Models Struggle To\\nUse Their Entire Input Context?\\nOur multi-document question answering and key-\\nvalue retrieval results show that language model\\nperformance degrades significantly when they must\\naccess relevant information in the middle of long in-\\nput contexts. To better understand why, we perform\\nsome preliminary investigations into the role of\\nmodel architecture (e.g., decoder-only vs. encoder-\\ndecoder), query-aware contextualization, and the\\neffects of instruction fine-tuning.\\n5.1 Effect of Model Architecture\\nThe open models we evaluate in §3 and §4 are all\\ndecoder-only models—at each timestep, they may\\nonly attend to prior tokens. To better understand\\nthe potential effects of model architecture on how\\nlanguage model use context, we compare decoder-\\nonly and encoder-decoder language models.1st 25th 50th 75th\\nPosition of Key to Retrieve405060708090100Accuracy\\n75 Key-Value Pairs (~4K tokens)\\n1st 35th 70th 105th 140th\\nPosition of Key to Retrieve405060708090100Accuracy\\n140 Key-Value Pairs (~8K tokens)\\n1st 50th 100th 150th 200th 250th 300th\\nPosition of Key to Retrieve405060708090100Accuracy\\n300 Key-Value Pairs (~16K tokens)\\nclaude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mpt-30b-instruct longchat-13b-16kFigure 10: The effect of changing the input context length and the position of relevant information on key-value\\nretrieval performance. Lower positions are closer to the start of the input context. Although some models are largely\\nperfect on this synthetic task (e.g., claude-1.3 and claude-1.3), we see again that performance is often highest when\\nrelevant information is occurs at the very start or very end of the context, and rapidly degrades when models must\\nretrieve from the middle of the input context. LongChat-13B (16K) in the 140 key-value setting is a notable outlier;\\nwhen the relevant information is at the start of the input context, it tends to generate code to retrieve the key, rather\\nthan outputting the value itself.\\n1st 5th 10th\\nPosition of Document with the Answer5055606570Accuracy\\n10 T otal Retrieved Documents\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer5055606570Accuracy\\n20 T otal Retrieved Documents\\n1st 5th 10th 15th 20th 25th 30th\\nPosition of Document with the Answer5055606570Accuracy\\n30 T otal Retrieved Documents\\nmpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2\\nFigure 11: Encoder-decoder models (Flan-UL2 and Flan-T5-XXL) are relatively robust to changes in the position\\nof relevant information within their input context when evaluated on sequences that are shorter than their encoder’s\\ntraining-time maximum sequence length (2048 and 512 tokens, respectively). However, when these models are\\nevaluated on sequences longer than those seen during training (20- and 30-document settings), they also exhibit\\na U-shaped performance curve, where performance is much higher when the relevant information occurs at the\\nbeginning or end of the input context as opposed to the middle.\\nWe experiment with Flan-T5-XXL (Raffel et al.,\\n2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,\\n2023). Flan-T5-XXL is trained with a sequences\\nof 512 tokens (encoder and decoder). Flan-UL2\\nis initially trained with sequences of 512 tokens\\n(encoder and decoder), but is then pre-trained for\\nan extra 100K steps with 1024 tokens (encoder and\\ndecoder), before instruction-tuning on sequences\\nwith 2048 tokens in the encoder and 512 tokens\\nin the decoder. However, since these models use\\nrelative positional embeddings, they can (in prin-\\nciple) extrapolate beyond these maximum context\\nlengths; Shaham et al. (2023) find that both models\\ncan perform well with sequences of 8K tokens.\\nFigure 11 juxtaposes the performance of\\ndecoder-only and encoder-decoder models. WhenFlan-UL2 is evaluated on sequences within its 2048\\ntraining-time context window, its performance is\\nrelatively robust to changes in the position of rel-\\nevant information within the input context. When\\nevaluated on settings with sequences longer than\\n2048 tokens, Flan-UL2 performance begins to de-\\ngrade when relevant information is place in the mid-\\ndle. Flan-T5-XXL shows a similar trend, where\\nlonger input contexts result in a greater perfor-\\nmance degradation when placing relevant infor-\\nmation in the middle of the input context.\\nWe speculate that encoder-decoder models may\\nmake better use of their context windows because\\ntheir bidirectional encoder allows processing each\\ndocument in the context of future documents, po-\\ntentially enhancing relative importance estimation1st 5th 10th 15th 20th\\nPosition of Document with the Answer50607080Accuracy\\n20 T otal Retrieved Documents\\n(Query-Aware Contextualization)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613gpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16kFigure 12: Query-aware contextualization (i.e., placing\\nthe question before andafter the documents in the in-\\nput context) improves multi-document QA performance\\nwhen relevant information occurs at the very beginning,\\nbut slightly decreases performance otherwise.\\nbetween documents.\\n5.2 Effect of Query-Aware Contextualization\\nOur experiments in §3 and §4 place the query (i.e.,\\nquestion to answer or key to retrieve) after the data\\nto process (i.e., the documents or the key-value\\npairs). As a result, decoder-only models cannot\\nattend to query tokens when contextualizing doc-\\numents or key-value pairs, since the query only\\nappears at the end of the prompt and decoder-only\\nmodels can only attend to prior tokens at each\\ntimestep. On the other hand, encoder-decoder mod-\\nels use a bidirectional encoder to contextualize in-\\nput contexts, and seem to be more robust to changes\\nin the position of relevant information in their in-\\nput context—can use this intuition to also improve\\nthe performance of decoder-only models by plac-\\ning the query before andafter the data, enabling\\nquery-aware contextualization of documents (or\\nkey-value pairs)?\\nWe find that query-aware contextualization dra-\\nmatically improves performance on the key-value\\nretrieval task. For example, GPT-3.5-Turbo (16K)\\n(with query-aware contextualization) achieves per-\\nfect performance when evaluated with 300 key-\\nvalue pairs. In contrast, without query-aware con-\\ntextualization, it achieves a lowest performance of\\n45.6% in the same setting (Figure 10).\\nIn contrast, query-aware contextualization min-\\nimally affects performance trends in the multi-\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer44464850525456Accuracy\\n20 T otal Retrieved Documents\\nmpt-30b mpt-30b-instructFigure 13: Multi-document QA performance of MPT-\\n30B-Instruct compared against its base model (i.e., be-\\nfore instruction fine-tuning) MPT-30B. Both models\\nhave a U-shaped performance curve, where performance\\nis much higher when relevant information occurs at the\\nstart or end of the input context, indicating that the\\ninstruction tuning process itself is not necessarily re-\\nsponsible for these performance trends.\\ndocument question answering task. In particular,\\nit improves performance when the relevant infor-\\nmation is located at the very beginning of the input\\ncontext, but slightly decreases performance in other\\nsettings.\\n5.3 Effect of Instruction-Tuning\\nAll of the models that we evaluated in §3 and\\n§4 are instruction-tuned—after their initial pre-\\ntraining, they undergo supervised fine-tuning on\\na dataset of instructions and responses. In this\\nsupervised instruction-tuning data, the task specifi-\\ncation and/or instruction is commonly placed at the\\nbeginning of the input context, which might lead\\ninstruction-tuned language models to place more\\nweight on the start of the input context.\\nTo better understand the potential effects of\\ninstruction-tuning on how language models use\\nlong input contexts, we compare the multi-\\ndocument question answering performance of\\nMPT-30B-Instruct against its base model (i.e., be-\\nfore instruction fine-tuning) MPT-30B. We use the\\nsame experimental setup as §3.\\nFigure 13 compares the multi-document QA\\nperformance of MPT-30B and MPT-30B-Instruct\\nas a function of the position of the relevant in-\\nformation in the input context. Surprisingly, wesee that both MPT-30B and MPT-30B-Instruct ex-\\nhibit a U-shaped performance curve, where perfor-\\nmance is highest when relevant information occurs\\nat the very beginning or very end of the context.\\nAlthough the absolute performance of MPT-30B-\\nInstruct is uniformly higher than that of MPT-30B,\\ntheir overall performance trends are quite similar.\\nThese observations complement prior work,\\nwhich found that language models are biased to-\\nwards recent tokens (i.e., the end of the input con-\\ntext; Khandelwal et al., 2018; Press et al., 2021).\\nThis recency bias is generally shown in the con-\\ntext of next-word prediction on contiguous text,\\nwhere language models minimally benefit from\\nlong-range information (Sun et al., 2021). In con-\\ntrast, our results show that language models are\\ncapable of using longer-range information (i.e., the\\nbeginning of the input context) when prompted\\nwith instruction-formatted data. We hypothesize\\nthat language models learn to use these contexts\\nfrom similarly-formatted data that may occur in\\nwebtext seen during pre-training, e.g., StackOver-\\nflow questions and answers.\\n6 Is More Context Is Always Better?\\nA Case Study With Open-Domain QA\\nIn practical settings, there is often a trade-off with\\nincreased the input context length—providing the\\ninstruction-tuned language model with more infor-\\nmation may help improve downstream task perfor-\\nmance, but also increases the amount of content\\nthat the model must reason over. Even if a language\\nmodel can take in 16K tokens, is it actually benefi-\\ncial to provide 16K tokens of context? The answer\\nto this question is downstream task-specific since it\\ndepends on the marginal value of the added context\\nand the model’s ability to effectively use long input\\ncontexts, but we perform a case study with open-\\ndomain question answering on NaturalQuestions-\\nOpen to better understand this trade-off.\\nWe use models in a standard retriever-reader\\nsetup. A retrieval system (Contriever, fine-tuned\\non MS-MARCO) takes an input query from\\nNaturalQuestions-Open and returns kdocuments\\nfrom Wikipedia. To condition instruction-tuned\\nlanguage models on these retrieved documents, we\\nsimply include them in the prompt. We evaluate\\nretriever recall and reader accuracy (whether any\\nof the annotated answers appear in the predicted\\noutput) as a function of the number of retrieved\\ndocuments k. We use a subset of NaturalQuestions-\\n510 20 30 40 50\\nNumber of Retrieved Docs5060708090Metric\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613mpt-30b-instruct\\nlongchat-13b-16k\\ncontriever recallFigure 14: Retriever recall and model performance as a\\nfunction of the number of retrieved documents. Model\\nperformance saturates long before retriever recall satu-\\nrates, indicating that the models have difficulty making\\nuse of the extra retrieved documents.\\nOpen where the long answer is a paragraph (as\\nopposed to a table or a list).\\nFigure 14 presents open-domain QA results. We\\nsee that reader model performance saturates long\\nbefore retriever performance levels off, indicating\\nthat readers are not effectively using the extra con-\\ntext. Using more than 20 retrieved documents only\\nmarginally improves reader performance ( ∼1.5%\\nfor GPT-3.5-Turbo and ∼1% for Claude), while sig-\\nnificantly increasing the input context length (and\\nthus latency and cost). These results, coupled with\\nthe observation that models are better at retriev-\\ning and using information at the start or end of the\\ninput contexts, suggest that effective reranking of\\nretrieved documents (pushing relevant information\\ncloser to the start of the input context) or ranked list\\ntruncation (returning fewer documents when neces-\\nsary; Arampatzis et al., 2009) may be promising di-\\nrections for improving how language-model-based\\nreaders use retrieved context.\\n7 Related Work\\n7.1 Long-context language models\\nThere is a rich line of work in designing performant\\nlanguage models with cheaper scaling than Trans-\\nformers in the context length. Many lines of work\\npursue Transformer variants with attention modi-\\nfications like recurrence (Dai et al., 2019), factor-\\nizing attention into computationally less intensive\\napproximations (Beltagy et al., 2020; Zaheer et al.,2020), or low-rank approximations (Wang et al.,\\n2020; Peng et al., 2021); see Tay et al. (2022) for\\na comprehensive overview. Dao et al. (2022) in-\\nstead provide a faster exact attention by a carefully-\\ncrafted IO-aware CUDA kernel. Separately, there\\nare attempts to do away with attention entire to\\nremove quadratic sequence length complexity, of-\\nten through convolution and/or linear RNNs, e.g.,\\nin RWKV (Peng, 2023), S4 (Gu et al., 2022), or\\nHyena (Poli et al., 2023). Many efforts evaluate per-\\nplexity on a diverse web corpus as a proxy for the\\nability to process long contexts; this work shows\\nthat precise knowledge access on long contexts\\nmay be an added challenge.\\n7.2 How do language models use context?\\nThe pioneering work of Khandelwal et al. (2018)\\nshowed that small LSTM language models make\\nincreasingly coarse use of longer-term context;\\nSankar et al. (2019) found similar results in dia-\\nlogue models. Petroni et al. (2020) were among the\\nfirst to demonstrate the potential of combining con-\\ntext from an information retrieval system with a pre-\\ntrained language models for unsupervised question\\nanswering. O’Connor and Andreas (2021) found\\nthat many information-destroying operations had\\nmarginal effects on Transformer LMs’ predictions.\\nKrishna et al. (2022) found that long-context neu-\\nral generation in modestly-sized Transformer lan-\\nguage models degenerates because models fail to\\nproperly condition on long context. Finally, study-\\ning long-context models, Sun et al. (2021) found\\nthat longer contexts improves prediction of only\\na few tokens, an empirical finding consistent with\\nthe theory of Sharan et al. (2018), who showed\\nthat sequence distributions with bounded mutual\\ninformation necessarily lead to marginal average\\nprediction benefits from increasingly long context.\\n7.3 The serial-position effect\\nThe U-shaped curve we observe in this work has\\na connection in psychology known as the serial-\\nposition effect (Ebbinghaus, 1913; Murdock Jr,\\n1962), that states that in free-association recall\\nof elements from a list, humans tend to best re-\\nmember the first and last elements of the list. The\\nserial-position effect plays a role in understanding\\nhow humans develop short- and long-term memory.\\nObserving a serial-position-like effect in LLMs is\\nperhaps surprising, since the self-attention mecha-\\nnisms underlying Transformer LLMs being techni-\\ncally equally capable of retrieving any token fromtheir contexts.\\n8 Conclusion\\nWe empirically study how language models use\\nlong input contexts via a series of controlled ex-\\nperiments on two tasks that require identifying\\nand using relevant information in-context: multi-\\ndocument question answering and key-value re-\\ntrieval. We find that language models often strug-\\ngle to use information in the middle of long input\\ncontexts, and that performance decreases as the\\ninput context grows longer. We conduct a pre-\\nliminary investigation of the role of (i) model ar-\\nchitecture, (ii) query-aware contextualization, and\\n(iii) instruction-tuning to better understand how\\neach of these factors might affect how language\\nmodels use context. Finally, we conclude with a\\npractical case study of open-domain question an-\\nswering, finding that the performance of language\\nmodel readers saturates far before retriever recall.\\nOur results and analysis provide a better under-\\nstanding of how language models use their input\\ncontext and provides new evaluation protocols for\\nfuture long-context models.\\nAcknowledgments\\nWe thank Sewon Min for her help with AmbigQA.\\nIn addition, we thank Eric Wallace and Sang\\nMichael Xie for feedback and discussions that\\nhelped improve this work. This work was sup-\\nported by the Stanford Center for Research on\\nFoundation Models (CRFM), by OpenAI via an\\nAPI credits grant to the CRFM, and by Anthropic\\nvia the Claude academic access program.\\nReferences\\nAvi Arampatzis, Jaap Kamps, and Stephen Robertson.\\n2009. Where to stop reading a ranked list? threshold\\noptimization using truncated score distributions. In\\nProc. of SIGIR .\\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\\n2020. Longformer: The long-document transformer.\\nArXiv:2004.05150.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\\nand Jason Wei. 2022. Scaling instruction-finetuned\\nlanguage models. ArXiv:2210.11416.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\\nTransformer-XL: Attentive language models beyond\\na fixed-length context. In Proc. of ACL .\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness.\\nArXiv:2205.14135.\\nHermann Ebbinghaus. 1913. Memory: A contribution\\nto experimental psychology. H. A. Ruger & C. E.\\nBussenius, Trans.\\nDaniel Y . Fu, Tri Dao, Khaled Kamal Saab, Armin W.\\nThomas, Atri Rudra, and Christopher Ré. 2023. Hun-\\ngry hungry hippos: Towards language modeling with\\nstate space models. In Proc. of ICLR .\\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\\nciently modeling long sequences with structured state\\nspaces. In Proc. of ICLR .\\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\\nand Edouard Grave. 2021. Unsupervised dense\\ninformation retrieval with contrastive learning.\\nArXiv:2112.09118.\\nGautier Izacard and Edouard Grave. 2021. Leveraging\\npassage retrieval with generative models for open\\ndomain question answering. In Proc. of EACL .\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2022. Large lan-\\nguage models struggle to learn long-tail knowledge.\\nArXiv:2211.08411.\\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky.\\n2018. Sharp nearby, fuzzy far away: How neural\\nlanguage models use context. In Proc. of ACL .\\nKalpesh Krishna, Yapei Chang, John Wieting, and Mo-\\nhit Iyyer. 2022. RankGen: Improving text generation\\nwith large ranking models. In Proc. of EMNLP .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral Questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proc. of ACL .Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-\\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\\nMa, , and Hao Zhang. 2023. How long can open-\\nsource LLMs truly promise on context length?\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proc. of ACL .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\\nbiguous open-domain questions. In Proc. of EMNLP .\\nBennet B. Murdock Jr. 1962. The serial position effect\\nof free recall. Journal of experimental psychology ,\\n64(5):482.\\nJoe O’Connor and Jacob Andreas. 2021. What context\\nfeatures can transformer language models use? In\\nProc. of ACL .\\nDimitris Papailiopoulos, Kangwook Lee, and Jy-\\nyong Sohn. 2023. A little retrieval test for large\\nlanguage models. https://github.com/anadim/\\nthe-little-retrieval-test .\\nBo Peng. 2023. RWKV-LM. https://github.com/\\nBlinkDL/RWKV-LM .\\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\\nRandom feature attention. In Proc. of ICLR .\\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\\nRocktäschel, Yuxiang Wu, Alexander H Miller, and\\nSebastian Riedel. 2020. How context affects lan-\\nguage models’ factual predictions. In Proc. of AKBC .\\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\\nfano Ermon, and Christopher Ré. 2023. Hyena hierar-\\nchy: Towards larger convolutional language models.\\nInProc. of ICML .\\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\\nShortformer: Better language modeling using shorter\\ninputs. In Proc. of ACL .\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research ,\\n21(140):1–67.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. ArXiv:2302.00083.\\nOhad Rubin and Jonathan Berant. 2023. Long-\\nrange language modeling with self-retrieval.\\nArXiv:2306.13421.Chinnadhurai Sankar, Sandeep Subramanian, Chris Pal,\\nSarath Chandar, and Yoshua Bengio. 2019. Do neu-\\nral dialog systems use the conversation history effec-\\ntively? an empirical study. In Proc. of ACL .\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\\nCancedda, and Thomas Scialom. 2023. Toolformer:\\nLanguage models can teach themselves to use tools.\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Be-\\nrant, and Omer Levy. 2023. ZeroSCROLLS: A\\nzero-shot benchmark for long text understanding.\\nArXiv:2305.14196.\\nVatsal Sharan, Sham Kakade, Percy Liang, and Gregory\\nValiant. 2018. Prediction with a short memory. In\\nProc. of STOC .\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen tau Yih. 2023. REPLUG: Retrieval-augmented\\nblack-box language models. ArXiv:2301.12652.\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\\nMicke, and Mohit Iyyer. 2021. Do long-range lan-\\nguage models actually use long-range context? In\\nProc. of EMNLP .\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\\nzler. 2022. Efficient transformers: A survey. ACM\\nComputing Surveys , 55(6).\\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier\\nGarcia, Jason Wei, Xuezhi Wang, Hyung Won\\nChung, Siamak Shakeri, Dara Bahri, Tal Schuster,\\nHuaixiu Steven Zheng, Denny Zhou, Neil Houlsby,\\nand Donald Metzler. 2023. UL2: Unifying language\\nlearning paradigms. ArXiv:2205.05131.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023. LLaMA:\\nOpen and efficient foundation language models.\\nArXiv:2302.13971.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Proc. of NeurIPS .\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\\nand Hao Ma. 2020. Linformer: Self-attention with\\nlinear complexity. ArXiv , abs/2006.04768.\\nLili Yu, Dániel Simig, Colin Flaherty, Armen Agha-\\njanyan, Luke Zettlemoyer, and Mike Lewis. 2023.\\nMEGABYTE: Predicting million-byte sequences\\nwith multiscale transformers. ArXiv:2305.07185.\\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,Li Yang, and Amr Ahmed. 2020. Big bird: Trans-\\nformers for longer sequences. In Proc. of NeurIPS ,\\nvolume 33.A Ambiguity in Multi-Document QA\\nDistractor Documents\\nFollowing a variety of past work on\\nNaturalQuestions-Open (Izacard et al., 2021;\\nIzacard and Grave, 2021, inter alia ), we use a\\nstandard Wikipedia dump from late 2018 as our\\nretrieval corpus. However, this standard Wikipedia\\ndump has a small amount of temporal mismatch\\nwith the data in NaturalQuestions.\\nFor example, consider the question “what nfl\\nteam does robert griffin iii play for”. The Natu-\\nralQuestions annotated answer is “currently a free\\nagent”. However, the Wikipedia retrieval corpus\\ncontains the information that he plays for the “Balti-\\nmore Ravens”, since he was released from the team\\nbetween the Wikipedia dump’s timestamp and the\\nNaturalQuestions annotation process.\\nWe use the ambiguity annotations of Min et al.\\n(2020) to create a subset unambiguous questions.\\nExperiments on this unambiguous subset of the\\ndata show similar results and conclusions as the\\nexperiments on the full questions collection (Fig-\\nure 15).\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer60657075Accuracy\\n20 T otal Retrieved Documents (Unambiguous Questions)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613gpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 15: Language model performance on a unam-\\nbiguous subset of questions.\\nB Randomizing Distractor Order in\\nMulti-Document QA\\nOur prompt instructs the language model to use\\nthe provided search results to answer the ques-\\ntion. There may be a prior in the pre-training or\\ninstruction-tuning data to treat search results as\\nsorted by decreasing relevance (i.e., the documents\\nnear the beginning of the input context are more\\nlikely to be useful than those at the end). To vali-date that our conclusions are not simply a byprod-\\nuct of this bias, we run experiments the modified\\ninstruction “Write a high-quality answer for the\\ngiven question using only the provided search re-\\nsults (some of which might be irrelevant). The\\nsearch results are ordered randomly.” In addition,\\nwe randomly shuffle the k−1distractor documents.\\nFigure 16 presents the results of this experiment.\\nWe continue to see a U-shaped performance curve,\\nwith performance degrading when language mod-\\nels must use information in the middle of their\\ninput contexts. Comparing the results in §3.3 with\\nthose when randomizing the distractor order and\\nmentioning such in the prompt, we see that ran-\\ndomization slightly decrases performance when\\nthe relevant information is at the very beginning\\nof the context, and slightly increases performance\\nwhen using information in the middle and end of\\nthe context.\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer5560657075Accuracy\\n20 T otal Retrieved Documents (Randomly Ordered)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613gpt-3.5-turbo-16k-0613\\nmpt-30b-instruct\\nlongchat-13b-16k\\nFigure 16: Language model performance when random-\\nizing the order of the distractors (rather than presenting\\nthem in order of decreasing relevance) and mentioning\\nas such in the prompt.C GPT-4 Performance\\nWe evaluate GPT-4 on a subset of 500 random\\nexamples (Figure 17). GPT-4 achieves higher abso-\\nlute performance than any other language model,\\nbut still shows a U-shaped performance curve—its\\nperformance is highest when relevant information\\noccurs at the very start or end of the context, and\\nperformance degrades when it must use informa-\\ntion in the middle of its input context.\\n1st 5th 10th 15th 20th\\nPosition of Document with the Answer5060708090Accuracy\\n20 T otal Retrieved Documents (500 Question Sample)\\nclaude-1.3\\nclaude-1.3-100k\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-16k-0613mpt-30b-instruct\\nlongchat-13b-16k\\ngpt-4-0613\\nFigure 17: Although GPT-4 has higher absolute perfor-\\nmance than other models, its performance still degrades\\nwhen relevant information occurs in the middle of the\\ninput context.\\nD Closed-book and Oracle Performance\\nTable 1 presents language model performance\\non the closed-book and oracle settings for multi-\\ndocument question answering. In the closed-book\\nsetting, language models are not given any docu-\\nments in their input context, and must rely on their\\nparametric memory to generate the correct answer.\\nIn the oracle setting, language models are given the\\nsingle document that contains the answer, and must\\nuse it to answer the question. This represents an\\nupper-bound on task performance.Model Closed-Book Oracle\\nLongChat-13B (16K) 35.0% 83.35%\\nMPT-30B-Instruct 31.5% 81.9%\\nGPT-3.5-Turbo 56.1% 88.3\\nGPT-3.5-Turbo (16K) 56.0% 88.6\\nClaude 48.3% 76.1%\\nClaude (100K) 48.2% 76.4%\\nTable 1: Closed-book and oracle accuracy of language\\nmodels on the multi-document question answering task.'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(filename, query, top_k) -> list[Document]:\n",
    "    result = db.query(filename, query, top_k)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='sample.pdf-p3-3', text='hysics) ... \\nDocument [2](Title: Asian Americans in science and \\ntechnology) ... \\nDocument [3](Title: Scientist) ... \\nQuestion: who got the first nobel prize in physics \\nAnswer: Input Context \\nWilhelm Conrad Röntgen Desired Answer Figure 4: Modulating the position of relevant informa-\\ntion within the input context for the multi-document\\nquestion answering example presented in Figure 2. Re-\\nordering the documents in the input context does not\\naffect the desired output. The relevant document for cor-\\nrectly answering the request is bolded within the input\\ncontext.\\n3.2 Models\\nWe analyze several state-of-the-art open and closed\\nmodels. We use greedy decoding when generating\\noutputs and leave exploration of other decoding\\nmethods to future work. We use a standard set of\\nprompts for each model (depicted in Figure 2).\\nOpen models. We experiment with MPT-30B-\\nInstruct, which has a maximum context length of\\n8192 tokens. The model was initially pre-trained\\non 1 trilion tokens using 2048-token se', metadata=Metadata(filename='sample.pdf', page=3)),\n",
       " Document(id='sample.pdf-p1-4', text='val-\\nuated on sequences longer than those seen during\\ntraining (§5.1). In addition, query-aware contex-\\ntualization (placing the query before andafter the\\ndocuments or key-value pairs) enables models to\\nperform the synthetic key-value task perfectly, but\\nminimally changes trends in multi-document QA\\n(§5.2). Finally, even base language models (i.e.,\\nwithout instruction fine-tuning) show a U-shaped\\nperformance curve as we vary the position of rele-\\nvant information in the input context.\\nLastly, we perform a case study with retriever-\\nreader models on open-domain question answering\\nto better understand the trade-off between adding\\nmore information to an input context and increas-\\ning the amount of content that the model must rea-\\nson over (§6)—in contrast to our controlled multi-\\ndocument QA task, where the context always con-\\ntains exactly one document that answers the ques-\\ntion, none or many of the top kdocuments may\\ncontain the answer in the open-domain QA sett-\\nting. When retrieving ', metadata=Metadata(filename='sample.pdf', page=1)),\n",
       " Document(id='sample.pdf-p11-3', text='ot to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proc. of ACL .\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\\nbiguous open-domain questions. In Proc. of EMNLP .\\nBennet B. Murdock Jr. 1962. The serial position effect\\nof free recall. Journal of experimental psychology ,\\n64(5):482.\\nJoe O’Connor and Jacob Andreas. 2021. What context\\nfeatures can transformer language models use? In\\nProc. of ACL .\\nDimitris Papailiopoulos, Kangwook Lee, and Jy-\\nyong Sohn. 2023. A little retrieval test for large\\nlanguage models. https://github.com/anadim/\\nthe-little-retrieval-test .\\nBo Peng. 2023. RWKV-LM. https://github.com/\\nBlinkDL/RWKV-LM .\\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\\nRandom feature attention. In Proc. of ICLR .\\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\\nRocktäschel, Yuxiang Wu, Alexander H Miller, and\\nSebastian Riedel. 2020. ', metadata=Metadata(filename='sample.pdf', page=11)),\n",
       " Document(id='sample.pdf-p11-2', text='oc. of EMNLP .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral Questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\\n2019. Latent retrieval for weakly supervised open\\ndomain question answering. In Proc. of ACL .Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-\\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\\nMa, , and Hao Zhang. 2023. How long can open-\\nsource LLMs truly promise on context length?\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories', metadata=Metadata(filename='sample.pdf', page=11)),\n",
       " Document(id='sample.pdf-p11-5', text='ham. 2023. In-context retrieval-augmented lan-\\nguage models. ArXiv:2302.00083.\\nOhad Rubin and Jonathan Berant. 2023. Long-\\nrange language modeling with self-retrieval.\\nArXiv:2306.13421.', metadata=Metadata(filename='sample.pdf', page=11)),\n",
       " Document(id='sample.pdf-p11-0', text='Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\\nand Jason Wei. 2022. Scaling instruction-finetuned\\nlanguage models. ArXiv:2210.11416.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\\nTransformer-XL: Attentive language models beyond\\na fixed-length context. In Proc. of ACL .\\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\\nand Christopher Ré. 2022. FlashAttention: Fast and\\nmemory-efficient exact attention with IO-awareness.\\nArXiv:2205.14135.\\nHermann Ebbinghaus. 1913. Memory: A contribution\\nto experimental psychology. H. A. Ruger & C. E.\\nBussenius, Trans.\\nDaniel Y . Fu, Tri Dao, Khaled Kamal Saab, Armin W.\\nThomas, Atri Rudra, and Christopher Ré. 2023. Hun-\\ngry hungry hippos: Towards language modeling with\\nstate space models. In Proc. of ICLR .\\nAlbert Gu, Karan Goel, and Christopher Ré. 2022. Effi-\\nciently modeling long sequences with structured state\\nspaces. In Proc. of ICLR .\\nGautier ', metadata=Metadata(filename='sample.pdf', page=11)),\n",
       " Document(id='sample.pdf-p10-3', text='rieval. We find that language models often strug-\\ngle to use information in the middle of long input\\ncontexts, and that performance decreases as the\\ninput context grows longer. We conduct a pre-\\nliminary investigation of the role of (i) model ar-\\nchitecture, (ii) query-aware contextualization, and\\n(iii) instruction-tuning to better understand how\\neach of these factors might affect how language\\nmodels use context. Finally, we conclude with a\\npractical case study of open-domain question an-\\nswering, finding that the performance of language\\nmodel readers saturates far before retriever recall.\\nOur results and analysis provide a better under-\\nstanding of how language models use their input\\ncontext and provides new evaluation protocols for\\nfuture long-context models.\\nAcknowledgments\\nWe thank Sewon Min for her help with AmbigQA.\\nIn addition, we thank Eric Wallace and Sang\\nMichael Xie for feedback and discussions that\\nhelped improve this work. This work was sup-\\nported by the Stanford Center f', metadata=Metadata(filename='sample.pdf', page=10)),\n",
       " Document(id='sample.pdf-p3-1', text='equest is bolded within the input context.\\nWrite a high-quality answer for the given question \\nusing only the provided search results (some of \\nwhich might be irrelevant). \\nDocument [1](Title: Asian Americans in science and \\ntechnology) ... \\nDocument [2](Title: List of Nobel laureates in \\nPhysics) ... \\nDocument [3](Title: Scientist) ... \\nDocument [4](Title: Norwegian Americans) ... \\nDocument [5](Title: Maria Goeppert Mayer) ... \\nQuestion: who got the first nobel prize in physics \\nAnswer: Input Context Input Context \\nWilhelm Conrad Röntgen Desired Answer \\nFigure 3: Modulating the input context length of the\\nmulti-document question answering example presented\\nin Figure 2. Adding additional documents that do not\\ncontain the answer increases the length of the input con-\\ntext, but does not affect the desired output. The relevant\\ndocument pair for correctly answering the request is\\nbolded within the input context.\\nFollowing Kandpal et al. (2022) and Mallen et al.\\n(2023), we use accuracy as o', metadata=Metadata(filename='sample.pdf', page=3)),\n",
       " Document(id='sample.pdf-p2-2', text='ons of tokens, raising the po-\\ntential of further dramatic increases in language\\nmodel maximum context length (Gu et al., 2022;\\nFu et al., 2023; Poli et al., 2023; Yu et al., 2023,\\ninter alia ).\\n3 Multi-Document Question Answering\\nOur goal is to better understand how language mod-\\nels use their input context. To this end, we analyze\\nmodel performance on multi-document questionanswering, which requires models to find relevant\\ninformation within an input context and using it to\\nanswer the question. In particular, we make con-\\ntrolled changes to the length of the input context\\nand the position of the relevant information and\\nmeasure changes in task performance.\\n3.1 Experimental Setup\\nOur multi-document question answering task\\nclosely parallels the retrieval-augmented genera-\\ntion setup underlying commercial search and ques-\\ntion answering applications (e.g., Bing Chat). In\\nthese experiments, the model inputs are (i) a ques-\\ntion to answer and (ii) kdocuments (e.g., passages\\nfrom Wikipedia', metadata=Metadata(filename='sample.pdf', page=2)),\n",
       " Document(id='sample.pdf-p11-4', text='is, Aleksandra Piktus, Tim\\nRocktäschel, Yuxiang Wu, Alexander H Miller, and\\nSebastian Riedel. 2020. How context affects lan-\\nguage models’ factual predictions. In Proc. of AKBC .\\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\\nfano Ermon, and Christopher Ré. 2023. Hyena hierar-\\nchy: Towards larger convolutional language models.\\nInProc. of ICML .\\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\\nShortformer: Better language modeling using shorter\\ninputs. In Proc. of ACL .\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\\nlimits of transfer learning with a unified text-to-text\\ntransformer. Journal of Machine Learning Research ,\\n21(140):1–67.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. 2023. In-context retrieval-augmented lan-\\nguage models. ArXiv:2302.00083.\\nOhad Rubin and Jonath', metadata=Metadata(filename='sample.pdf', page=11))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query('sample.pdf', '이 논문의 내용은 어떤 내용이야?', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ddf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(result)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/pandas/core/frame.py:664\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    659\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    660\u001b[0m     )\n\u001b[1;32m    662\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    663\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    665\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    666\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs-analysis/lib/python3.10/site-packages/pandas/core/internals/construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[1;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "ddf = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('cs-analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "605fb24626e5d9dde86fb4511df1db7e98c0db454ef87401d4ffa2d92a62c0b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
